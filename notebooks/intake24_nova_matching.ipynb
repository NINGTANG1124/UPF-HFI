{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NINGTANG1124/UPF-HFI/blob/main/notebooks/intake24_nova_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect googledrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9izKK0g9LUQ",
        "outputId": "821dedc1-ab50-431c-effd-0e3a2e76ad2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Data Processing"
      ],
      "metadata": {
        "id": "HzWJRGRDwuI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 Intake24 Data"
      ],
      "metadata": {
        "id": "ZrpfCWlSz_xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read intake data (including Descriptionen and FoodGroupen)\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/UPF-HFI/Bradford_original data/1. Dietmasterfile_foodlevel_clean.xls\"\n",
        "intake_df = pd.read_excel(file_path)\n",
        "\n",
        "# Define text cleaning function\n",
        "def clean_text(col):\n",
        "    return col.astype(str).str.lower().str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
        "\n",
        "# Apply to key fields\n",
        "intake_df[\"Foodgroupen_clean\"] = clean_text(intake_df[\"Foodgroupen\"])\n",
        "intake_df[\"Descriptionen_clean\"] = clean_text(intake_df[\"Descriptionen\"])\n"
      ],
      "metadata": {
        "id": "c7bYEbA49bs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.12 NDNS NOVA"
      ],
      "metadata": {
        "id": "JUYge2zXa9Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立标准清洗列\n",
        "att3[\"desc_clean\"] = att3[\"Subsidiary food group name\"] \\\n",
        "    .astype(str).str.lower().str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
        "\n",
        "# 去掉带星号的行（* 代表规则判断不确定）\n",
        "att3_no_star = att3[~att3[\"desc_clean\"].str.contains(r\"\\*\")]\n"
      ],
      "metadata": {
        "id": "zzfMMJsGa8eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 NOVA Data"
      ],
      "metadata": {
        "id": "zuawPtHvy082"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nova file data cleaning\n",
        "ndns_df = pd.read_csv(\"/content/drive/MyDrive/UPF-HFI/nova/NDNS_NOVA_DATABASE.new2023.csv\", encoding=\"ISO-8859-1\")\n",
        "ndns_df.columns = ndns_df.columns.str.strip()\n",
        "ndns_df = ndns_df[[\"FoodName\", \"NOVA\"]].dropna()\n",
        "ndns_df[\"FoodName_clean\"] = ndns_df[\"FoodName\"].str.lower().str.replace(r\"[^\\w\\s]\", \" \", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "ndns_df = ndns_df.drop_duplicates(subset=[\"FoodName_clean\"])\n",
        "\n",
        "# Giulia file data cleaning\n",
        "giulia_df = pd.read_excel(\"/content/drive/MyDrive/UPF-HFI/nova/Training Data Original Given by NOVA Researchers - Corrections by Giulia Babak FNDDS 2009-10.xls\")\n",
        "giulia_df.columns = giulia_df.columns.str.strip()\n",
        "\n",
        "giulia_df = giulia_df[[\"Main_food_description\", \"SR_nova_group\"]].dropna()\n",
        "giulia_df = giulia_df.rename(columns={\"Main_food_description\": \"FoodName\", \"SR_nova_group\": \"NOVA\"})\n",
        "\n",
        "giulia_df[\"FoodName_clean\"] = giulia_df[\"FoodName\"].str.lower().str.replace(r\"[^\\w\\s]\", \" \", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "giulia_df = giulia_df.drop_duplicates(subset=[\"FoodName_clean\"])\n",
        "\n",
        "# off file data cleaning\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "off_clean = []\n",
        "with open(\"/content/drive/MyDrive/UPF-HFI/nova/openfoodfacts-popular-24.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            entry = json.loads(line)\n",
        "            if not isinstance(entry, dict):\n",
        "                continue\n",
        "\n",
        "            name = entry.get(\"product_name\") or entry.get(\"abbreviated_product_name\")\n",
        "            nova = entry.get(\"nova_group\")\n",
        "\n",
        "            if name and nova:\n",
        "                name_clean = re.sub(r\"[^\\w\\s]\", \" \", name.lower())\n",
        "                name_clean = re.sub(r\"\\s+\", \" \", name_clean).strip()\n",
        "                off_clean.append({\"FoodName_clean\": name_clean, \"NOVA\": nova})\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "off_df = pd.DataFrame(off_clean).drop_duplicates(subset=[\"FoodName_clean\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "OMkARVTN1cMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndns_df.to_csv(\"NDNS_clean.csv\", index=False)\n",
        "giulia_df.to_csv(\"Giulia_clean.csv\", index=False)\n",
        "off_df.to_csv(\"OFF_clean.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "9U74WsoNz287"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Keyword Matching"
      ],
      "metadata": {
        "id": "XrcR1bWbxRI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Media-Dicken matching"
      ],
      "metadata": {
        "id": "vZa0hVAApYsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 示例函数：完全依据 media 文件中提到的分类规则\n",
        "def media_based_nova_classification(row):\n",
        "    desc = row['Descriptionen'].lower()\n",
        "\n",
        "    # === Media rule: Homemade mayonnaise, salad cream, or French dressing → NOVA 4\n",
        "    if 'homemade' in desc and any(x in desc for x in ['mayonnaise', 'salad cream', 'french dressing']):\n",
        "        return 4\n",
        "\n",
        "    # === Media rule: Homemade buns, cakes, pastries, puddings, pancakes → NOVA 3\n",
        "    if 'homemade' in desc and any(x in desc for x in ['bun', 'cake', 'pastry', 'pudding', 'pancake']):\n",
        "        return 3\n",
        "\n",
        "    # === Media rule: Homemade pies or dumplings → NOVA 3\n",
        "    if 'homemade' in desc and any(x in desc for x in ['pie', 'dumpling']):\n",
        "        return 3\n",
        "\n",
        "    # === Media rule: Homemade gravy → NOVA 1 (unless oxo or granule, then NOVA 3)\n",
        "    if 'homemade' in desc and 'gravy' in desc:\n",
        "        if 'oxo' in desc or 'granule' in desc:\n",
        "            return 3\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    # === Media rule: Homemade stews, meat-based meals → NOVA 1\n",
        "    if 'homemade' in desc and ('stew' in desc or 'meat' in desc):\n",
        "        return 1\n",
        "\n",
        "    # === Media rule: Homemade cottage pie, shepherd’s pie → NOVA 1\n",
        "    if 'homemade' in desc and ('cottage pie' in desc or 'shepherd' in desc):\n",
        "        return 1\n",
        "\n",
        "    # === Media rule: Homemade battered/breaded fish → NOVA 3\n",
        "    if 'homemade' in desc and 'fish' in desc and any(x in desc for x in ['battered', 'breaded']):\n",
        "        return 3\n",
        "\n",
        "    # === Media rule: General battered/breaded fish → NOVA 4\n",
        "    if 'fish' in desc and any(x in desc for x in ['battered', 'breaded']):\n",
        "        return 4\n",
        "\n",
        "    # === Media rule: Canned vegetables in brine/syrup → NOVA 3, otherwise NOVA 1\n",
        "    if 'canned vegetable' in desc or 'tinned vegetable' in desc:\n",
        "        if 'brine' in desc or 'syrup' in desc:\n",
        "            return 3\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    # === Media rule: Canned fruit → NOVA 3\n",
        "    if 'canned fruit' in desc:\n",
        "        return 3\n",
        "\n",
        "    # === Media rule: Processed meats (bacon, ham, gammon, sliced meat) → NOVA 4\n",
        "    if any(x in desc for x in ['bacon', 'ham', 'gammon', 'sliced meat']):\n",
        "        return 4\n",
        "\n",
        "    # === Media rule: Specialty cured meats (prosciutto, parma, serrano) → NOVA 3\n",
        "    if any(x in desc for x in ['prosciutto', 'parma', 'serrano']):\n",
        "        return 3\n",
        "\n",
        "    # === Media rule: Nut butters (e.g., peanut butter) → NOVA 3\n",
        "    if 'nut butter' in desc:\n",
        "        return 3\n",
        "\n",
        "    # === Media rule: Gluten-free substitutes → NOVA 4\n",
        "    if 'gluten free' in desc:\n",
        "        return 4\n",
        "\n",
        "    # === Media rule: Jams, marmalade, lemon curd → NOVA 4\n",
        "    if any(x in desc for x in ['jam', 'marmalade', 'lemon curd']):\n",
        "        return 4\n",
        "\n",
        "    # === Media rule: Breakfast cereal or muesli → NOVA 4\n",
        "    if 'breakfast cereal' in desc or 'muesli' in desc:\n",
        "        return 4\n",
        "\n",
        "    # === Media rule: Plain porridge → NOVA 1\n",
        "    if 'porridge' in desc and 'plain' in desc:\n",
        "        return 1\n",
        "\n",
        "    # === Media rule: Plant-based milks → NOVA 1\n",
        "    if 'plant-based milk' in desc:\n",
        "        return 1\n",
        "\n",
        "    # === Media rule: Creams → NOVA 2\n",
        "    if any(x in desc for x in ['single cream', 'double cream', 'crème fraiche']):\n",
        "        return 2\n",
        "\n",
        "    # === Media rule: Chinese takeaway meals with soy sauce → NOVA 3\n",
        "    if 'soy sauce' in desc and any(x in desc for x in ['chow mein', 'chop suey']):\n",
        "        return 3\n",
        "\n",
        "    # === Media rule: Stir-fry dishes → NOVA 1\n",
        "    if 'stir fry' in desc:\n",
        "        return 1\n",
        "\n",
        "    return np.nan  # 如果都不匹配就返回空值\n",
        "\n"
      ],
      "metadata": {
        "id": "O82cn8jtpc1P"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qn6scyk2pfEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Description Part"
      ],
      "metadata": {
        "id": "nGbjvJhpxhvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_nova_by_description(text):\n",
        "    text = str(text).lower().strip()\n",
        "\n",
        "    # === NOVA 1: water ===\n",
        "    if any(w in text for w in [\"tap water\", \"still water\", \"filtered water\", \"plain water\"]):\n",
        "        if \"flavour\" not in text:\n",
        "            return 1, \"plain water (description)\"\n",
        "\n",
        "    # plain dairy\n",
        "    if any(w in text for w in [\"semi skimmed milk\", \"skimmed milk\", \"whole milk\"]) and \"flavour\" not in text:\n",
        "        return 1, \"plain milk\"\n",
        "    if any(w in text for w in [\"natural yoghurt\", \"fromage frais\"]) and \"flavour\" not in text:\n",
        "        return 1, \"plain yoghurt\"\n",
        "\n",
        "    # raw/unprocessed ===\n",
        "    # raw\n",
        "    import re\n",
        "\n",
        "    if re.search(r'\\braw\\b', text):\n",
        "        return 1, \"raw (word-bound)\"\n",
        "\n",
        "    # uncooked oat\n",
        "    if re.search(r\"\\buncooked\\b\", text) and re.search(r\"\\boat(s)?\\b\", text):\n",
        "        return 1, \"raw cereal: oats (description)\"\n",
        "\n",
        "\n",
        "    # === NOVA 3: homemade/light-processed ===\n",
        "    if any(w in text for w in [\"homemade\", \"home made\"]):\n",
        "        return 3, \"homemade\"\n",
        "    if any(w in text for w in [\"boiled\", \"mashed potato\", \"baked potato\", \"jacket potato\"]):\n",
        "        return 3, \"boiled/baked/jacket\"\n",
        "\n",
        "    # === NOVA 4: sachet porridge ===\n",
        "    if \"porridge sachet\" in text or (\"porridge\" in text and \"oat so simple\" in text):\n",
        "        return 4, \"sachet porridge (description)\"\n",
        "\n",
        "    # takeaway\n",
        "    if \"takeaway\" in text or \"take away\" in text:\n",
        "        return 4, \"takeaway food\"\n",
        "\n",
        "    # sweets/dessert/snack\n",
        "    if any(w in text for w in [\"jam\", \"conserve\", \"marmalade\", \"chocolate spread\", \"ice cream topping\", \"marzipan\"]):\n",
        "        return 4, \"spread/syrup\"\n",
        "    if any(w in text for w in [\"cracker\", \"savoury biscuit\", \"cheddar biscuit\", \"cream cracker\"]):\n",
        "        return 4, \"processed snack\"\n",
        "    if any(w in text for w in [\"sweets\", \"gums\", \"jelly\", \"boiled sweets\", \"mints\", \"liquorice\", \"popcorn\"]):\n",
        "        return 4, \"sweet snack\"\n",
        "    if any(w in text for w in [\"ice cream\", \"dessert\", \"milkshake\"]):\n",
        "        return 4, \"processed dessert\"\n",
        "    if any(w in text for w in [\"margarine\", \"clover spread\", \"flora\"]):\n",
        "        return 4, \"processed fat\"\n",
        "    if \"flavoured milk\" in text or \"chocolate milk\" in text:\n",
        "        return 4, \"flavoured milk\"\n",
        "    if \"ketchup\" in text and \"home made\" not in text:\n",
        "        return 4, \"processed ketchup\"\n",
        "    if \"instant\" in text and \"porridge\" not in text:\n",
        "        return 4, \"instant food\"\n",
        "\n",
        "\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "qYXdm7e1rO18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Group Part"
      ],
      "metadata": {
        "id": "C-h22cnixy3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_nova_by_group(group, description):\n",
        "    group = str(group).lower().strip()\n",
        "    description = str(description).lower().strip()\n",
        "\n",
        "    # === NOVA 1: group water ===\n",
        "    if group.strip() in [\"water\", \"tap water\", \"filtered water\"]:\n",
        "        return 1, \"water (group)\"\n",
        "\n",
        "    # milk/yoghurt\n",
        "    if \"fresh fruit\" in group:\n",
        "        return 1, \"fruit (group)\"\n",
        "    if \"dried fruit\" in group:\n",
        "        return 1, \"dried fruit (group)\"\n",
        "    if \"vegetables\" in group and \"fried\" not in group:\n",
        "        return 1, \"vegetables (group)\"\n",
        "    if any(word in group for word in [\"semi skimmed milk\", \"skimmed milk\", \"whole milk\"]):\n",
        "        if \"flavour\" not in description and \"fruit\" not in description:\n",
        "            return 1, \"milk (group)\"\n",
        "    if any(word in group for word in [\"natural yoghurt\", \"fromage frais\"]):\n",
        "        if \"flavour\" not in description and \"fruit\" not in description:\n",
        "            return 1, \"yoghurt/plain dairy (group)\"\n",
        "\n",
        "    # === NOVA 3: some fat ===\n",
        "    if any(w in group for w in [\"olive oil\", \"rapeseed oil\", \"sunflower oil\", \"vegetable oil\", \"butter\"]):\n",
        "        return 3, \"culinary fat/oil (group)\"\n",
        "\n",
        "    # === NOVA 4 ===\n",
        "    if any(w in group for w in [\"margarine\", \"fat spread\", \"flora\", \"dairy fat spreads\", \"hard marg\"]):\n",
        "        return 4, \"processed fat (group)\"\n",
        "    if any(w in group for w in [\"jam\", \"conserve\", \"marmalade\"]):\n",
        "        return 4, \"preserves (group)\"\n",
        "    if \"other breakfast cereals\" in group or \"muesli\" in group or \"bran flakes\" in group:\n",
        "        return 4, \"processed cereal (group)\"\n",
        "\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "zRt397tRpuES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(intake_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qbhgb3xD9Aq",
        "outputId": "a10211d0-42a7-48d7-8b51-ecf5072a1733",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['SurveyID', 'UserID', 'Source', 'Starttime', 'Submissiontime',\n",
            "       'Timetocomplete', 'Cookingoilused', 'Diet', 'Foodamount',\n",
            "       'Reasonforunusualfoodamount',\n",
            "       ...\n",
            "       'Modification_Identification', 'discontinued', 'NDNS_Checks',\n",
            "       'UserID_specific', 'Day', 'weekday', 'ratio', 'UserID_clean',\n",
            "       'Foodgroupen_clean', 'Descriptionen_clean'],\n",
            "      dtype='object', length=168)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Combined Rule Matching & Application"
      ],
      "metadata": {
        "id": "EZZ-4ylMx8fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_nova(row):\n",
        "    description = row[\"Descriptionen_clean\"]\n",
        "    group = row[\"Foodgroupen_clean\"]\n",
        "\n",
        "    # try description\n",
        "    nova, reason = match_nova_by_description(description)\n",
        "    if nova is not None:\n",
        "        return pd.Series([nova, \"description: \" + reason])\n",
        "\n",
        "    # fallback to group\n",
        "    nova, reason = match_nova_by_group(group, description)\n",
        "    if nova is not None:\n",
        "        return pd.Series([nova, \"group: \" + reason])\n",
        "\n",
        "    return pd.Series([None, None])\n"
      ],
      "metadata": {
        "id": "LgPplkugtTTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intake_df[[\"NOVA_step1\", \"match_reason\"]] = intake_df.apply(match_nova, axis=1)"
      ],
      "metadata": {
        "id": "-FKu22i2tWJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Save outcome_step1"
      ],
      "metadata": {
        "id": "pHMZXp0syrRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step1.csv\"\n",
        "\n",
        "cols_to_save = [\n",
        "    \"Descriptionen\",\n",
        "    \"Descriptionen_clean\",\n",
        "    \"Foodgroupen\",\n",
        "    \"Foodgroupen_clean\",\n",
        "    \"NOVA_step1\",\n",
        "    \"match_reason\"\n",
        "]\n",
        "intake_df[cols_to_save].to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "id": "pkShExdwiNwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. TF-IDF Matching"
      ],
      "metadata": {
        "id": "7sHOGGaH0d7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 2.1: Merge three NOVA control files and clean them\n",
        "ndns_df['source'] = 'NDNS'\n",
        "giulia_df['source'] = 'Giulia'\n",
        "off_df['source'] = 'OFF'\n",
        "\n",
        "nova_all = pd.concat([ndns_df, giulia_df, off_df], axis=0)\n",
        "nova_all = nova_all.drop_duplicates(subset='FoodName_clean').reset_index(drop=True)\n",
        "\n",
        "# Step 2.2: Select only the samples missing from NOVA_step1\n",
        "to_match_mask = intake_df['NOVA_step1'].isna()\n",
        "intake_tfidf_df = intake_df[to_match_mask].copy()\n",
        "\n",
        "# Step 2.3: TF-IDF vectorization and similarity calculation\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_intake = vectorizer.fit_transform(intake_tfidf_df['Descriptionen_clean'])\n",
        "tfidf_nova = vectorizer.transform(nova_all['FoodName_clean'])\n",
        "\n",
        "cos_sim = cosine_similarity(tfidf_intake, tfidf_nova)\n",
        "best_match_idx = cos_sim.argmax(axis=1)\n",
        "best_scores = cos_sim.max(axis=1)\n",
        "\n",
        "# Step 2.4: Only keep the matching results with score >= 0.85\n",
        "score_threshold = 0.85\n",
        "valid_mask = best_scores >= score_threshold\n",
        "\n",
        "intake_tfidf_df.loc[valid_mask, 'TFIDF_score'] = best_scores[valid_mask]\n",
        "intake_tfidf_df.loc[valid_mask, 'TFIDF_match_name'] = nova_all.iloc[best_match_idx[valid_mask]]['FoodName_clean'].values\n",
        "intake_tfidf_df.loc[valid_mask, 'NOVA_step2'] = nova_all.iloc[best_match_idx[valid_mask]]['NOVA'].values\n",
        "intake_tfidf_df.loc[valid_mask, 'Match_source'] = nova_all.iloc[best_match_idx[valid_mask]]['source'].values\n",
        "\n",
        "# Step 2.5: Merge the results back to the original intake_df\n",
        "intake_df.loc[intake_tfidf_df.index, 'TFIDF_score'] = intake_tfidf_df['TFIDF_score']\n",
        "intake_df.loc[intake_tfidf_df.index, 'TFIDF_match_name'] = intake_tfidf_df['TFIDF_match_name']\n",
        "intake_df.loc[intake_tfidf_df.index, 'NOVA_step2'] = intake_tfidf_df['NOVA_step2']\n",
        "intake_df.loc[intake_tfidf_df.index, 'Match_source'] = intake_tfidf_df['Match_source']\n"
      ],
      "metadata": {
        "id": "IEg-mNCD5l17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_step2_tfidf.csv\"\n",
        "\n",
        "columns_to_save = [\n",
        "    'Descriptionen', 'Descriptionen_clean',\n",
        "    'NOVA_step1', 'match_reason',\n",
        "    'NOVA_step2', 'TFIDF_score', 'TFIDF_match_name', 'Match_source'\n",
        "]\n",
        "intake_df[columns_to_save].to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "id": "42YjZA866A67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. SBERT"
      ],
      "metadata": {
        "id": "Il1Y1ot-64T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Step 3.1: Filtering samples not matched by Step1 and Step2\n",
        "mask_sbert = intake_df['NOVA_step1'].isna() & intake_df['NOVA_step2'].isna()\n",
        "intake_sbert_df = intake_df[mask_sbert].copy()\n",
        "\n",
        "# Step 3.2: Loading the SBERT Model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Step 3.3: Encoding into sentence vectors\n",
        "intake_embeddings = model.encode(intake_sbert_df['Descriptionen_clean'].tolist(), convert_to_tensor=True)\n",
        "nova_embeddings = model.encode(nova_all['FoodName_clean'].tolist(), convert_to_tensor=True)\n",
        "\n",
        "# Step 3.4: Similarity Matrix\n",
        "cos_scores = util.pytorch_cos_sim(intake_embeddings, nova_embeddings)\n",
        "best_match_scores, best_match_indices = cos_scores.max(dim=1)\n",
        "\n",
        "# Step 3.5: Threshold control\n",
        "threshold = 0.8\n",
        "score_array = best_match_scores.cpu().numpy()\n",
        "index_array = best_match_indices.cpu().numpy()\n",
        "\n",
        "# High score matching rows\n",
        "high_mask = score_array >= threshold\n",
        "low_mask = ~high_mask\n",
        "\n",
        "# Fill in the high score match results\n",
        "intake_sbert_df.loc[high_mask, 'SBERT_score'] = score_array[high_mask]\n",
        "intake_sbert_df.loc[high_mask, 'SBERT_match_name'] = nova_all.iloc[index_array[high_mask]]['FoodName_clean'].values\n",
        "intake_sbert_df.loc[high_mask, 'NOVA_step3'] = nova_all.iloc[index_array[high_mask]]['NOVA'].values\n",
        "intake_sbert_df.loc[high_mask, 'SBERT_match_source'] = nova_all.iloc[index_array[high_mask]]['source'].values\n",
        "\n",
        "# For low score matches, only the match information is recorded, but NOVA is not populated\n",
        "intake_sbert_df.loc[low_mask, 'SBERT_score'] = score_array[low_mask]\n",
        "intake_sbert_df.loc[low_mask, 'SBERT_match_name'] = nova_all.iloc[index_array[low_mask]]['FoodName_clean'].values\n",
        "intake_sbert_df.loc[low_mask, 'SBERT_flag'] = 'low_confidence'\n",
        "\n",
        "# Merge back to master table\n",
        "intake_df.loc[intake_sbert_df.index, ['SBERT_score', 'SBERT_match_name', 'NOVA_step3', 'SBERT_match_source', 'SBERT_flag']] = \\\n",
        "    intake_sbert_df[['SBERT_score', 'SBERT_match_name', 'NOVA_step3', 'SBERT_match_source', 'SBERT_flag']]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38h8mzSG62fP",
        "outputId": "1e5012c9-81ee-4e5d-a4de-de71751f37c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_step3_sbert.csv\"\n",
        "\n",
        "columns_to_save = [\n",
        "    'Descriptionen', 'Descriptionen_clean',\n",
        "    'NOVA_step1', 'match_reason',\n",
        "    'NOVA_step2', 'TFIDF_score', 'TFIDF_match_name',\n",
        "    'NOVA_step3', 'SBERT_score', 'SBERT_match_name',\n",
        "    'Match_source', 'SBERT_match_source', 'SBERT_flag'\n",
        "]\n",
        "intake_df[columns_to_save].to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "id": "0vLI2Qzt7bEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unmatched samples\n",
        "unmatched_mask = (\n",
        "    intake_df['NOVA_step1'].isna() &\n",
        "    intake_df['NOVA_step2'].isna() &\n",
        "    intake_df['NOVA_step3'].isna()\n",
        ")\n",
        "unmatched_count = unmatched_mask.sum()\n",
        "\n",
        "# Total sample size\n",
        "total_count = len(intake_df)\n",
        "matched_count = total_count - unmatched_count\n",
        "matched_percent = matched_count / total_count * 100\n",
        "\n",
        "print(f\"Number of matched samples:{matched_count} / {total_count}\")\n",
        "print(f\"Match Coverage:{matched_percent:.2f}%\")\n",
        "print(f\"Number of samples not yet matched:{unmatched_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adgjAAuZEOET",
        "outputId": "d073acfa-6603-4bdc-95f4-c8d4be4e56b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of matched samples:18175 / 22217\n",
            "Match Coverage:81.81%\n",
            "Number of samples not yet matched:4042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Validation conflicts"
      ],
      "metadata": {
        "id": "BVesAY8nYdzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if the samples matched successfully in Step 2 (TF-IDF) and Step 3 (SBERT) have different NOVA results for matching in the two models."
      ],
      "metadata": {
        "id": "6VDHZZT4HXF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing the NOVA of Step2 and Step3\n",
        "conflict_df = intake_df[\n",
        "    intake_df['NOVA_step2'].notna() &\n",
        "    intake_df['NOVA_step3'].notna() &\n",
        "    (intake_df['NOVA_step2'] != intake_df['NOVA_step3'])\n",
        "].copy()\n",
        "\n",
        "# Add a conflict description column\n",
        "conflict_df['Conflict_type'] = 'Step2 vs Step3 NOVA mismatch'\n",
        "\n",
        "print(f'Model conflict analysis only completed, a total of {len(conflict_df)} samples of NOVA classification conflicts for TF-IDF vs SBERT were found.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EVI_7MKYhCM",
        "outputId": "bbf1c33c-d56a-410c-ab0e-c7431991db81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model conflict analysis only completed, a total of 0 samples of NOVA classification conflicts for TF-IDF vs SBERT were found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Combine all of the above"
      ],
      "metadata": {
        "id": "RxV8q49BHDDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(intake_df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzZbtqCMc64H",
        "outputId": "fc8d90db-0acc-4bae-e695-8f6da9908fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SurveyID', 'UserID', 'Source', 'Starttime', 'Submissiontime', 'Timetocomplete', 'Cookingoilused', 'Diet', 'Foodamount', 'Reasonforunusualfoodamount', 'Proxy', 'ProxyIssues', 'MealIndex', 'MealID', 'Mealname', 'Mealtime', 'Foodsource', 'FoodIndex', 'Searchterm', 'FoodID', 'Intake24foodcode', 'Descriptionen', 'Descriptionlocal', 'Nutrienttablename', 'Nutrienttablecode', 'Foodgroupcode', 'Foodgroupen', 'Foodgrouplocal', 'Readymeal', 'Brand', 'Asservedweightfactor', 'Servingsizegml', 'Servingimage', 'Leftoversgml', 'Leftoversimage', 'Portionsizegml', 'Reasonableamount', 'MissingfoodID', 'Missingfooddescription', 'Missingfoodportionsize', 'Missingfoodleftovers', 'Subgroupcode', 'Water', 'Totalnitrogen', 'Nitrogenconversionfactor', 'Protein', 'Fat', 'Carbohydrate', 'Energykcal', 'EnergykJ', 'Alcohol', 'Englystfibre', 'Starch', 'Totalsugars', 'AOAC', 'Nonmilkextrinsicsugars', 'Intrinsicandmilksugars', 'Glucose', 'Fructose', 'Maltose', 'Lactose', 'Sucrose', 'OthersugarsUK', 'FSTablesugar', 'FSOtherAddedSugar', 'FSHoney', 'FSFruitJuice', 'FSDriedFruit', 'FSFruitPuree', 'FSStewedFruit', 'FSVegetablePuree', 'SatdFA', 'CisMonFA', 'Cisn3FA', 'Cisn6FA', 'TransFA', 'Cholesterol', 'Retinol', 'Totalcarotene', 'Alphacarotene', 'Betacarotene', 'Betacryptoxanthin', 'VitaminA', 'VitaminD', 'Thiamin', 'Riboflavin', 'Niacin', 'Tryptophan60', 'Niacinequivalent', 'VitaminC', 'VitaminE', 'VitaminB6', 'VitaminB12', 'Folate', 'Pantothenicacid', 'Biotin', 'Sodium', 'Potassium', 'Calcium', 'Magnesium', 'Phosphorus', 'Iron', 'Haemiron', 'Nonhaemiron', 'Copper', 'Zinc', 'Chloride', 'Iodine', 'Manganese', 'Selenium', 'TotalFS', 'Fruit', 'Driedfruit', 'Fruitjuice', 'Smoothiefruit', 'Tomatoes', 'Tomatopuree', 'Brassicaceae', 'YellowRedGreen', 'Beans', 'Nuts', 'OtherVegetables', 'Beef', 'Lamb', 'Pork', 'ProcessedRedMeat', 'OtherRedMeat', 'Burgers', 'Sausages', 'Offal', 'Poultry', 'ProcessedPoultry', 'GameBirds', 'WhiteFish', 'OilyFish', 'CannedTuna', 'Shellfish', 'CottageCheese', 'CheddarCheese', 'OtherCheese', 'NEWFoodGroupCode', 'DUPLICATE', 'ORPHAN', 'MISSING', 'OTHER', 'ADDITION_1', 'ADDITION_2', 'DAYCHANGE', 'DISCONTINUED', 'Checkedinitials', 'IDENTIFIERB2N2S2T2', 'Date', 'DAY', 'Original_Edited', 'AdditionReplacement', 'ModificationReason', 'ModificationDecisionTree', 'Researcher_Intake24', 'Modification_Identification', 'discontinued', 'NDNS_Checks', 'UserID_specific', 'Day', 'weekday', 'ratio', 'UserID_clean', 'Foodgroupen_clean', 'Descriptionen_clean', 'NOVA_step1', 'match_reason', 'TFIDF_score', 'TFIDF_match_name', 'NOVA_step2', 'Match_source', 'SBERT_score', 'SBERT_match_name', 'NOVA_step3', 'SBERT_match_source', 'SBERT_flag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rename_map = {\n",
        "    \"NOVA_step1\": \"NOVA_keyword\",\n",
        "    \"match_reason\": \"Keyword_reason\",\n",
        "    \"NOVA_step2\": \"NOVA_tfidf\",\n",
        "    \"TFIDF_match_name\": \"TFIDF_match\",\n",
        "    \"Match_source\": \"TFIDF_source\",\n",
        "    \"NOVA_step3\": \"NOVA_sbert\",\n",
        "    \"SBERT_match_name\": \"SBERT_match\",\n",
        "    \"SBERT_match_source\": \"SBERT_source\"\n",
        "}\n",
        "intake_df.rename(columns=rename_map, inplace=True)\n",
        "\n",
        "# Step 5.1 Checking for contradictory matches between models\n",
        "intake_df['Model_disagree_flag'] = (\n",
        "    intake_df['TFIDF_match'].notna() &\n",
        "    intake_df['SBERT_match'].notna() &\n",
        "    (intake_df['TFIDF_match'] != intake_df['SBERT_match'])\n",
        ")\n",
        "\n",
        "intake_df['NOVA_disagree_flag'] = (\n",
        "    intake_df['Model_disagree_flag'] &\n",
        "    (intake_df['NOVA_tfidf'] != intake_df['NOVA_sbert'])\n",
        ")\n",
        "\n",
        "intake_df['Decision_flag'] = intake_df['NOVA_disagree_flag'].map(\n",
        "    lambda x: \"manual_review\" if x else \"auto_accept\"\n",
        ")\n",
        "\n",
        "# Step 5.2 Select Final Trusted Value: Keyword > SBERT > TF-IDF\n",
        "final_mask = intake_df['Decision_flag'] == \"auto_accept\"\n",
        "intake_df['NOVA_final'] = None\n",
        "\n",
        "intake_df.loc[final_mask & intake_df['NOVA_keyword'].notna(), 'NOVA_final'] = intake_df['NOVA_keyword']\n",
        "intake_df.loc[final_mask & intake_df['NOVA_final'].isna() & intake_df['NOVA_sbert'].notna(), 'NOVA_final'] = intake_df['NOVA_sbert']\n",
        "intake_df.loc[final_mask & intake_df['NOVA_final'].isna() & intake_df['NOVA_tfidf'].notna(), 'NOVA_final'] = intake_df['NOVA_tfidf']\n",
        "\n",
        "# Step 5.3 Add column\n",
        "def get_matched_name(row):\n",
        "    if pd.notna(row['NOVA_keyword']):\n",
        "        return row['TFIDF_match']\n",
        "    elif pd.notna(row['NOVA_sbert']):\n",
        "        return row['SBERT_match']\n",
        "\n",
        "def get_final_source(row):\n",
        "    if pd.notna(row['NOVA_keyword']):\n",
        "        return 'keyword'\n",
        "    elif pd.notna(row['NOVA_sbert']):\n",
        "        return row['SBERT_source']\n",
        "    elif pd.notna(row['NOVA_tfidf']):\n",
        "        return row['TFIDF_source']\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_final_reason(row):\n",
        "    if pd.notna(row['NOVA_keyword']):\n",
        "        return row['Keyword_reason']\n",
        "    elif pd.notna(row['NOVA_sbert']):\n",
        "        return 'SBERT matched'\n",
        "    elif pd.notna(row['NOVA_tfidf']):\n",
        "        return 'TF-IDF matched'\n",
        "    else:\n",
        "        return 'Unmatched'\n",
        "\n",
        "intake_df['Matched_name'] = intake_df.apply(get_matched_name, axis=1)\n",
        "intake_df['Final_source'] = intake_df.apply(get_final_source, axis=1)\n",
        "intake_df['Final_reason'] = intake_df.apply(get_final_reason, axis=1)\n",
        "\n",
        "# Step 5.4 Save the final result\n",
        "final_cols = [\n",
        "    \"Descriptionen\", \"Descriptionen_clean\",\n",
        "    \"NOVA_keyword\", \"Keyword_reason\",\n",
        "    \"NOVA_tfidf\", \"TFIDF_score\", \"TFIDF_match\", \"TFIDF_source\",\n",
        "    \"NOVA_sbert\", \"SBERT_score\", \"SBERT_match\", \"SBERT_source\",\n",
        "    \"Model_disagree_flag\", \"NOVA_disagree_flag\", \"Decision_flag\",\n",
        "    \"NOVA_final\",\"Matched_name\",\"Final_source\", \"Final_reason\"\n",
        "]\n",
        "\n",
        "intake_df[final_cols].to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_final.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "xa5JQ623GIbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slim_cols = [\n",
        "    \"Descriptionen\",\n",
        "    \"NOVA_final\",\n",
        "    \"Matched_name\",\n",
        "    \"Final_reason\"\n",
        "]\n",
        "\n",
        "intake_df[slim_cols].to_csv(\n",
        "    \"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_final_slim.csv\",\n",
        "    index=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "NcOpaRPrKyih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Unmatched-18.19%(4042)"
      ],
      "metadata": {
        "id": "Cu10YcODLxzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the Failure_reason column to the summary table\n",
        "def get_failure_reason(row):\n",
        "    if pd.notna(row['NOVA_final']):\n",
        "        return None\n",
        "    elif row['Decision_flag'] == 'manual_review':\n",
        "        return 'Model disagreement'\n",
        "    elif pd.isna(row['NOVA_keyword']) and pd.isna(row['NOVA_tfidf']) and pd.isna(row['NOVA_sbert']):\n",
        "        return 'No match found'\n",
        "    else:\n",
        "        return 'Conflict unresolved'\n",
        "\n",
        "intake_df['Failure_reason'] = intake_df.apply(get_failure_reason, axis=1)\n",
        "\n",
        "# Filter out unmatched rows\n",
        "unmatched_df = intake_df[intake_df['NOVA_final'].isna()].copy()\n",
        "\n",
        "# Save\n",
        "unmatched_df[[\"Descriptionen\", \"Failure_reason\"]].to_csv(\n",
        "    \"/content/drive/MyDrive/UPF-HFI/outcome/intake_unmatched_summary.csv\",\n",
        "    index=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZNroKW18MHv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 6.1: Get unmatched samples\n",
        "unmatched_df = intake_df[intake_df['NOVA_final'].isna()].copy()\n",
        "\n",
        "# Step 6.2: TF-IDF fallback（score ≥ 0.6）\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_intake = vectorizer.fit_transform(unmatched_df['Descriptionen_clean'])\n",
        "tfidf_nova = vectorizer.transform(nova_all['FoodName_clean'])\n",
        "cos_sim = cosine_similarity(tfidf_intake, tfidf_nova)\n",
        "\n",
        "tfidf_best_idx = cos_sim.argmax(axis=1)\n",
        "tfidf_best_scores = cos_sim.max(axis=1)\n",
        "tfidf_thresh = 0.6\n",
        "tfidf_mask = tfidf_best_scores >= tfidf_thresh\n",
        "\n",
        "unmatched_df.loc[tfidf_mask, 'NOVA_final_fallback'] = nova_all.iloc[tfidf_best_idx[tfidf_mask]]['NOVA'].values\n",
        "unmatched_df.loc[tfidf_mask, 'Fallback_source'] = nova_all.iloc[tfidf_best_idx[tfidf_mask]]['source'].values\n",
        "unmatched_df.loc[tfidf_mask, 'Fallback_match_name'] = nova_all.iloc[tfidf_best_idx[tfidf_mask]]['FoodName_clean'].values\n",
        "unmatched_df.loc[tfidf_mask, 'Fallback_reason'] = 'TF-IDF fallback'\n",
        "\n",
        "# Step 6.3: SBERT fallback match (score ≥ 0.6, TF-IDF misses only)\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "sbert_input = unmatched_df['Descriptionen_clean'].tolist()\n",
        "sbert_embed_intake = model.encode(sbert_input, convert_to_tensor=True)\n",
        "sbert_embed_nova = model.encode(nova_all['FoodName_clean'].tolist(), convert_to_tensor=True)\n",
        "\n",
        "sbert_sim = util.pytorch_cos_sim(sbert_embed_intake, sbert_embed_nova)\n",
        "sbert_best_scores, sbert_best_idx = sbert_sim.max(dim=1)\n",
        "\n",
        "sbert_thresh = 0.6\n",
        "\n",
        "bool_mask = sbert_best_scores.cpu().numpy() >= sbert_thresh\n",
        "sbert_mask = pd.Series(bool_mask, index=unmatched_df.index)\n",
        "\n",
        "# Get matches with index alignment\n",
        "matched_indices = sbert_best_idx.cpu().numpy()[sbert_mask.values]\n",
        "matched_rows = unmatched_df[sbert_mask]\n",
        "\n",
        "# Fill in the fallback match result\n",
        "unmatched_df.loc[matched_rows.index, 'NOVA_final_fallback'] = nova_all.iloc[matched_indices]['NOVA'].values\n",
        "unmatched_df.loc[matched_rows.index, 'Fallback_source'] = nova_all.iloc[matched_indices]['source'].values\n",
        "unmatched_df.loc[matched_rows.index, 'Fallback_match_name'] = nova_all.iloc[matched_indices]['FoodName_clean'].values\n",
        "unmatched_df.loc[matched_rows.index, 'Fallback_reason'] = 'SBERT fallback'"
      ],
      "metadata": {
        "id": "JwFUlzawO12q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6.4: Filter out successful fallback rows and save them\n",
        "fallback_df = unmatched_df[unmatched_df['NOVA_final_fallback'].notna()][[\n",
        "    \"Descriptionen\", \"NOVA_final_fallback\", \"Fallback_source\",\"Fallback_match_name\",\"Fallback_reason\"\n",
        "]]\n",
        "\n",
        "fallback_df.to_csv(\n",
        "    \"/content/drive/MyDrive/UPF-HFI/outcome/intake_unmatched_fallback_matched.csv\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(f'Fallback match complete, total {len(fallback_df)} strips unmatched sample')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6DvjorrU-gu",
        "outputId": "5570176b-0316-44f8-ab03-9f18840bb3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fallback match complete, total 3809 strips unmatched sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step6.5: Getting unmatched samples of fallback failures\n",
        "failed_fallback_df = unmatched_df[unmatched_df['NOVA_final_fallback'].isna()]\n",
        "\n",
        "failure_counts = (\n",
        "    failed_fallback_df['Descriptionen']\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "failure_counts.columns = ['Descriptionen', 'Count']\n",
        "\n",
        "# Export\n",
        "failure_counts.to_csv(\n",
        "    \"/content/drive/MyDrive/UPF-HFI/outcome/fallback_failure_counts.csv\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "# Displayed\n",
        "print(\"Description of all fallback failure samples:\")\n",
        "print(failure_counts.head(30))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpnCgd4uaVcQ",
        "outputId": "256596a9-a915-4404-9ea1-edde4013d1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Description of all fallback failure samples:\n",
            "                                        Descriptionen  Count\n",
            "0                                        Vimto, still     61\n",
            "1                                  Pom Bears (crisps)     52\n",
            "2                                            Pringles     18\n",
            "3                                         Cheesy mash     15\n",
            "4                                               Fanta     10\n",
            "5                             Magnum classic or white     10\n",
            "6                                            Smarties      8\n",
            "7                        Lollipops (e.g. Chupa Chups)      7\n",
            "8                                   Fox's party rings      6\n",
            "9                            Walker Sensations crisps      6\n",
            "10                                          Poppadums      5\n",
            "11                                         Fanta Zero      4\n",
            "12                                     Malteser bunny      3\n",
            "13                                      7 up / Sprite      3\n",
            "14                                             Solero      3\n",
            "15                                       Kinder Bueno      2\n",
            "16     Yam, fried (Mach alu (Dioscorea species) vaga)      2\n",
            "17                           Lentil dahl / Tarka dahl      2\n",
            "18                                       Time Out bar      1\n",
            "19                                     Ferrero Rocher      1\n",
            "20                                     Tyrells crisps      1\n",
            "21                                             Tahini      1\n",
            "22  Yellow lentil dahl, with ghee (Dal tarka/ Dal ...      1\n",
            "23                          Red gram dahl (Arhar dal)      1\n",
            "24                                       Vimto, fizzy      1\n"
          ]
        }
      ]
    }
  ]
}