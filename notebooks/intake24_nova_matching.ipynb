{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NINGTANG1124/UPF-HFI/blob/main/notebooks/intake24_nova_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect googledrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9izKK0g9LUQ",
        "outputId": "3dd30aac-00ec-46de-ab12-e7d26150b619"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Data Processing"
      ],
      "metadata": {
        "id": "HzWJRGRDwuI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 Intake24 Data"
      ],
      "metadata": {
        "id": "ZrpfCWlSz_xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read intake data (including Descriptionen and FoodGroupen)\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/UPF-HFI/Bradford_original data/1. Dietmasterfile_foodlevel_clean.xls\"\n",
        "intake_df = pd.read_excel(file_path)\n",
        "\n",
        "# Define text cleaning function\n",
        "def clean_text(col):\n",
        "    return col.astype(str).str.lower().str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
        "\n",
        "# Apply to key fields\n",
        "intake_df[\"Foodgroupen_clean\"] = clean_text(intake_df[\"Foodgroupen\"])\n",
        "intake_df[\"Descriptionen_clean\"] = clean_text(intake_df[\"Descriptionen\"])\n"
      ],
      "metadata": {
        "id": "c7bYEbA49bs9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 NOVA Data"
      ],
      "metadata": {
        "id": "zuawPtHvy082"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nova file data cleaning\n",
        "ndns_df = pd.read_csv(\"/content/drive/MyDrive/UPF-HFI/nova/NDNS_NOVA_DATABASE.new2023.csv\", encoding=\"ISO-8859-1\")\n",
        "ndns_df.columns = ndns_df.columns.str.strip()\n",
        "ndns_df = ndns_df[[\"FoodName\", \"NOVA\"]].dropna()\n",
        "ndns_df[\"FoodName_clean\"] = ndns_df[\"FoodName\"].str.lower().str.replace(r\"[^\\w\\s]\", \" \", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "ndns_df = ndns_df.drop_duplicates(subset=[\"FoodName_clean\"])\n",
        "\n",
        "# Giulia file data cleaning\n",
        "giulia_df = pd.read_excel(\"/content/drive/MyDrive/UPF-HFI/nova/Training Data Original Given by NOVA Researchers - Corrections by Giulia Babak FNDDS 2009-10.xls\")\n",
        "giulia_df.columns = giulia_df.columns.str.strip()\n",
        "\n",
        "giulia_df = giulia_df[[\"Main_food_description\", \"SR_nova_group\"]].dropna()\n",
        "giulia_df = giulia_df.rename(columns={\"Main_food_description\": \"FoodName\", \"SR_nova_group\": \"NOVA\"})\n",
        "\n",
        "giulia_df[\"FoodName_clean\"] = giulia_df[\"FoodName\"].str.lower().str.replace(r\"[^\\w\\s]\", \" \", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "giulia_df = giulia_df.drop_duplicates(subset=[\"FoodName_clean\"])\n",
        "\n",
        "# off file data cleaning\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "off_clean = []\n",
        "with open(\"/content/drive/MyDrive/UPF-HFI/nova/openfoodfacts-popular-24.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            entry = json.loads(line)\n",
        "            if not isinstance(entry, dict):\n",
        "                continue\n",
        "\n",
        "            name = entry.get(\"product_name\") or entry.get(\"abbreviated_product_name\")\n",
        "            nova = entry.get(\"nova_group\")\n",
        "\n",
        "            if name and nova:\n",
        "                name_clean = re.sub(r\"[^\\w\\s]\", \" \", name.lower())\n",
        "                name_clean = re.sub(r\"\\s+\", \" \", name_clean).strip()\n",
        "                off_clean.append({\"FoodName_clean\": name_clean, \"NOVA\": nova})\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "off_df = pd.DataFrame(off_clean).drop_duplicates(subset=[\"FoodName_clean\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "OMkARVTN1cMN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndns_df.to_csv(\"NDNS_clean.csv\", index=False)\n",
        "giulia_df.to_csv(\"Giulia_clean.csv\", index=False)\n",
        "off_df.to_csv(\"OFF_clean.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "9U74WsoNz287"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Keyword Matching"
      ],
      "metadata": {
        "id": "XrcR1bWbxRI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Description Part"
      ],
      "metadata": {
        "id": "nGbjvJhpxhvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_nova_by_description(text):\n",
        "    text = str(text).lower().strip()\n",
        "\n",
        "    # === NOVA 1: water ===\n",
        "    if any(w in text for w in [\"tap water\", \"still water\", \"filtered water\", \"plain water\"]):\n",
        "        if \"flavour\" not in text:\n",
        "            return 1, \"plain water (description)\"\n",
        "\n",
        "    # === NOVA 1: plain===\n",
        "    if any(w in text for w in [\"semi skimmed milk\", \"skimmed milk\", \"whole milk\"]) and \"flavour\" not in text:\n",
        "        return 1, \"plain milk\"\n",
        "    if any(w in text for w in [\"natural yoghurt\", \"fromage frais\"]) and \"flavour\" not in text:\n",
        "        return 1, \"plain yoghurt\"\n",
        "\n",
        "    # === NOVA 1: raw/unprocessed ===\n",
        "    # raw\n",
        "    import re\n",
        "\n",
        "    if re.search(r'\\braw\\b', text):\n",
        "        return 1, \"raw (word-bound)\"\n",
        "\n",
        "    # uncooked oat\n",
        "    if re.search(r\"\\buncooked\\b\", text) and re.search(r\"\\boat(s)?\\b\", text):\n",
        "        return 1, \"raw cereal: oats (description)\"\n",
        "\n",
        "\n",
        "    # === NOVA 3: homemade/light-processed ===\n",
        "    if any(w in text for w in [\"homemade\", \"home made\"]):\n",
        "        return 3, \"homemade\"\n",
        "    if any(w in text for w in [\"boiled\", \"mashed potato\", \"baked potato\", \"jacket potato\"]):\n",
        "        return 3, \"boiled/baked/jacket\"\n",
        "\n",
        "    # === NOVA 4: sachet porridge ===\n",
        "    if \"porridge sachet\" in text or (\"porridge\" in text and \"oat so simple\" in text):\n",
        "        return 4, \"sachet porridge (description)\"\n",
        "\n",
        "    # === NOVA 4: takeaway  ===\n",
        "    if \"takeaway\" in text or \"take away\" in text:\n",
        "        return 4, \"takeaway food\"\n",
        "\n",
        "    # === NOVA 4: sweets/dessert/snack ===\n",
        "    if any(w in text for w in [\"jam\", \"conserve\", \"marmalade\", \"chocolate spread\", \"ice cream topping\", \"marzipan\"]):\n",
        "        return 4, \"spread/syrup\"\n",
        "    if any(w in text for w in [\"cracker\", \"savoury biscuit\", \"cheddar biscuit\", \"cream cracker\"]):\n",
        "        return 4, \"processed snack\"\n",
        "    if any(w in text for w in [\"sweets\", \"gums\", \"jelly\", \"boiled sweets\", \"mints\", \"liquorice\", \"popcorn\"]):\n",
        "        return 4, \"sweet snack\"\n",
        "    if any(w in text for w in [\"ice cream\", \"dessert\", \"milkshake\"]):\n",
        "        return 4, \"processed dessert\"\n",
        "    if any(w in text for w in [\"margarine\", \"clover spread\", \"flora\"]):\n",
        "        return 4, \"processed fat\"\n",
        "    if \"flavoured milk\" in text or \"chocolate milk\" in text:\n",
        "        return 4, \"flavoured milk\"\n",
        "    if \"ketchup\" in text and \"home made\" not in text:\n",
        "        return 4, \"processed ketchup\"\n",
        "    if \"instant\" in text and \"porridge\" not in text:\n",
        "        return 4, \"instant food\"\n",
        "    # === NOVA 4: takeaway 快餐类 ===\n",
        "    if \"takeaway\" in text or \"take away\" in text:\n",
        "        return 4, \"takeaway food\"\n",
        "\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "qYXdm7e1rO18"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Group Part"
      ],
      "metadata": {
        "id": "C-h22cnixy3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_nova_by_group(group, description):\n",
        "    group = str(group).lower().strip()\n",
        "    description = str(description).lower().strip()\n",
        "\n",
        "    # === NOVA 1: group water ===\n",
        "    if group.strip() in [\"water\", \"tap water\", \"filtered water\"]:\n",
        "        return 1, \"water (group)\"\n",
        "\n",
        "    # === NOVA 1: milk/yoghurt ===\n",
        "    if \"fresh fruit\" in group:\n",
        "        return 1, \"fruit (group)\"\n",
        "    if \"dried fruit\" in group:\n",
        "        return 1, \"dried fruit (group)\"\n",
        "    if \"vegetables\" in group and \"fried\" not in group:\n",
        "        return 1, \"vegetables (group)\"\n",
        "    if any(word in group for word in [\"semi skimmed milk\", \"skimmed milk\", \"whole milk\"]):\n",
        "        if \"flavour\" not in description and \"fruit\" not in description:\n",
        "            return 1, \"milk (group)\"\n",
        "    if any(word in group for word in [\"natural yoghurt\", \"fromage frais\"]):\n",
        "        if \"flavour\" not in description and \"fruit\" not in description:\n",
        "            return 1, \"yoghurt/plain dairy (group)\"\n",
        "\n",
        "    # === NOVA 3: some fat ===\n",
        "    if any(w in group for w in [\"olive oil\", \"rapeseed oil\", \"sunflower oil\", \"vegetable oil\", \"butter\"]):\n",
        "        return 3, \"culinary fat/oil (group)\"\n",
        "\n",
        "    # === NOVA 4 ===\n",
        "    if any(w in group for w in [\"margarine\", \"fat spread\", \"flora\", \"dairy fat spreads\", \"hard marg\"]):\n",
        "        return 4, \"processed fat (group)\"\n",
        "    if any(w in group for w in [\"jam\", \"conserve\", \"marmalade\"]):\n",
        "        return 4, \"preserves (group)\"\n",
        "    if \"other breakfast cereals\" in group or \"muesli\" in group or \"bran flakes\" in group:\n",
        "        return 4, \"processed cereal (group)\"\n",
        "\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "zRt397tRpuES"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(intake_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qbhgb3xD9Aq",
        "outputId": "94b4b2e4-61fb-4c0a-eae1-2bde314515b9",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['SurveyID', 'UserID', 'Source', 'Starttime', 'Submissiontime',\n",
            "       'Timetocomplete', 'Cookingoilused', 'Diet', 'Foodamount',\n",
            "       'Reasonforunusualfoodamount',\n",
            "       ...\n",
            "       'Modification_Identification', 'discontinued', 'NDNS_Checks',\n",
            "       'UserID_specific', 'Day', 'weekday', 'ratio', 'UserID_clean',\n",
            "       'Foodgroupen_clean', 'Descriptionen_clean'],\n",
            "      dtype='object', length=168)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Combined Rule Matching & Application"
      ],
      "metadata": {
        "id": "EZZ-4ylMx8fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_nova(row):\n",
        "    description = row[\"Descriptionen_clean\"]\n",
        "    group = row[\"Foodgroupen_clean\"]\n",
        "\n",
        "    # try description\n",
        "    nova, reason = match_nova_by_description(description)\n",
        "    if nova is not None:\n",
        "        return pd.Series([nova, \"description: \" + reason])\n",
        "\n",
        "    # fallback to group\n",
        "    nova, reason = match_nova_by_group(group, description)\n",
        "    if nova is not None:\n",
        "        return pd.Series([nova, \"group: \" + reason])\n",
        "\n",
        "    return pd.Series([None, None])\n"
      ],
      "metadata": {
        "id": "LgPplkugtTTV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intake_df[[\"NOVA_step1\", \"match_reason\"]] = intake_df.apply(match_nova, axis=1)"
      ],
      "metadata": {
        "id": "-FKu22i2tWJ9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Save outcome_step1"
      ],
      "metadata": {
        "id": "pHMZXp0syrRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step1.csv\"\n",
        "\n",
        "cols_to_save = [\n",
        "    \"Descriptionen\",\n",
        "    \"Descriptionen_clean\",\n",
        "    \"Foodgroupen\",\n",
        "    \"Foodgroupen_clean\",\n",
        "    \"NOVA_step1\",\n",
        "    \"match_reason\"\n",
        "]\n",
        "intake_df[cols_to_save].to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "id": "pkShExdwiNwg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. TF-IDF Matching"
      ],
      "metadata": {
        "id": "7sHOGGaH0d7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 2.1: Merge three NOVA control files and clean them\n",
        "ndns_df['source'] = 'NDNS'\n",
        "giulia_df['source'] = 'Giulia'\n",
        "off_df['source'] = 'OFF'\n",
        "\n",
        "nova_all = pd.concat([ndns_df, giulia_df, off_df], axis=0)\n",
        "nova_all = nova_all.drop_duplicates(subset='FoodName_clean').reset_index(drop=True)\n",
        "\n",
        "# Step 2.2: Select only the samples missing from NOVA_step1\n",
        "to_match_mask = intake_df['NOVA_step1'].isna()\n",
        "intake_tfidf_df = intake_df[to_match_mask].copy()\n",
        "\n",
        "# Step 2.3: TF-IDF vectorization and similarity calculation\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_intake = vectorizer.fit_transform(intake_tfidf_df['Descriptionen_clean'])\n",
        "tfidf_nova = vectorizer.transform(nova_all['FoodName_clean'])\n",
        "\n",
        "cos_sim = cosine_similarity(tfidf_intake, tfidf_nova)\n",
        "best_match_idx = cos_sim.argmax(axis=1)\n",
        "best_scores = cos_sim.max(axis=1)\n",
        "\n",
        "# Step 2.4: Only keep the matching results with score >= 0.85\n",
        "score_threshold = 0.85\n",
        "valid_mask = best_scores >= score_threshold\n",
        "\n",
        "intake_tfidf_df.loc[valid_mask, 'TFIDF_score'] = best_scores[valid_mask]\n",
        "intake_tfidf_df.loc[valid_mask, 'TFIDF_match_name'] = nova_all.iloc[best_match_idx[valid_mask]]['FoodName_clean'].values\n",
        "intake_tfidf_df.loc[valid_mask, 'NOVA_step2'] = nova_all.iloc[best_match_idx[valid_mask]]['NOVA'].values\n",
        "intake_tfidf_df.loc[valid_mask, 'Match_source'] = nova_all.iloc[best_match_idx[valid_mask]]['source'].values\n",
        "\n",
        "# Step 2.5: Merge the results back to the original intake_df\n",
        "intake_df.loc[intake_tfidf_df.index, 'TFIDF_score'] = intake_tfidf_df['TFIDF_score']\n",
        "intake_df.loc[intake_tfidf_df.index, 'TFIDF_match_name'] = intake_tfidf_df['TFIDF_match_name']\n",
        "intake_df.loc[intake_tfidf_df.index, 'NOVA_step2'] = intake_tfidf_df['NOVA_step2']\n",
        "intake_df.loc[intake_tfidf_df.index, 'Match_source'] = intake_tfidf_df['Match_source']\n"
      ],
      "metadata": {
        "id": "IEg-mNCD5l17"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_step2_tfidf.csv\"\n",
        "\n",
        "columns_to_save = [\n",
        "    'Descriptionen', 'Descriptionen_clean',\n",
        "    'NOVA_step1', 'match_reason',\n",
        "    'NOVA_step2', 'TFIDF_score', 'TFIDF_match_name', 'Match_source'\n",
        "]\n",
        "intake_df[columns_to_save].to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "id": "42YjZA866A67"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. SBERT"
      ],
      "metadata": {
        "id": "Il1Y1ot-64T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Step 3.1: 筛选 Step1 和 Step2 都未匹配的样本\n",
        "mask_sbert = intake_df['NOVA_step1'].isna() & intake_df['NOVA_step2'].isna()\n",
        "intake_sbert_df = intake_df[mask_sbert].copy()\n",
        "\n",
        "# Step 3.2: 加载 SBERT 模型\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Step 3.3: 编码为句向量\n",
        "intake_embeddings = model.encode(intake_sbert_df['Descriptionen_clean'].tolist(), convert_to_tensor=True)\n",
        "nova_embeddings = model.encode(nova_all['FoodName_clean'].tolist(), convert_to_tensor=True)\n",
        "\n",
        "# Step 3.4: 相似度矩阵\n",
        "cos_scores = util.pytorch_cos_sim(intake_embeddings, nova_embeddings)\n",
        "best_match_scores, best_match_indices = cos_scores.max(dim=1)\n",
        "\n",
        "# Step 3.5: 阈值控制\n",
        "threshold = 0.8\n",
        "score_array = best_match_scores.cpu().numpy()\n",
        "index_array = best_match_indices.cpu().numpy()\n",
        "\n",
        "# 高分匹配行\n",
        "high_mask = score_array >= threshold\n",
        "low_mask = ~high_mask\n",
        "\n",
        "# 填入高分匹配结果\n",
        "intake_sbert_df.loc[high_mask, 'SBERT_score'] = score_array[high_mask]\n",
        "intake_sbert_df.loc[high_mask, 'SBERT_match_name'] = nova_all.iloc[index_array[high_mask]]['FoodName_clean'].values\n",
        "intake_sbert_df.loc[high_mask, 'NOVA_step3'] = nova_all.iloc[index_array[high_mask]]['NOVA'].values\n",
        "intake_sbert_df.loc[high_mask, 'SBERT_match_source'] = nova_all.iloc[index_array[high_mask]]['source'].values\n",
        "\n",
        "# 对低分匹配，仅记录匹配信息，但不填入 NOVA\n",
        "intake_sbert_df.loc[low_mask, 'SBERT_score'] = score_array[low_mask]\n",
        "intake_sbert_df.loc[low_mask, 'SBERT_match_name'] = nova_all.iloc[index_array[low_mask]]['FoodName_clean'].values\n",
        "intake_sbert_df.loc[low_mask, 'SBERT_flag'] = 'low_confidence'\n",
        "\n",
        "# 合并回主表\n",
        "intake_df.loc[intake_sbert_df.index, ['SBERT_score', 'SBERT_match_name', 'NOVA_step3', 'SBERT_match_source', 'SBERT_flag']] = \\\n",
        "    intake_sbert_df[['SBERT_score', 'SBERT_match_name', 'NOVA_step3', 'SBERT_match_source', 'SBERT_flag']]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38h8mzSG62fP",
        "outputId": "89393441-dd4a-4e81-c289-b92f8f11e1b9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置输出路径\n",
        "output_path = \"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_step3_sbert.csv\"\n",
        "\n",
        "# 保存 SBERT 匹配结果（保留 step1~4 及关键分数和来源）\n",
        "columns_to_save = [\n",
        "    'Descriptionen', 'Descriptionen_clean',\n",
        "    'NOVA_step1', 'match_reason',\n",
        "    'NOVA_step2', 'TFIDF_score', 'TFIDF_match_name',\n",
        "    'NOVA_step3', 'SBERT_score', 'SBERT_match_name',\n",
        "    'Match_source', 'SBERT_match_source', 'SBERT_flag'\n",
        "]\n",
        "intake_df[columns_to_save].to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "id": "0vLI2Qzt7bEN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算未匹配的样本数量\n",
        "unmatched_mask = (\n",
        "    intake_df['NOVA_step1'].isna() &\n",
        "    intake_df['NOVA_step2'].isna() &\n",
        "    intake_df['NOVA_step3'].isna()\n",
        ")\n",
        "unmatched_count = unmatched_mask.sum()\n",
        "\n",
        "# 总样本数量\n",
        "total_count = len(intake_df)\n",
        "matched_count = total_count - unmatched_count\n",
        "matched_percent = matched_count / total_count * 100\n",
        "\n",
        "print(f\"已匹配样本数量：{matched_count} / {total_count}\")\n",
        "print(f\"匹配覆盖率：{matched_percent:.2f}%\")\n",
        "print(f\"尚未匹配样本数量：{unmatched_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adgjAAuZEOET",
        "outputId": "2f72d18a-60dc-4f25-9d9e-ec850e728294"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已匹配样本数量：18175 / 22217\n",
            "匹配覆盖率：81.81%\n",
            "尚未匹配样本数量：4042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 验证冲突"
      ],
      "metadata": {
        "id": "BVesAY8nYdzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 4: 检查模型冲突（仅比较 Step2 vs Step3 的 NOVA） ===\n",
        "conflict_df = intake_df[\n",
        "    intake_df['NOVA_step2'].notna() &\n",
        "    intake_df['NOVA_step3'].notna() &\n",
        "    (intake_df['NOVA_step2'] != intake_df['NOVA_step3'])\n",
        "].copy()\n",
        "\n",
        "# 添加冲突说明列\n",
        "conflict_df['Conflict_type'] = 'Step2 vs Step3 NOVA mismatch'\n",
        "\n",
        "# 保存冲突详情\n",
        "conflict_df.to_csv(\n",
        "    \"/content/drive/MyDrive/UPF-HFI/outcome/step4_conflict_TFIDF_SBERT.csv\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(f\"仅模型冲突分析完成，共发现 {len(conflict_df)} 条 TF-IDF vs SBERT 的 NOVA 分类冲突样本。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EVI_7MKYhCM",
        "outputId": "f0701a26-c9d4-49d1-e62a-ac5dad5ebf74"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "仅模型冲突分析完成，共发现 0 条 TF-IDF vs SBERT 的 NOVA 分类冲突样本。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 合并以上所有"
      ],
      "metadata": {
        "id": "RxV8q49BHDDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(intake_df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzZbtqCMc64H",
        "outputId": "ddec1690-20e5-4f5d-953b-37e35d684230"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SurveyID', 'UserID', 'Source', 'Starttime', 'Submissiontime', 'Timetocomplete', 'Cookingoilused', 'Diet', 'Foodamount', 'Reasonforunusualfoodamount', 'Proxy', 'ProxyIssues', 'MealIndex', 'MealID', 'Mealname', 'Mealtime', 'Foodsource', 'FoodIndex', 'Searchterm', 'FoodID', 'Intake24foodcode', 'Descriptionen', 'Descriptionlocal', 'Nutrienttablename', 'Nutrienttablecode', 'Foodgroupcode', 'Foodgroupen', 'Foodgrouplocal', 'Readymeal', 'Brand', 'Asservedweightfactor', 'Servingsizegml', 'Servingimage', 'Leftoversgml', 'Leftoversimage', 'Portionsizegml', 'Reasonableamount', 'MissingfoodID', 'Missingfooddescription', 'Missingfoodportionsize', 'Missingfoodleftovers', 'Subgroupcode', 'Water', 'Totalnitrogen', 'Nitrogenconversionfactor', 'Protein', 'Fat', 'Carbohydrate', 'Energykcal', 'EnergykJ', 'Alcohol', 'Englystfibre', 'Starch', 'Totalsugars', 'AOAC', 'Nonmilkextrinsicsugars', 'Intrinsicandmilksugars', 'Glucose', 'Fructose', 'Maltose', 'Lactose', 'Sucrose', 'OthersugarsUK', 'FSTablesugar', 'FSOtherAddedSugar', 'FSHoney', 'FSFruitJuice', 'FSDriedFruit', 'FSFruitPuree', 'FSStewedFruit', 'FSVegetablePuree', 'SatdFA', 'CisMonFA', 'Cisn3FA', 'Cisn6FA', 'TransFA', 'Cholesterol', 'Retinol', 'Totalcarotene', 'Alphacarotene', 'Betacarotene', 'Betacryptoxanthin', 'VitaminA', 'VitaminD', 'Thiamin', 'Riboflavin', 'Niacin', 'Tryptophan60', 'Niacinequivalent', 'VitaminC', 'VitaminE', 'VitaminB6', 'VitaminB12', 'Folate', 'Pantothenicacid', 'Biotin', 'Sodium', 'Potassium', 'Calcium', 'Magnesium', 'Phosphorus', 'Iron', 'Haemiron', 'Nonhaemiron', 'Copper', 'Zinc', 'Chloride', 'Iodine', 'Manganese', 'Selenium', 'TotalFS', 'Fruit', 'Driedfruit', 'Fruitjuice', 'Smoothiefruit', 'Tomatoes', 'Tomatopuree', 'Brassicaceae', 'YellowRedGreen', 'Beans', 'Nuts', 'OtherVegetables', 'Beef', 'Lamb', 'Pork', 'ProcessedRedMeat', 'OtherRedMeat', 'Burgers', 'Sausages', 'Offal', 'Poultry', 'ProcessedPoultry', 'GameBirds', 'WhiteFish', 'OilyFish', 'CannedTuna', 'Shellfish', 'CottageCheese', 'CheddarCheese', 'OtherCheese', 'NEWFoodGroupCode', 'DUPLICATE', 'ORPHAN', 'MISSING', 'OTHER', 'ADDITION_1', 'ADDITION_2', 'DAYCHANGE', 'DISCONTINUED', 'Checkedinitials', 'IDENTIFIERB2N2S2T2', 'Date', 'DAY', 'Original_Edited', 'AdditionReplacement', 'ModificationReason', 'ModificationDecisionTree', 'Researcher_Intake24', 'Modification_Identification', 'discontinued', 'NDNS_Checks', 'UserID_specific', 'Day', 'weekday', 'ratio', 'UserID_clean', 'Foodgroupen_clean', 'Descriptionen_clean', 'NOVA_keyword', 'Keyword_reason', 'TFIDF_score', 'TFIDF_match', 'NOVA_tfidf', 'TFIDF_source', 'SBERT_score', 'SBERT_match', 'NOVA_sbert', 'SBERT_source', 'SBERT_flag', 'Model_disagree_flag', 'NOVA_disagree_flag', 'Decision_flag', 'NOVA_final', 'Final_source', 'Final_reason']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5.0 提前重命名（避免旧列名出错）\n",
        "rename_map = {\n",
        "    \"NOVA_step1\": \"NOVA_keyword\",\n",
        "    \"match_reason\": \"Keyword_reason\",\n",
        "    \"NOVA_step2\": \"NOVA_tfidf\",\n",
        "    \"TFIDF_match_name\": \"TFIDF_match\",\n",
        "    \"Match_source\": \"TFIDF_source\",\n",
        "    \"NOVA_step3\": \"NOVA_sbert\",\n",
        "    \"SBERT_match_name\": \"SBERT_match\",\n",
        "    \"SBERT_match_source\": \"SBERT_source\"\n",
        "}\n",
        "intake_df.rename(columns=rename_map, inplace=True)\n",
        "\n",
        "# Step 5.1 检查模型间矛盾匹配\n",
        "intake_df['Model_disagree_flag'] = (\n",
        "    intake_df['TFIDF_match'].notna() &\n",
        "    intake_df['SBERT_match'].notna() &\n",
        "    (intake_df['TFIDF_match'] != intake_df['SBERT_match'])\n",
        ")\n",
        "\n",
        "intake_df['NOVA_disagree_flag'] = (\n",
        "    intake_df['Model_disagree_flag'] &\n",
        "    (intake_df['NOVA_tfidf'] != intake_df['NOVA_sbert'])\n",
        ")\n",
        "\n",
        "intake_df['Decision_flag'] = intake_df['NOVA_disagree_flag'].map(\n",
        "    lambda x: \"manual_review\" if x else \"auto_accept\"\n",
        ")\n",
        "\n",
        "# Step 5.2 选择最终可信值：Keyword > SBERT > TF-IDF\n",
        "final_mask = intake_df['Decision_flag'] == \"auto_accept\"\n",
        "intake_df['NOVA_final'] = None\n",
        "\n",
        "intake_df.loc[final_mask & intake_df['NOVA_keyword'].notna(), 'NOVA_final'] = intake_df['NOVA_keyword']\n",
        "intake_df.loc[final_mask & intake_df['NOVA_final'].isna() & intake_df['NOVA_sbert'].notna(), 'NOVA_final'] = intake_df['NOVA_sbert']\n",
        "intake_df.loc[final_mask & intake_df['NOVA_final'].isna() & intake_df['NOVA_tfidf'].notna(), 'NOVA_final'] = intake_df['NOVA_tfidf']\n",
        "\n",
        "# Step 5.3 来源与理由（含 Rule-based → keyword）\n",
        "def get_final_source(row):\n",
        "    if pd.notna(row['NOVA_keyword']):\n",
        "        return 'keyword'\n",
        "    elif pd.notna(row['NOVA_sbert']):\n",
        "        return row['SBERT_source']\n",
        "    elif pd.notna(row['NOVA_tfidf']):\n",
        "        return row['TFIDF_source']\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_final_reason(row):\n",
        "    if pd.notna(row['NOVA_keyword']):\n",
        "        return row['Keyword_reason']\n",
        "    elif pd.notna(row['NOVA_sbert']):\n",
        "        return 'SBERT matched'\n",
        "    elif pd.notna(row['NOVA_tfidf']):\n",
        "        return 'TF-IDF matched'\n",
        "    else:\n",
        "        return 'Unmatched'\n",
        "\n",
        "intake_df['Final_source'] = intake_df.apply(get_final_source, axis=1)\n",
        "intake_df['Final_reason'] = intake_df.apply(get_final_reason, axis=1)\n",
        "\n",
        "# Step 5.4 保存最终结果\n",
        "final_cols = [\n",
        "    \"Descriptionen\", \"Descriptionen_clean\",\n",
        "    \"NOVA_keyword\", \"Keyword_reason\",\n",
        "    \"NOVA_tfidf\", \"TFIDF_score\", \"TFIDF_match\", \"TFIDF_source\",\n",
        "    \"NOVA_sbert\", \"SBERT_score\", \"SBERT_match\", \"SBERT_source\",\n",
        "    \"Model_disagree_flag\", \"NOVA_disagree_flag\", \"Decision_flag\",\n",
        "    \"NOVA_final\", \"Final_source\", \"Final_reason\"\n",
        "]\n",
        "\n",
        "intake_df[final_cols].to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_final.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "xa5JQ623GIbE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slim_cols = [\n",
        "    \"Descriptionen\",\n",
        "    \"NOVA_final\",\n",
        "    \"Final_reason\"\n",
        "]\n",
        "\n",
        "intake_df[slim_cols].to_csv(\n",
        "    \"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_final_slim.csv\",\n",
        "    index=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "NcOpaRPrKyih"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Unmatched-18.19%(4042)"
      ],
      "metadata": {
        "id": "Cu10YcODLxzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 添加 Failure_reason 列到总表中（如果尚未加过）\n",
        "def get_failure_reason(row):\n",
        "    if pd.notna(row['NOVA_final']):\n",
        "        return None\n",
        "    elif row['Decision_flag'] == 'manual_review':\n",
        "        return 'Model disagreement'\n",
        "    elif pd.isna(row['NOVA_keyword']) and pd.isna(row['NOVA_tfidf']) and pd.isna(row['NOVA_sbert']):\n",
        "        return 'No match found'\n",
        "    else:\n",
        "        return 'Conflict unresolved'\n",
        "\n",
        "intake_df['Failure_reason'] = intake_df.apply(get_failure_reason, axis=1)\n",
        "\n",
        "# 筛选出 unmatched 行\n",
        "unmatched_df = intake_df[intake_df['NOVA_final'].isna()].copy()\n",
        "\n",
        "# 保存 unmatched 文件\n",
        "unmatched_df[[\"Descriptionen\", \"Failure_reason\"]].to_csv(\n",
        "    \"/content/drive/MyDrive/UPF-HFI/outcome/intake_unmatched_summary.csv\",\n",
        "    index=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZNroKW18MHv1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 6.1: 获取未匹配样本（NOVA_final为空）\n",
        "unmatched_df = intake_df[intake_df['NOVA_final'].isna()].copy()\n",
        "\n",
        "# Step 6.2: TF-IDF fallback 匹配（score ≥ 0.6）\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_intake = vectorizer.fit_transform(unmatched_df['Descriptionen_clean'])\n",
        "tfidf_nova = vectorizer.transform(nova_all['FoodName_clean'])\n",
        "cos_sim = cosine_similarity(tfidf_intake, tfidf_nova)\n",
        "\n",
        "tfidf_best_idx = cos_sim.argmax(axis=1)\n",
        "tfidf_best_scores = cos_sim.max(axis=1)\n",
        "tfidf_thresh = 0.6\n",
        "tfidf_mask = tfidf_best_scores >= tfidf_thresh\n",
        "\n",
        "unmatched_df.loc[tfidf_mask, 'NOVA_final_fallback'] = nova_all.iloc[tfidf_best_idx[tfidf_mask]]['NOVA'].values\n",
        "unmatched_df.loc[tfidf_mask, 'Fallback_source'] = nova_all.iloc[tfidf_best_idx[tfidf_mask]]['source'].values\n",
        "unmatched_df.loc[tfidf_mask, 'Fallback_reason'] = 'TF-IDF fallback'\n",
        "\n",
        "# Step 6.3: SBERT fallback 匹配（score ≥ 0.6，仅限 TF-IDF 未命中的）\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "sbert_input = unmatched_df['Descriptionen_clean'].tolist()\n",
        "sbert_embed_intake = model.encode(sbert_input, convert_to_tensor=True)\n",
        "sbert_embed_nova = model.encode(nova_all['FoodName_clean'].tolist(), convert_to_tensor=True)\n",
        "\n",
        "sbert_sim = util.pytorch_cos_sim(sbert_embed_intake, sbert_embed_nova)\n",
        "sbert_best_scores, sbert_best_idx = sbert_sim.max(dim=1)\n",
        "\n",
        "sbert_thresh = 0.6\n",
        "# bool_mask 是 np.array，转成 Series 保留索引对齐\n",
        "bool_mask = sbert_best_scores.cpu().numpy() >= sbert_thresh\n",
        "sbert_mask = pd.Series(bool_mask, index=unmatched_df.index)\n",
        "\n",
        "# 用索引对齐方式获取匹配结果\n",
        "matched_indices = sbert_best_idx.cpu().numpy()[sbert_mask.values]\n",
        "matched_rows = unmatched_df[sbert_mask]\n",
        "\n",
        "# 填入 fallback 匹配结果（通过 .iloc 避免 index 错位）\n",
        "unmatched_df.loc[matched_rows.index, 'NOVA_final_fallback'] = nova_all.iloc[matched_indices]['NOVA'].values\n",
        "unmatched_df.loc[matched_rows.index, 'Fallback_source'] = nova_all.iloc[matched_indices]['source'].values\n",
        "unmatched_df.loc[matched_rows.index, 'Fallback_reason'] = 'SBERT fallback'"
      ],
      "metadata": {
        "id": "JwFUlzawO12q"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6.4: 筛选出匹配成功的 fallback 行并保存\n",
        "fallback_df = unmatched_df[unmatched_df['NOVA_final_fallback'].notna()][[\n",
        "    \"Descriptionen\", \"NOVA_final_fallback\", \"Fallback_source\", \"Fallback_reason\"\n",
        "]]\n",
        "\n",
        "fallback_df.to_csv(\n",
        "    \"/content/drive/MyDrive/UPF-HFI/outcome/intake_unmatched_fallback_matched.csv\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(f\"Fallback 匹配完成，共补全 {len(fallback_df)} 条 unmatched 样本\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6DvjorrU-gu",
        "outputId": "6e2b9411-4d05-4b74-af07-21b1f1d4f386"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fallback 匹配完成，共补全 3809 条 unmatched 样本\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step6.5: 获取 fallback 失败的 unmatched 样本\n",
        "failed_fallback_df = unmatched_df[unmatched_df['NOVA_final_fallback'].isna()]\n",
        "\n",
        "# 统计 Descriptionen 出现次数，并重命名列\n",
        "failure_counts = (\n",
        "    failed_fallback_df['Descriptionen']\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "failure_counts.columns = ['Descriptionen', 'Count']\n",
        "\n",
        "# 按出现次数降序显示\n",
        "print(\"所有 fallback 失败样本的 Descriptionen（含重复）:\")\n",
        "print(failure_counts.head(30))  # 你可以调整数量\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpnCgd4uaVcQ",
        "outputId": "0c3a62dc-9db9-4ba7-d722-045f73581f84"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "所有 fallback 失败样本的 Descriptionen（含重复）:\n",
            "                                        Descriptionen  Count\n",
            "0                                        Vimto, still     61\n",
            "1                                  Pom Bears (crisps)     52\n",
            "2                                            Pringles     18\n",
            "3                                         Cheesy mash     15\n",
            "4                                               Fanta     10\n",
            "5                             Magnum classic or white     10\n",
            "6                                            Smarties      8\n",
            "7                        Lollipops (e.g. Chupa Chups)      7\n",
            "8                                   Fox's party rings      6\n",
            "9                            Walker Sensations crisps      6\n",
            "10                                          Poppadums      5\n",
            "11                                         Fanta Zero      4\n",
            "12                                     Malteser bunny      3\n",
            "13                                      7 up / Sprite      3\n",
            "14                                             Solero      3\n",
            "15                                       Kinder Bueno      2\n",
            "16     Yam, fried (Mach alu (Dioscorea species) vaga)      2\n",
            "17                           Lentil dahl / Tarka dahl      2\n",
            "18                                       Time Out bar      1\n",
            "19                                     Ferrero Rocher      1\n",
            "20                                     Tyrells crisps      1\n",
            "21                                             Tahini      1\n",
            "22  Yellow lentil dahl, with ghee (Dal tarka/ Dal ...      1\n",
            "23                          Red gram dahl (Arhar dal)      1\n",
            "24                                       Vimto, fizzy      1\n"
          ]
        }
      ]
    }
  ]
}