{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25be2d9f",
   "metadata": {},
   "source": [
    "# Integrated Data Pipeline Notebook\n",
    "\n",
    "**Generated:** 2025-08-29 07:04\n",
    "\n",
    "This notebook merges the four uploaded notebooks into a single, linear pipeline with clear stage headers:\n",
    "\n",
    "1. **NOVA Classification**  \n",
    "2. **Weighted Intakes**  \n",
    "3. **HFI Variables**  \n",
    "4. **Final Modelling**\n",
    "\n",
    "> Tip: Run each stage in order. If variables or file paths differ between source notebooks, you may need to tweak the handoff cells between stages (saving/loading intermediates).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Standard header (project-wide) ====\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import warnings, sys, re, json, os\n",
    "SEED = 20250829\n",
    "np.random.seed(SEED)\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "DATA = ROOT / \"data\"; REF = ROOT / \"ref\"; OUT = ROOT / \"outputs\"\n",
    "for p in [DATA, REF, OUT, OUT/\"figures\", OUT/\"tables\", OUT/\"logs\", OUT/\"intermediate\"]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Plot/save helper\n",
    "def savefig(name, ext=(\".png\",\".pdf\")):\n",
    "    for e in ext:\n",
    "        plt.savefig(OUT/\"figures\"/f\"{name}{e}\", dpi=300, bbox_inches=\"tight\")\n",
    "print(\"Paths ready:\", DATA, REF, OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0418f",
   "metadata": {},
   "source": [
    "## Stage 1 — NOVA Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a6217",
   "metadata": {},
   "source": [
    "> Handoff note: Ensure variable names and file paths from this stage align with the next stage. Consider writing intermediates to `OUT/'intermediate'` and reading them in the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a4c0b",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/NINGTANG1124/UPF-HFI/blob/main/notebooks/NOVA_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect googledrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30864e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read intake data (including Descriptionen and FoodGroupen)\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "file_path = \"/content/drive/MyDrive/UPF-HFI/Bradford_original data/1. Dietmasterfile_foodlevel_clean.xls\"\n",
    "intake_df = pd.read_excel(file_path)\n",
    "\n",
    "# Define text cleaning function\n",
    "def clean_text(col):\n",
    "    return col.astype(str).str.lower().str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "# Apply to key fields\n",
    "intake_df[\"Foodgroupen_clean\"] = clean_text(intake_df[\"Foodgroupen\"])\n",
    "intake_df[\"Descriptionen_clean\"] = clean_text(intake_df[\"Descriptionen\"])\n",
    "\n",
    "# Add a cleaning column for Subgroupcode\n",
    "intake_df[\"Subgroupcode_clean\"] = clean_text(intake_df[\"Subgroupcode\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6abd941",
   "metadata": {},
   "source": [
    "# Match foodgroups that already have NOVA1-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# att3\n",
    "att3_path = \"/content/drive/MyDrive/UPF-HFI/nova matching files/att3-excel.xlsx\"\n",
    "\n",
    "# read\n",
    "att3 = pd.read_excel(att3_path)\n",
    "\n",
    "# Cleaning group code\n",
    "att3['code_clean'] = att3['Subsidiary food group code'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Create a dictionary (including *)\n",
    "group_to_nova = dict(zip(att3['code_clean'], att3['NOVA food group']))\n",
    "\n",
    "# Clean the Subgroupcode column of the intake data\n",
    "intake_df['Subgroupcode_clean'] = intake_df['Subgroupcode'].astype(str).str.upper().str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map group code to NOVA_step1\n",
    "intake_df['NOVA_step1'] = intake_df['Subgroupcode_clean'].map(group_to_nova)\n",
    "\n",
    "# Add matching description column\n",
    "intake_df['match_reason_step1'] = intake_df['NOVA_step1'].apply(\n",
    "    lambda x: 'group code match' if str(x).isdigit() else None\n",
    ")\n",
    "\n",
    "# Matching Preview\n",
    "intake_df['NOVA_step1'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfcde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep key columns to export check files\n",
    "cols_to_export = [\n",
    "    'Descriptionen', 'Foodgroupen',\n",
    "    'Subgroupcode', 'Subgroupcode_clean',\n",
    "    'NOVA_step1', 'match_reason_step1'\n",
    "]\n",
    "\n",
    "check_df = intake_df[cols_to_export].copy()\n",
    "check_df.to_excel(\"/content/drive/MyDrive/UPF-HFI/NOVA classification/outcome/step1.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8345510",
   "metadata": {},
   "source": [
    "Step 2: Manual Processing\n",
    "Processing Logic: Manual processing according to Dicken rule documentation - Lisa verification - Approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38046c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the table from step 2\n",
    "step2_path = \"/content/drive/MyDrive/UPF-HFI/NOVA classification/outcome/step2.xlsx\"\n",
    "step2_df = pd.read_excel(step2_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to extract the Descriptionen -> NOVA_step2 mapping from step2_df\n",
    "desc_to_nova2 = step2_df.set_index(\"Descriptionen\")[\"NOVA_step2\"].to_dict()\n",
    "desc_to_reason2 = step2_df.set_index(\"Descriptionen\")[\"match_reason_step2\"].to_dict()\n",
    "\n",
    "# Map directly into intake_df\n",
    "intake_df[\"NOVA_step2\"] = intake_df[\"Descriptionen\"].map(desc_to_nova2)\n",
    "intake_df[\"match_reason_step2\"] = intake_df[\"Descriptionen\"].map(desc_to_reason2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c814257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an intake24 dataset including NOVA\n",
    "intake_df['NOVA'] = intake_df['NOVA_step2'].combine_first(intake_df['NOVA_step1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c017074",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d190bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake_df['NOVA'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c207f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the entire intake table\n",
    "intake_df.to_excel(\n",
    "    \"/content/drive/MyDrive/UPF-HFI/NOVA classification/outcome/intake24_with_nova.xlsx\",\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c03218",
   "metadata": {},
   "source": [
    "## Stage 2 — Weighted Intakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a570e6",
   "metadata": {},
   "source": [
    "> Handoff note: Ensure variable names and file paths from this stage align with the next stage. Consider writing intermediates to `OUT/'intermediate'` and reading them in the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb312eee",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/NINGTANG1124/UPF-HFI/blob/main/notebooks/Weighted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22721eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect googledrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3691ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# intake24\n",
    "file_path = '/content/drive/MyDrive/UPF-HFI/NOVA classification/outcome/intake24_with_nova.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Create a helper function: return the day of the week based on the content of DAYCHANGE\n",
    "def extract_weekday(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    # Numbers (Excel dates)\n",
    "    if isinstance(value, (int, float)):\n",
    "        try:\n",
    "            date = pd.to_datetime('1899-12-30') + pd.to_timedelta(value, unit='D')\n",
    "            return date.day_name()# Return Monday to Sunday\n",
    "        except:\n",
    "            return np.nan\n",
    "    # string (try to extract the day of the week)\n",
    "    elif isinstance(value, str):\n",
    "        match = re.search(r'(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)', value)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return np.nan\n",
    "\n",
    "df['Actual_Weekday'] = df['DAYCHANGE'].apply(extract_weekday)\n",
    "\n",
    "# If Actual_Weekday is empty (indicating no DAYCHANGE information), use DAY to complete it.\n",
    "df['DAY_clean'] = df['DAY'].astype(str).str.extract(r'(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)', expand=False)\n",
    "df['Actual_Weekday'] = df['Actual_Weekday'].fillna(df['DAY_clean'])\n",
    "\n",
    "# Revised_WeekdayType\n",
    "df['Revised_WeekdayType'] = df['Actual_Weekday'].apply(\n",
    "    lambda x: 'Weekend' if x in ['Saturday', 'Sunday'] else ('Weekday' if pd.notna(x) else np.nan)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out key columns for manual inspection\n",
    "cols_to_check = [\n",
    "    'UserID_clean',\n",
    "    'DAYCHANGE',\n",
    "    'DAY',\n",
    "    'Actual_Weekday',\n",
    "    'Revised_WeekdayType'\n",
    "]\n",
    "\n",
    "check_df = df[cols_to_check].copy()\n",
    "\n",
    "# save\n",
    "check_df.to_excel(\n",
    "    '/content/drive/MyDrive/UPF-HFI/Model/outcome/check/check_weekdaytype.xlsx',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Exported to intake24_check_weekdaytype.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the complete intake24 table\n",
    "df.to_excel(\n",
    "    '/content/drive/MyDrive/UPF-HFI/Model/outcome/intake24_cleaned_with_weekdaytype.xlsx',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"The entire table intake24_cleaned_with_weekdaytype.xlsx has been exported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf457c4f",
   "metadata": {},
   "source": [
    "# Weighted_upf_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a06aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read\n",
    "file_path = \"/content/drive/MyDrive/UPF-HFI/Model/outcome/intake24_cleaned_with_weekdaytype.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# === Step 1: Type Conversion ===\n",
    "df['Revised_WeekdayType'] = df['Revised_WeekdayType'].astype(str)\n",
    "df['Energykcal'] = pd.to_numeric(df['Energykcal'], errors='coerce')\n",
    "df['NOVA'] = pd.to_numeric(df['NOVA'], errors='coerce')\n",
    "\n",
    "# === Step 2: Calculate the total energy of weekday/weekend ===\n",
    "total_kcal = df.groupby(['UserID_clean', 'Revised_WeekdayType'])['Energykcal'].sum().unstack(fill_value=0).reset_index()\n",
    "total_kcal.rename(columns={'Weekday': 'weekday_kcal', 'Weekend': 'weekend_kcal'}, inplace=True)\n",
    "\n",
    "# === Step 3: Calculate the total UPF energy of weekday/weekend (NOVA=4) ===\n",
    "upf_df = df[df['NOVA'] == 4]\n",
    "upf_kcal = upf_df.groupby(['UserID_clean', 'Revised_WeekdayType'])['Energykcal'].sum().unstack(fill_value=0).reset_index()\n",
    "upf_kcal.rename(columns={'Weekday': 'weekday_upf_kcal', 'Weekend': 'weekend_upf_kcal'}, inplace=True)\n",
    "\n",
    "# === Step 4: Merge Data ===\n",
    "merged = pd.merge(total_kcal, upf_kcal, on='UserID_clean', how='outer').fillna(0)\n",
    "\n",
    "# === Step 5: Calculate weighted total energy and weighted UPF ===\n",
    "merged['weighted_kcal'] = 5/7 * merged['weekday_kcal'] + 2/7 * merged['weekend_kcal']\n",
    "merged['weighted_upf_kcal'] = 5/7 * merged['weekday_upf_kcal'] + 2/7 * merged['weekend_upf_kcal']\n",
    "\n",
    "# === Step 6: Calculate UPF percentage ===\n",
    "merged['weighted_upf_percent'] = (merged['weighted_upf_kcal'] / merged['weighted_kcal']) * 100\n",
    "\n",
    "# === Step 7: Export ===\n",
    "merged.to_excel(\"/content/drive/MyDrive/UPF-HFI/Model/outcome/weighted_upf_percent.xlsx\", index=False)\n",
    "print(\"weighted_upf_percent exported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b643ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into intake24\n",
    "df = pd.merge(df, merged[['UserID_clean', 'weighted_upf_percent']], on='UserID_clean', how='left')\n",
    "df.to_excel(\"/content/drive/MyDrive/UPF-HFI/Model/outcome/intake24_cleaned_with_weighted_upf_percent.xlsx\", index=False)\n",
    "print(\"intake24_upf_percent exported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e5968",
   "metadata": {},
   "source": [
    "# weighted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c77ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Step 0: Import intake24 data ===\n",
    "file_path = \"/content/drive/MyDrive/UPF-HFI/Model/outcome/intake24_cleaned_with_weekdaytype.xlsx\"\n",
    "intake_df = pd.read_excel(file_path)\n",
    "\n",
    "# Convert related columns to numerical values to prevent non-numerical values from interfering with calculations\n",
    "intake_df[\"Energykcal\"] = pd.to_numeric(intake_df[\"Energykcal\"], errors=\"coerce\")\n",
    "intake_df[\"NOVA\"] = pd.to_numeric(intake_df[\"NOVA\"], errors=\"coerce\")\n",
    "\n",
    "# === Step 1: Mark NOVA weights ===\n",
    "nova_weights = {1: 1, 2: 2, 3: 3, 4: 4}\n",
    "intake_df[\"UPF_weight\"] = intake_df[\"NOVA\"].map(nova_weights)\n",
    "\n",
    "# === Step 2: Calculate the weighted score for each record ===\n",
    "intake_df[\"weighted_score\"] = intake_df[\"Energykcal\"] * intake_df[\"UPF_weight\"]\n",
    "\n",
    "# === Step 3: Group by UserID_clean and Revised_WeekdayType, calculate total energy and score ===\n",
    "agg_df = (\n",
    "    intake_df\n",
    "    .groupby([\"UserID_clean\", \"Revised_WeekdayType\"])\n",
    "    .agg(\n",
    "        total_kcal=(\"Energykcal\", \"sum\"),\n",
    "        weighted_kcal=(\"weighted_score\", \"sum\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# === Step 4: Calculate the weighted score ratio for each group ===\n",
    "agg_df[\"score_ratio\"] = agg_df[\"weighted_kcal\"] / agg_df[\"total_kcal\"]\n",
    "\n",
    "# === Step 5: Split into weekday and weekend and take weighted average ===\n",
    "weekday = agg_df[agg_df[\"Revised_WeekdayType\"] == \"Weekday\"][[\"UserID_clean\", \"score_ratio\"]].rename(columns={\"score_ratio\": \"weekday_score\"})\n",
    "weekend = agg_df[agg_df[\"Revised_WeekdayType\"] == \"Weekend\"][[\"UserID_clean\", \"score_ratio\"]].rename(columns={\"score_ratio\": \"weekend_score\"})\n",
    "\n",
    "# Merge two subtables and fill missing values with 0\n",
    "merged = pd.merge(weekday, weekend, on=\"UserID_clean\", how=\"outer\").fillna(0)\n",
    "\n",
    "# === Step 6: Weighted average synthesis of the final score (5:2 weighting) ===\n",
    "merged[\"weighted_upf_score\"] = merged[\"weekday_score\"] * 5/7 + merged[\"weekend_score\"] * 2/7\n",
    "\n",
    "# === Step 7: Export results ===\n",
    "output_path = \"/content/drive/MyDrive/UPF-HFI/Model/outcome/weighted_upf_score.xlsx\"\n",
    "merged.to_excel(output_path, index=False)\n",
    "\n",
    "print(\"Successfully exported weighted_upf_score.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a13d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into intake24\n",
    "df = pd.merge(df, merged[['UserID_clean', 'weighted_upf_score']], on='UserID_clean', how='left')\n",
    "df.to_excel(\"/content/drive/MyDrive/UPF-HFI/Model/outcome/intake24_cleaned_with_weighted_upf_score.xlsx\", index=False)\n",
    "print(\"intake24_weighted_score successfully exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5adc6",
   "metadata": {},
   "source": [
    "# Select a dependent variable that is sensitive to HFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# survey\n",
    "survey_path = \"/content/drive/MyDrive/UPF-HFI/Bradford_original data/4. SurveyMasterfile_clean.xlsx\"\n",
    "survey_df = pd.read_excel(survey_path)\n",
    "\n",
    "hfi_columns = ['insecurity1', 'insecurity2', 'insecurity3', 'insecurity3a', 'insecurity4', 'insecurity5']\n",
    "\n",
    "# Scoring function: 1, 2 => 1 point, the rest (3, 4) are 0 points\n",
    "def score_response(x):\n",
    "    return 1 if x in [1, 2] else 0\n",
    "\n",
    "# Create HFI total score\n",
    "survey_df['HFI_score'] = survey_df[hfi_columns].applymap(score_response).sum(axis=1)\n",
    "\n",
    "# Create a binary variable (score >= 2 for food insecurity)\n",
    "survey_df['HFI_binary'] = survey_df['HFI_score'].apply(lambda x: 1 if x >= 2 else 0)\n",
    "\n",
    "# Keep useful columns\n",
    "hfi_df = survey_df[['UserID_clean', 'HFI_score', 'HFI_binary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPF\n",
    "upf_percent = pd.read_excel(\"/content/drive/MyDrive/UPF-HFI/Model/outcome/weighted_upf_percent.xlsx\")\n",
    "upf_score = pd.read_excel(\"/content/drive/MyDrive/UPF-HFI/Model/outcome/weighted_upf_score.xlsx\")\n",
    "\n",
    "# Merge two UPF metrics\n",
    "upf = pd.merge(upf_percent, upf_score, on=\"UserID_clean\", how=\"inner\")\n",
    "\n",
    "# Merge HFI indicator\n",
    "merged = pd.merge(upf, hfi_df, on=\"UserID_clean\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5438b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Percentage box plot\n",
    "sns.boxplot(data=merged, x=\"HFI_binary\", y=\"weighted_upf_percent\", ax=axes[0])\n",
    "axes[0].set_title(\"UPF Percent vs HFI\")\n",
    "\n",
    "# Intensity score box plot\n",
    "sns.boxplot(data=merged, x=\"HFI_binary\", y=\"weighted_upf_score\", ax=axes[1])\n",
    "axes[1].set_title(\"UPF Score vs HFI\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b88ed8",
   "metadata": {},
   "source": [
    "# Mann-Whitney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Group by HFI\n",
    "group0 = merged[merged['HFI_binary'] == 0]\n",
    "group1 = merged[merged['HFI_binary'] == 1]\n",
    "\n",
    "# Statistical tests\n",
    "p_percent = mannwhitneyu(group0['weighted_upf_percent'], group1['weighted_upf_percent']).pvalue\n",
    "p_score = mannwhitneyu(group0['weighted_upf_score'], group1['weighted_upf_score']).pvalue\n",
    "\n",
    "print(f\"UPF percentage p-value: {p_percent:.4f}\")\n",
    "print(f\"UPF strength score p-value: {p_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca39e5",
   "metadata": {},
   "source": [
    "## Stage 3 — HFI Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af7bb5d",
   "metadata": {},
   "source": [
    "> Handoff note: Ensure variable names and file paths from this stage align with the next stage. Consider writing intermediates to `OUT/'intermediate'` and reading them in the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd43c784",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/NINGTANG1124/UPF-HFI/blob/main/notebooks/HFI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9059dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to googledrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7dd85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# survey data\n",
    "survey_path = \"/content/drive/MyDrive/UPF-HFI/Bradford_original data/4. SurveyMasterfile_clean.xlsx\"\n",
    "df = pd.read_excel(survey_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea0fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Input validation and type standardization\n",
    "import numpy as np\n",
    "\n",
    "# Required columns\n",
    "REQ = [\"UserID_clean\",\"insecurity1\",\"insecurity2\",\"insecurity3\",\"insecurity3a\",\"insecurity4\",\"insecurity5\"]\n",
    "missing = [c for c in REQ if c not in df.columns]\n",
    "assert not missing, f\"Missing required columns:{missing}\"\n",
    "\n",
    "# Type unification (preserving NaN)\n",
    "hfi_cols = [\"insecurity1\",\"insecurity2\",\"insecurity3\",\"insecurity3a\",\"insecurity4\",\"insecurity5\"]\n",
    "df[hfi_cols] = df[hfi_cols].apply(pd.to_numeric, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Encoding range check\n",
    "valid_vals = {\n",
    "    \"insecurity1\": {1,2,3,4,5},   # HH3\n",
    "    \"insecurity2\": {1,2,3,4,5},   # HH4\n",
    "    \"insecurity3\": {1,2,3},       # AD1\n",
    "    \"insecurity3a\":{1,2,3,4},     # AD1a\n",
    "    \"insecurity4\": {1,2,3},       # AD2\n",
    "    \"insecurity5\": {1,2,3},       # AD3\n",
    "}\n",
    "viol = {}\n",
    "for c, ok in valid_vals.items():\n",
    "    bad_mask = df[c].notna() & ~df[c].isin(ok)\n",
    "    if bad_mask.any():\n",
    "        viol[c] = sorted(df.loc[bad_mask, c].unique().tolist())\n",
    "\n",
    "print(\"Out of range encoding:\", viol if viol else \"{}\")\n",
    "\n",
    "# Quickly view the distribution of values in each column (including NaN)\n",
    "for c in hfi_cols:\n",
    "    print(f\"\\n== {c} ==\")\n",
    "    print(df[c].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Map each question into a score (1/0/NaN)\n",
    "# Mapping rules\n",
    "map_hh_often_sometimes = {1:1, 2:1, 3:0, 4:np.nan, 5:np.nan}  # HH3/HH4\n",
    "map_yes_no              = {1:1, 2:0, 3:np.nan}                 # AD1/AD2/AD3\n",
    "map_ad1a                = {1:1, 2:1, 3:0, 4:np.nan}            # AD1a\n",
    "\n",
    "# HH3/HH4\n",
    "df[\"hh3_score\"] = df[\"insecurity1\"].map(map_hh_often_sometimes)\n",
    "df[\"hh4_score\"] = df[\"insecurity2\"].map(map_hh_often_sometimes)\n",
    "\n",
    "# AD1/AD2/AD3\n",
    "df[\"ad1_score\"] = df[\"insecurity3\"].map(map_yes_no)\n",
    "df[\"ad2_score\"] = df[\"insecurity4\"].map(map_yes_no)\n",
    "df[\"ad3_score\"] = df[\"insecurity5\"].map(map_yes_no)\n",
    "\n",
    "# AD1a: only AD1=Yes(1) is mapped; AD1=No(0) → 0; AD1 missing → NaN\n",
    "df[\"ad1a_score\"] = np.where(\n",
    "    df[\"ad1_score\"].eq(1),\n",
    "    df[\"insecurity3a\"].map(map_ad1a),\n",
    "    np.where(df[\"ad1_score\"].eq(0), 0, np.nan)\n",
    ").astype(float)\n",
    "\n",
    "# Check the value after mapping\n",
    "score_cols = [\"hh3_score\",\"hh4_score\",\"ad1_score\",\"ad1a_score\",\"ad2_score\",\"ad3_score\"]\n",
    "for c in score_cols:\n",
    "    print(f\"\\n{c} Value distribution:\")\n",
    "    print(df[c].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Raw Total Score + “Minimum Valid Answer” Rule\n",
    "score_cols = [\"hh3_score\",\"hh4_score\",\"ad1_score\",\"ad1a_score\",\"ad2_score\",\"ad3_score\"]\n",
    "\n",
    "# 1) Number of valid answers\n",
    "df[\"HFI_valid_items\"] = df[score_cols].notna().sum(axis=1)\n",
    "\n",
    "# 2) Total score (returns NaN if less than 5 items)\n",
    "df[\"HFI_raw_score\"] = df[score_cols].sum(axis=1, min_count=5)\n",
    "\n",
    "# 3) Validity: integers from 0..6 (or NaN)\n",
    "mask = df[\"HFI_raw_score\"].notna()\n",
    "assert df.loc[mask, \"HFI_raw_score\"].between(0,6).all(), \"HFI_raw_score 越界\"\n",
    "df.loc[mask, \"HFI_raw_score\"] = df.loc[mask, \"HFI_raw_score\"].round().astype(int)\n",
    "\n",
    "# 4) Quick View\n",
    "print(\"Number of people with valid answers <5/percentage:\",\n",
    "      int((df['HFI_valid_items']<5).sum()),\n",
    "      (df['HFI_valid_items']<5).mean().round(3))\n",
    "print(df[\"HFI_valid_items\"].value_counts().sort_index())\n",
    "print(df[\"HFI_raw_score\"].describe())\n",
    "print(df[\"HFI_raw_score\"].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Categorical variables\n",
    "\n",
    "# Three classification functions\n",
    "def classify_hfi(score):\n",
    "    if pd.isna(score):\n",
    "        return None\n",
    "    if score <= 1:\n",
    "        return \"Food secure\"\n",
    "    elif 2 <= score <= 4:\n",
    "        return \"Low food security\"\n",
    "    else:\n",
    "        return \"Very low food security\"\n",
    "\n",
    "df[\"HFI_category\"] = df[\"HFI_raw_score\"].apply(classify_hfi)\n",
    "\n",
    "# Set ordered categories\n",
    "cat_dtype = pd.api.types.CategoricalDtype(\n",
    "    categories=[\"Food secure\",\"Low food security\",\"Very low food security\"],\n",
    "    ordered=True\n",
    ")\n",
    "df[\"HFI_category\"] = df[\"HFI_category\"].astype(cat_dtype)\n",
    "\n",
    "# Binary classification (0/1/NaN)\n",
    "df[\"HFI_binary\"] = df[\"HFI_raw_score\"].apply(\n",
    "    lambda x: (1 if (pd.notna(x) and x>=2) else (0 if (pd.notna(x) and x<=1) else np.nan))\n",
    ").astype(\"Int64\")\n",
    "\n",
    "# Check the distribution\n",
    "print(\"Three-category distribution:\\n\", df[\"HFI_category\"].value_counts(dropna=False))\n",
    "print(\"\\nBinary distribution:\\n\", df[\"HFI_binary\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe7b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "output_path = \"/content/drive/MyDrive/UPF-HFI/Model/outcome/survey_with_HFI.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Saved to:{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d40cf",
   "metadata": {},
   "source": [
    "## Stage 4 — Final Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9df32b",
   "metadata": {},
   "source": [
    "> Handoff note: Ensure variable names and file paths from this stage align with the next stage. Consider writing intermediates to `OUT/'intermediate'` and reading them in the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f024d8",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/NINGTANG1124/UPF-HFI/blob/main/notebooks/final_model_chinese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f90184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to googledrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97248d1c",
   "metadata": {},
   "source": [
    "## 合并 HFI 到建模数据表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 120)\n",
    "\n",
    "# 读取\n",
    "survey_df = pd.read_excel( \"/content/drive/MyDrive/UPF-HFI/Model/outcome/survey_with_HFI.xlsx\")\n",
    "df_upf    = pd.read_excel(\"/content/drive/MyDrive/UPF-HFI/Model/outcome/weighted_upf_percent.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8312c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 2. 先检查 ====\n",
    "issues = []\n",
    "\n",
    "# 检查关键列\n",
    "for col in [\"UserID_clean\", \"HFI_binary\", \"HFI_raw_score\"]:\n",
    "    if col not in survey_df.columns:\n",
    "        issues.append(f\"survey 缺少 {col}\")\n",
    "\n",
    "for col in [\"UserID_clean\", \"weighted_upf_percent\"]:\n",
    "    if col not in df_upf.columns:\n",
    "        issues.append(f\"upf 缺少 {col}\")\n",
    "\n",
    "# 检查 ID 是否有缺失/重复\n",
    "for name, df in [(\"survey\", survey_df), (\"upf\", df_upf)]:\n",
    "    n_na = df[\"UserID_clean\"].isna().sum()\n",
    "    n_dup = df[\"UserID_clean\"].duplicated().sum()\n",
    "    if n_na:  issues.append(f\"{name} 的 UserID_clean 有 {n_na} 个缺失\")\n",
    "    if n_dup: issues.append(f\"{name} 的 UserID_clean 有 {n_dup} 个重复\")\n",
    "\n",
    "# 检查 upf_percent 是否数值\n",
    "bad_upf = pd.to_numeric(df_upf[\"weighted_upf_percent\"], errors=\"coerce\").isna().sum()\n",
    "if bad_upf:\n",
    "    issues.append(f\"weighted_upf_percent 有 {bad_upf} 个非数值/缺失\")\n",
    "\n",
    "# 打印检查结果\n",
    "if issues:\n",
    "    print(\"检查发现以下问题：\")\n",
    "    for i, msg in enumerate(issues, 1):\n",
    "        print(f\"{i}. {msg}\")\n",
    "else:\n",
    "    print(\"没有发现问题，可以直接处理\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf13832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 检查关键变量缺失情况 ====\n",
    "check_cols = [\"HFI_binary\", \"weighted_upf_percent\"]\n",
    "missing_info = {}\n",
    "\n",
    "for col in check_cols:\n",
    "    if col not in survey_df.columns and col not in df_upf.columns:\n",
    "        print(f\"数据中找不到列: {col}\")\n",
    "        continue\n",
    "\n",
    "    if col in survey_df.columns:\n",
    "        n_miss = survey_df[col].isna().sum()\n",
    "        missing_info[col] = n_miss\n",
    "        print(f\"{col} 在 survey_df 中缺失 {n_miss} 行\")\n",
    "\n",
    "    if col in df_upf.columns:\n",
    "        n_miss = df_upf[col].isna().sum()\n",
    "        missing_info[col] = n_miss\n",
    "        print(f\"{col} 在 upf_df 中缺失 {n_miss} 行\")\n",
    "\n",
    "# ==== 判断是否需要处理 ====\n",
    "if all(v == 0 for v in missing_info.values()):\n",
    "    print(\"两列都没有缺失，可以直接进入合并/建模\")\n",
    "else:\n",
    "    print(\"存在缺失，需要处理后再合并\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 合并 survey 和 upf\n",
    "df_model = pd.merge(survey_df, df_upf, on=\"UserID_clean\", how=\"inner\")\n",
    "\n",
    "# 2. 删掉 HFI_binary 缺失的\n",
    "df_model = df_model.dropna(subset=[\"HFI_binary\"])\n",
    "\n",
    "print(\"合并后样本量：\", len(df_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出合并后的数据\n",
    "df_model.to_excel(\"/content/drive/MyDrive/UPF-HFI/Model/outcome/merged_model.xlsx\", index=False)\n",
    "\n",
    "print(\"已导出到 /content/drive/MyDrive/UPF-HFI/Model/outcome/merged_model.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1215edee",
   "metadata": {},
   "source": [
    "# 0. 准备：工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c719a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# ============  Imports & Settings  ============\n",
    "# =============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ----- 正态分位工具（SciPy优先，内置做后备） -----\n",
    "# ------------------------------------------------\n",
    "try:\n",
    "    from scipy.stats import norm as _norm\n",
    "    def _zcrit(alpha=0.05):   # 双侧 alpha -> z*\n",
    "        return _norm.ppf(1 - alpha/2)\n",
    "    def _cdf(x):\n",
    "        return _norm.cdf(x)\n",
    "except Exception:\n",
    "    from statistics import NormalDist\n",
    "    _nd = NormalDist()\n",
    "    def _zcrit(alpha=0.05):\n",
    "        return _nd.inv_cdf(1 - alpha/2)\n",
    "    def _cdf(x):\n",
    "        return _nd.cdf(x)\n",
    "\n",
    "# --------------------------------\n",
    "# ----- OLS 拟合（稳健方差） -----\n",
    "# --------------------------------\n",
    "def fit_ols(formula, data, cov_type=\"HC3\"):\n",
    "    \"\"\"\n",
    "    OLS 拟合 + 稳健标准误（默认 HC3）\n",
    "    返回：robust 结果对象\n",
    "    \"\"\"\n",
    "    model = smf.ols(formula, data=data).fit()\n",
    "    if cov_type:\n",
    "        model = model.get_robustcov_results(cov_type=cov_type)\n",
    "    return model\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ----- 统一结果取值（兼容 ndarray / pandas.Series） -----\n",
    "# ----------------------------------------------------------\n",
    "def _names_params_bse_p(res):\n",
    "    \"\"\"\n",
    "    统一把 params/bse/pvalues 取成 Series，并带上模型列名。\n",
    "    无论 statsmodels 返回 ndarray 还是 Series 都能兼容。\n",
    "    \"\"\"\n",
    "    names   = list(res.model.exog_names)\n",
    "    params  = pd.Series(np.asarray(res.params).ravel(),  index=names)\n",
    "    bse     = pd.Series(np.asarray(res.bse).ravel(),     index=names)\n",
    "    pvalues = pd.Series(np.asarray(res.pvalues).ravel(), index=names)\n",
    "    return names, params, bse, pvalues\n",
    "\n",
    "def get_term(result, term, default=np.nan):\n",
    "    \"\"\"安全获取某个项的系数（不存在则给默认值）。\"\"\"\n",
    "    _, params, _, _ = _names_params_bse_p(result)\n",
    "    return params.get(term, default)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ----- 置信区间 / 线性组合（边际效应）/ 交互整体检验 -----\n",
    "# ------------------------------------------------------------\n",
    "def coef_ci(result, term, alpha=0.05):\n",
    "    \"\"\"\n",
    "    取指定项的点估计、95%CI 和 p 值；若项不存在返回 None\n",
    "    \"\"\"\n",
    "    names, params, bse, pvalues = _names_params_bse_p(result)\n",
    "    if term not in params.index:\n",
    "        return None\n",
    "    est = params[term]\n",
    "    se  = bse[term]\n",
    "    z   = _zcrit(alpha)\n",
    "    ci  = (est - z*se, est + z*se)\n",
    "    return est, ci, pvalues[term]\n",
    "\n",
    "def linear_combo(result, combo):\n",
    "    \"\"\"\n",
    "    线性组合估计（用于交互项下的 HFI 边际效应等）\n",
    "    combo 例子：\n",
    "      {'HFI_binary':1,\n",
    "       \"HFI_binary:C(ethn_participant, Treatment(reference='White'))[T.Asian]\":1}\n",
    "    返回：(点估计, 95%CI, p 值)\n",
    "    \"\"\"\n",
    "    names, params, _, _ = _names_params_bse_p(result)\n",
    "    cov = np.asarray(result.cov_params())\n",
    "    w = np.zeros(len(names))\n",
    "    for k, v in combo.items():\n",
    "        if k in names:\n",
    "            w[names.index(k)] = v\n",
    "    est = float(np.dot(w, params.values))\n",
    "    var = float(np.dot(w, np.dot(cov, w)))\n",
    "    se  = np.sqrt(var)\n",
    "    if se > 0:\n",
    "        zval = est / se\n",
    "        ci   = (est - _zcrit(0.05)*se, est + _zcrit(0.05)*se)\n",
    "        p    = 2 * (1 - _cdf(abs(zval)))\n",
    "    else:\n",
    "        ci, p = (np.nan, np.nan), np.nan\n",
    "    return est, ci, p\n",
    "\n",
    "def joint_test_interaction(result, prefix):\n",
    "    \"\"\"\n",
    "    交互整体检验：检验所有以 prefix 开头的系数为 0\n",
    "    例如 prefix = \"HFI_binary:C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \"\"\"\n",
    "    names, params, _, _ = _names_params_bse_p(result)\n",
    "    terms = [nm for nm in names if nm.startswith(prefix)]\n",
    "    if not terms:\n",
    "        return np.nan\n",
    "    R = np.zeros((len(terms), len(names)))\n",
    "    for i, t in enumerate(terms):\n",
    "        R[i, names.index(t)] = 1.0\n",
    "    ft = result.f_test(R)\n",
    "    return float(np.asarray(ft.pvalue))\n",
    "\n",
    "# -------------------------\n",
    "# ----- 绘图（coefplot） -----\n",
    "# -------------------------\n",
    "def coefplot(ax, labels, est, lo, hi, title, xlabel=\"Coefficient (pp)\"):\n",
    "    \"\"\"\n",
    "    系数图（点 + 95%CI）\n",
    "    labels: y 轴标签列表\n",
    "    est/lo/hi: 对应点估计与置信区间上下界的等长列表\n",
    "    \"\"\"\n",
    "    y = np.arange(len(labels))\n",
    "    ax.errorbar(est, y, xerr=[est-lo, hi-est], fmt='o', capsize=4)\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.axvline(0, linestyle='--', linewidth=1)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_title(title)\n",
    "    ax.invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb68ad",
   "metadata": {},
   "source": [
    "# 1. 族裔合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"ethn_participant\" in df_model.columns, \"未找到 ethn_participant 列（应为1–9编码）。\"\n",
    "\n",
    "# 备份原始九类编码\n",
    "df_model = df_model.copy()\n",
    "df_model.rename(columns={\"ethn_participant\": \"ethn_participant_raw\"}, inplace=True)\n",
    "\n",
    "# 强制转换为整数编码，兼容缺失\n",
    "codes = pd.to_numeric(df_model[\"ethn_participant_raw\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# 映射到三大类\n",
    "code_to_ethn3 = {\n",
    "    1: \"White\", 2: \"White\",\n",
    "    3: \"Asian\", 4: \"Asian\", 5: \"Asian\", 6: \"Asian\",\n",
    "    7: \"All other ethnicities\", 8: \"All other ethnicities\", 9: \"All other ethnicities\"\n",
    "}\n",
    "df_model[\"ethn_participant\"] = codes.map(code_to_ethn3)\n",
    "\n",
    "# 设定为分类变量并固定顺序（参考组=White）\n",
    "df_model[\"ethn_participant\"] = pd.Categorical(\n",
    "    df_model[\"ethn_participant\"],\n",
    "    categories=[\"White\", \"Asian\", \"All other ethnicities\"],\n",
    "    ordered=False\n",
    ")\n",
    "\n",
    "# 健诊：展示原始九类分布与三类分布\n",
    "print(\"原始九类（1–9）计数：\")\n",
    "print(codes.value_counts(dropna=False).sort_index())\n",
    "print(\"\\n合并后三类计数（含缺失）：\")\n",
    "print(df_model[\"ethn_participant\"].value_counts(dropna=False))\n",
    "\n",
    "# 若存在异常编码（非1–9），提示但不报错\n",
    "invalid = codes[~codes.isin(list(range(1,10))) & codes.notna()].unique()\n",
    "if len(invalid) > 0:\n",
    "    print(f\"\\n[提示] 发现非常规族裔编码：{list(invalid)}；已在合并后记为缺失。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2305d79",
   "metadata": {},
   "source": [
    "# 2. Core-A 主模型（不含 SES）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ab233",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOME = \"weighted_upf_percent\"\n",
    "assert OUTCOME in df_model.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 2. Core-A 主模型（不含 SES）— 精简稳健版 =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "coreA_cols = [OUTCOME, \"HFI_binary\", \"age_participant\",\n",
    "              \"gender_participant\", \"ethn_participant\", \"child_numbers\"]\n",
    "\n",
    "# 1) 构造分析集 + 基本体检\n",
    "df_all = df_model[coreA_cols].copy()\n",
    "\n",
    "# 目标变量合法范围（0–100）\n",
    "y_bad_mask = ~pd.to_numeric(df_all[OUTCOME], errors=\"coerce\").between(0, 100)\n",
    "bad = int(y_bad_mask.sum())\n",
    "if bad:\n",
    "    print(f\"[警告] {OUTCOME} 有 {bad} 条不在 0–100，将置为缺失。\")\n",
    "    df_all.loc[y_bad_mask, OUTCOME] = np.nan\n",
    "\n",
    "# 数值化（防止字符串混入）\n",
    "for col in [\"child_numbers\", \"age_participant\", \"HFI_binary\"]:\n",
    "    df_all[col] = pd.to_numeric(df_all[col], errors=\"coerce\")\n",
    "\n",
    "# 丢失追踪\n",
    "mask_keep = df_all.notna().all(axis=1)\n",
    "dat_coreA   = df_all.loc[mask_keep].copy()\n",
    "dropped_coreA = df_all.loc[~mask_keep].copy()\n",
    "\n",
    "print(f\"Core-A 保留样本: {len(dat_coreA)}\")\n",
    "print(f\"Core-A 剔除样本: {len(dropped_coreA)}\")\n",
    "print(\"\\n每个变量的缺失计数（降序）:\")\n",
    "print(df_all.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "# 2) 指定基准并拟合（HC3 稳健 SE）\n",
    "form_coreA = (\n",
    "    f\"{OUTCOME} ~ HFI_binary\"\n",
    "    \" + age_participant\"\n",
    "    \" + C(gender_participant, Treatment(reference=1))\"            # 1 = Male 作为基准\n",
    "    \" + C(ethn_participant, Treatment(reference='White'))\"         # White 作为基准\n",
    "    \" + child_numbers\"\n",
    ")\n",
    "\n",
    "res_coreA = fit_ols(form_coreA, dat_coreA, cov_type=\"HC3\")\n",
    "\n",
    "# 3) 报告主效应（HFI）\n",
    "est, ci, p = coef_ci(res_coreA, \"HFI_binary\")\n",
    "print(f\"\\nCore-A: HFI vs secure = {est:.2f} pp (95%CI {ci[0]:.2f}, {ci[1]:.2f}); p={p:.3f}\")\n",
    "\n",
    "# （可选）partial R²\n",
    "try:\n",
    "    r2_hfi, p_wald = partial_R2(res_coreA, \"HFI_binary\")\n",
    "    print(f\"partial R² (HFI): {r2_hfi:.3f}（Wald p={p_wald:.3f}）\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeda6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# ============ 美化版 Forest Plot — Core-A (HC3) ==========\n",
    "# =========================================================\n",
    "import re, textwrap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "show_age_per_sd = True      # 若想显示“每 +1SD”，置 True；否则按每年\n",
    "gender_map = {1:\"Male\", 2:\"Female\", 3:\"Other\"}  # 你给定的性别口径\n",
    "\n",
    "# 取系数/区间/显著性\n",
    "try:\n",
    "    z = _zcrit(0.05)\n",
    "except NameError:\n",
    "    z = 1.95996398454\n",
    "\n",
    "names  = list(res_coreA.model.exog_names)\n",
    "params = pd.Series(np.asarray(res_coreA.params).ravel(),  index=names)\n",
    "bse    = pd.Series(np.asarray(res_coreA.bse).ravel(),     index=names)\n",
    "pvals  = pd.Series(np.asarray(res_coreA.pvalues).ravel(), index=names)\n",
    "\n",
    "# ==== 修改①：构建 df 时加入 se 列（后面要缩放） ====\n",
    "df = pd.DataFrame({\n",
    "    \"term\": names,\n",
    "    \"b\":   params.values,\n",
    "    \"se\":  bse.values,                                # <-- 新增\n",
    "    \"lo\":  (params - z*bse).values,\n",
    "    \"hi\":  (params + z*bse).values,\n",
    "    \"p\":   pvals.values\n",
    "})\n",
    "\n",
    "# 仅保留关心的项\n",
    "keep = (\n",
    "    (df.term==\"HFI_binary\") |\n",
    "    (df.term==\"age_participant\") |\n",
    "    (df.term==\"child_numbers\") |\n",
    "    (df.term.str.startswith(\"C(gender_participant\")) |\n",
    "    (df.term.str.startswith(\"C(ethn_participant\"))\n",
    ")\n",
    "df = df.loc[keep].copy()\n",
    "\n",
    "# ==== 修改②：年龄换算为 +1SD（连同 se 一起缩放） ====\n",
    "if show_age_per_sd and \"age_participant\" in df.term.values:\n",
    "    sd_age = float(dat_coreA[\"age_participant\"].astype(float).std())\n",
    "    idx = df.term==\"age_participant\"\n",
    "    df.loc[idx, [\"b\",\"lo\",\"hi\",\"se\"]] = (\n",
    "        df.loc[idx, [\"b\",\"lo\",\"hi\",\"se\"]].to_numpy() * sd_age\n",
    "    )\n",
    "    age_label = f\"Age (+1 SD = {sd_age:.2f}y)\"\n",
    "else:\n",
    "    age_label = \"Age (per year)\"\n",
    "\n",
    "# ---------- 2) 生成可读标签 ----------\n",
    "def label(term: str) -> str:\n",
    "    if term==\"HFI_binary\": return \"HFI: Insecure vs Secure\"\n",
    "    if term==\"age_participant\": return age_label\n",
    "    if term==\"child_numbers\": return \"Children in household (per child)\"\n",
    "    mg = re.match(r\"C\\(gender_participant.*\\)\\[T\\.(.+)\\]\", term)\n",
    "    if mg:\n",
    "        code = mg.group(1)\n",
    "        name = gender_map.get(code, gender_map.get(int(code), code))\n",
    "        return f\"Gender: {name} (vs Male)\"\n",
    "    me = re.match(r\"C\\(ethn_participant.*\\)\\[T\\.(.+)\\]\", term)\n",
    "    if me:\n",
    "        v = me.group(1).strip()\n",
    "        return f\"Ethnicity: {v} (vs White)\"\n",
    "    return term\n",
    "\n",
    "df[\"label\"] = df[\"term\"].apply(label)\n",
    "df[\"label_wrapped\"] = df[\"label\"].apply(lambda s: textwrap.fill(s, width=34))\n",
    "\n",
    "# 排序：HFI → Age → Children → Gender → Ethnicity\n",
    "order = [\"HFI_binary\",\"age_participant\",\"child_numbers\"]\n",
    "order += sorted(df.loc[df.term.str.startswith(\"C(gender_participant\"),\"term\"])\n",
    "order += sorted(df.loc[df.term.str.startswith(\"C(ethn_participant\"),\"term\"])\n",
    "df = df.set_index(\"term\").loc[order].reset_index()\n",
    "\n",
    "# ---------- 3) 绘图（粗横条 + 显著=实心/不显著=空心） ----------\n",
    "labels = df[\"label_wrapped\"].tolist()\n",
    "# 取数组\n",
    "b, lo, hi, p = map(np.asarray, [df[\"b\"], df[\"lo\"], df[\"hi\"], df[\"p\"]])\n",
    "se = np.asarray(df[\"se\"])   # ← 新增：用于显示 SE（已随 +1SD 同步缩放）\n",
    "\n",
    "# ==== 修改③：统一用 ypos，不再用 y ====\n",
    "ypos = np.arange(len(labels))\n",
    "sig = p < 0.05\n",
    "\n",
    "fig_h = 3.6 + 0.35*len(labels)\n",
    "fig, ax = plt.subplots(figsize=(9.5, fig_h), dpi=160)\n",
    "\n",
    "# 自适应范围 + 0线/负区背景\n",
    "pad = 1.05 * max(abs(np.nanmin(lo)), abs(np.nanmax(hi)))\n",
    "ax.set_xlim(-pad, pad)\n",
    "ax.axvline(0, ls=(0,(3,3)), lw=1.2, zorder=1)\n",
    "ax.axvspan(-pad, 0, color=\"0.96\", zorder=0)\n",
    "\n",
    "# 95%CI 粗横条 + 点（显著实心/不显著空心）\n",
    "for yi, (loi, hii) in enumerate(zip(lo, hi)):\n",
    "    ax.hlines(yi, xmin=loi, xmax=hii, lw=4.0, zorder=2)\n",
    "ax.plot(b[sig],  ypos[sig],  \"o\", ms=8, zorder=3)\n",
    "ax.plot(b[~sig], ypos[~sig], \"o\", ms=8, mfc=\"none\", zorder=3)\n",
    "\n",
    "# 轴与标签\n",
    "ax.set_yticks(ypos); ax.set_yticklabels(labels, fontsize=11)\n",
    "ax.set_xlabel(\"Adjusted difference in UPF% (pp)\", fontsize=11)\n",
    "\n",
    "# 右侧标注“点估计 [CI]”\n",
    "def stars(pv): return \"***\" if pv<1e-3 else \"**\" if pv<1e-2 else \"*\" if pv<0.05 else \"\"\n",
    "for yi, (bi, loi, hii, pi, sei) in enumerate(zip(b, lo, hi, p, se)):   # ← 加 se\n",
    "    ax.text(1.01, yi, f\"{bi:.2f} [{loi:.2f}, {hii:.2f}]; SE={sei:.2f} {stars(pi)}\",\n",
    "            transform=ax.get_yaxis_transform(), ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "# 高亮 HFI 行\n",
    "if \"HFI_binary\" in df[\"term\"].values:\n",
    "    hfi_idx = int(np.where(df[\"term\"].values==\"HFI_binary\")[0])\n",
    "    ax.axhspan(hfi_idx-0.45, hfi_idx+0.45, alpha=0.06, zorder=0)\n",
    "    ax.get_yticklabels()[hfi_idx].set_fontweight(\"bold\")\n",
    "\n",
    "ax.set_title(\"Forest plot — Core-A coefficients (HC3)\", loc=\"left\", fontsize=15, weight=\"bold\", pad=6)\n",
    "fig.text(0.12, 0.02, f\"OLS (HC3); N={len(dat_coreA)}; refs: Male, White, Secure\",\n",
    "         ha=\"left\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout(rect=[0.22, 0.06, 0.98, 0.95])\n",
    "\n",
    "# 保存/显示\n",
    "out = \"/content/drive/MyDrive/UPF-HFI/outcome_figure/forest_coreA.png\"   # 可改路径\n",
    "plt.savefig(out, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show(); plt.close(fig)\n",
    "print(\"Saved:\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c6196",
   "metadata": {},
   "source": [
    "# 3. Core-B 交互（HFI × 族裔三类；参考White）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c08131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3. Core-B 交互（HFI × 族裔三类；参考 White）— 稳健版 =====\n",
    "need_cols_B = [OUTCOME, \"HFI_binary\", \"age_participant\",\n",
    "               \"gender_participant\", \"ethn_participant\", \"child_numbers\"]\n",
    "\n",
    "dat_coreB = df_model[need_cols_B].copy()\n",
    "for col in [\"age_participant\", \"child_numbers\", \"HFI_binary\"]:\n",
    "    dat_coreB[col] = pd.to_numeric(dat_coreB[col], errors=\"coerce\")\n",
    "dat_coreB = dat_coreB.dropna().copy()\n",
    "\n",
    "print(f\"Core-B 样本量: {len(dat_coreB)}\")\n",
    "\n",
    "form_coreB = (\n",
    "    f\"{OUTCOME} ~ HFI_binary\"\n",
    "    \" + age_participant\"\n",
    "    \" + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \" + child_numbers\"\n",
    "    \" + HFI_binary:C(ethn_participant, Treatment(reference='White'))\"\n",
    ")\n",
    "res_coreB = fit_ols(form_coreB, dat_coreB, cov_type=\"HC3\")\n",
    "\n",
    "groups = [\"White\", \"Asian\", \"All other ethnicities\"]\n",
    "\n",
    "def _find_inter_name(result, group_label: str):\n",
    "    \"\"\"在模型里找到对应族裔的交互项名；若不存在返回 None。\"\"\"\n",
    "    names = list(getattr(result.model, \"exog_names\", []))\n",
    "    suf = f\"[T.{group_label}]\"\n",
    "    for nm in names:\n",
    "        if nm.startswith(\"HFI_binary:C(ethn_participant\") and nm.endswith(suf):\n",
    "            return nm\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "for g in groups:\n",
    "    combo = {\"HFI_binary\": 1.0}\n",
    "    if g != \"White\":\n",
    "        key = _find_inter_name(res_coreB, g)\n",
    "        if key is None:\n",
    "            print(f\"[警告] 未找到 {g} 的交互项，可能该水平在样本中缺失。\")\n",
    "        else:\n",
    "            combo[key] = 1.0\n",
    "    est, ci, p = linear_combo(res_coreB, combo)\n",
    "    lo, hi = ci\n",
    "    rows.append({\"group\": g, \"d\": est, \"lo\": lo, \"hi\": hi, \"p\": p})\n",
    "\n",
    "effects_df = pd.DataFrame(rows)\n",
    "p_inter = joint_test_interaction(res_coreB, \"HFI_binary:C(ethn_participant, Treatment(reference='White'))\")\n",
    "\n",
    "# 漂亮地打印\n",
    "print(\"\\nEthnicity-specific ΔUPF (insecure − secure):\")\n",
    "for r in effects_df.itertuples():\n",
    "    sig = \"*\" if r.p < 0.05 else \"\"\n",
    "    print(f\"  {r.group:>22}: {r.d:+.2f} pp [{r.lo:.2f}, {r.hi:.2f}]  p={r.p:.3f}{sig}\")\n",
    "print(f\"\\nOverall interaction test: p-interaction = {p_inter:.3f}\")\n",
    "\n",
    "# 组内 n 与导出\n",
    "n_by_grp = dat_coreB.groupby(\"ethn_participant\")[\"HFI_binary\"].size()\n",
    "effects_df[\"n\"] = effects_df[\"group\"].map(n_by_grp.to_dict())\n",
    "effects_df.to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome_figure/coreB_ethnicity_effects.csv\", index=False)\n",
    "print(\"Saved: /content/drive/MyDrive/UPF-HFI/outcome_figure/coreB_ethnicity_effects.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27584faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Figure: ΔUPF by Ethnicity (HFI insecure − secure), with 95% CI ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# 若上一块已生成 effects_df，这里直接复用；否则请先运行上一块\n",
    "df_eff = effects_df.copy()\n",
    "\n",
    "# 固定顺序 & 标签加 n\n",
    "order = [\"White\", \"Asian\", \"All other ethnicities\"]\n",
    "df_eff[\"n\"] = df_eff[\"n\"].fillna(0).astype(int)\n",
    "df_eff[\"label\"] = df_eff.apply(lambda r: f\"{r['group']} (n={r['n']})\", axis=1)\n",
    "df_eff = df_eff.set_index(\"group\").loc[order].reset_index()\n",
    "\n",
    "# 画图：粗横条 + 实心/空心点；对称坐标 + 加粗零线\n",
    "ypos = np.arange(len(df_eff))\n",
    "sig  = df_eff[\"p\"].values < 0.05\n",
    "\n",
    "fig_h = 3.2 + 0.35 * len(df_eff)\n",
    "fig, ax = plt.subplots(figsize=(7.6, fig_h), dpi=180)\n",
    "\n",
    "# 95% CI 横条\n",
    "for i, (lo, hi) in enumerate(zip(df_eff[\"lo\"], df_eff[\"hi\"])):\n",
    "    ax.hlines(ypos[i], xmin=lo, xmax=hi, lw=4.0, zorder=2)\n",
    "\n",
    "# 点：显著=实心，不显著=空心\n",
    "ax.plot(df_eff[\"d\"].values[sig],  ypos[sig],  \"o\", ms=8, zorder=3)\n",
    "ax.plot(df_eff[\"d\"].values[~sig], ypos[~sig], \"o\", ms=8, mfc=\"none\", zorder=3)\n",
    "\n",
    "# 轴 & 0 线 & 对称范围\n",
    "mx = 1.05 * max(abs(df_eff[\"lo\"].min()), abs(df_eff[\"hi\"].max()))\n",
    "ax.set_xlim(-mx, mx)\n",
    "ax.axvspan(-mx, 0, color=\"0.96\", zorder=0)\n",
    "ax.axvline(0, ls=(0,(3,3)), lw=1.6)\n",
    "\n",
    "ax.set_yticks(ypos); ax.set_yticklabels(df_eff[\"label\"], fontsize=11)\n",
    "ax.set_xlabel(\"Δ UPF% (pp) — Insecure minus Secure\", fontsize=11)\n",
    "\n",
    "# 右侧数值+星号\n",
    "def stars(pv): return \"***\" if pv<1e-3 else \"**\" if pv<1e-2 else \"*\" if pv<0.05 else \"\"\n",
    "for yi, r in enumerate(df_eff.itertuples()):\n",
    "    ax.text(1.01, yi, f\"{r.d:.2f} [{r.lo:.2f}, {r.hi:.2f}] {stars(r.p)}\",\n",
    "            transform=ax.get_yaxis_transform(), ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "# 标题含 p-interaction；图注含 N 与参考组\n",
    "title = f\"Ethnicity-specific ΔUPF (HFI: insecure − secure)  (p-int={p_inter:.3f})\"\n",
    "ax.set_title(title, loc=\"left\", fontsize=14, weight=\"bold\", pad=6)\n",
    "fig.text(0.12, 0.02, f\"OLS (HC3); N={len(dat_coreB)}; refs: Male, White, Secure\", ha=\"left\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "# 美化与保存\n",
    "ax.grid(axis='x', ls=':', alpha=0.35)\n",
    "ax.spines[['top','right']].set_visible(False)\n",
    "plt.tight_layout(rect=[0.22, 0.06, 0.98, 0.95])\n",
    "\n",
    "Path(\"output\").mkdir(exist_ok=True)\n",
    "plt.savefig(\"/content/drive/MyDrive/UPF-HFI/outcome_figure/coreB_ethnicity_dUPF.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show(); plt.close(fig)\n",
    "print(\"Saved: /content/drive/MyDrive/UPF-HFI/outcome_figure/coreB_ethnicity_dUPF.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39240a84",
   "metadata": {},
   "source": [
    "# 4. Robustness（结构性混杂）：一次性加入 SES（income, employ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d728c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Robustness: add SES once (same-N), print only =====\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# 同一样本（用于 Core-A 与 +SES 两个模型的严格对比）\n",
    "ses_cols = [OUTCOME, \"HFI_binary\", \"age_participant\",\n",
    "            \"gender_participant\", \"ethn_participant\", \"child_numbers\",\n",
    "            \"income\", \"employ\"]\n",
    "dat_R = df_model[ses_cols].copy()\n",
    "for col in [\"HFI_binary\",\"age_participant\",\"child_numbers\",\"income\"]:\n",
    "    dat_R[col] = pd.to_numeric(dat_R[col], errors=\"coerce\")\n",
    "dat_R = dat_R.dropna().copy()\n",
    "\n",
    "print(f\"\\nRobustness(+SES) 样本量（same-N）: {len(dat_R)}\")\n",
    "\n",
    "# 两个模型都基于同一个 dat_R\n",
    "res_coreA_sameN = fit_ols(\n",
    "    f\"{OUTCOME} ~ HFI_binary + age_participant\"\n",
    "    \" + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \" + child_numbers\",\n",
    "    dat_R, cov_type=\"HC3\"\n",
    ")\n",
    "res_ses = fit_ols(\n",
    "    f\"{OUTCOME} ~ HFI_binary + age_participant\"\n",
    "    \" + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \" + child_numbers + income + C(employ)\",\n",
    "    dat_R, cov_type=\"HC3\"\n",
    ")\n",
    "\n",
    "# 取系数与区间\n",
    "b_core, ci_core, p_core = coef_ci(res_coreA_sameN, \"HFI_binary\")\n",
    "b_ses , ci_ses , p_ses  = coef_ci(res_ses,          \"HFI_binary\")\n",
    "\n",
    "# 增减比例（b_core 的相对变化）\n",
    "atten = np.nan\n",
    "if pd.notna(b_core) and not np.isclose(b_core, 0.0):\n",
    "    atten = 100.0 * (b_core - b_ses) / abs(b_core)\n",
    "chg = \"attenuation\" if atten >= 0 else \"increase\"\n",
    "\n",
    "print(f\"HFI: Core-A = {b_core:.2f} pp [ {ci_core[0]:.2f}, {ci_core[1]:.2f} ] (p={p_core:.3f})\")\n",
    "print(f\"     +SES    = {b_ses:.2f} pp [ {ci_ses[0]:.2f}, {ci_ses[1]:.2f} ] (p={p_ses:.3f})\")\n",
    "print(f\"Δ = {b_ses-b_core:+.2f} pp  ({chg} {abs(atten):.1f}%)  — Same sample N={len(dat_R)}; OLS(HC3)\")\n",
    "\n",
    "# 可选：报告 HFI 的 partial R²（若你已引入 partial_R2 工具函数）\n",
    "try:\n",
    "    r2_core, pF_core = partial_R2(res_coreA_sameN, \"HFI_binary\")\n",
    "    r2_ses , pF_ses  = partial_R2(res_ses,          \"HFI_binary\")\n",
    "    print(f\"partial R²(HFI): Core-A {r2_core:.3f} (p={pF_core:.3f}) → +SES {r2_ses:.3f} (p={pF_ses:.3f})\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 若想导出到表格：\n",
    "out_tab = pd.DataFrame({\n",
    "    \"model\":[\"Core-A\",\"Core-A + SES\"],\n",
    "    \"beta\":[b_core, b_ses],\n",
    "    \"ci_lo\":[ci_core[0], ci_ses[0]],\n",
    "    \"ci_hi\":[ci_core[1], ci_ses[1]],\n",
    "    \"p\":[p_core, p_ses],\n",
    "    \"N\":len(dat_R)\n",
    "})\n",
    "out_tab.to_csv(\"output/robustness_ses_sameN.csv\", index=False)\n",
    "print(\"Saved: output/robustness_ses_sameN.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Figure 4. Effect of adding SES (same N) =====\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "labels = [\"Core-A (no SES)\", \"+SES (income, employment)\"]\n",
    "b  = np.array([b_core, b_ses], dtype=float)\n",
    "lo = np.array([ci_core[0], ci_ses[0]], dtype=float)\n",
    "hi = np.array([ci_core[1], ci_ses[1]], dtype=float)\n",
    "err = np.vstack([b - lo, hi - b])\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 12, \"axes.labelsize\": 10,\n",
    "    \"xtick.labelsize\": 9, \"ytick.labelsize\": 9,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.6, 3.6), dpi=200)\n",
    "ypos = np.arange(2)\n",
    "\n",
    "# 横轴范围 & 底色\n",
    "mx = float(np.max(np.abs(np.r_[lo, hi])))\n",
    "ax.set_xlim(-1.25*mx, 1.25*mx)\n",
    "ax.axvspan(-1.15*mx, 0, color=\"0.95\", zorder=0)   # 负区浅底\n",
    "ax.axvline(0, linestyle='--', linewidth=1.8, zorder=1)\n",
    "\n",
    "# 误差线 + 点 + 连接线\n",
    "ax.errorbar(b, ypos, xerr=err, fmt='o', capsize=7, elinewidth=2, markersize=5, zorder=3)\n",
    "ax.plot(b, ypos, linewidth=1.6, alpha=0.9, zorder=2)\n",
    "\n",
    "# 轴标签\n",
    "ax.set_yticks(ypos, labels)\n",
    "ax.set_xlabel(\"HFI → UPF% (pp), coefficient with 95% CI\")\n",
    "ax.set_title(\"Effect of adding SES (same N)\")\n",
    "\n",
    "# 右侧文字（Δ 与增减比例）\n",
    "delta = b[1] - b[0]\n",
    "chg = \"attenuation\" if atten >= 0 else \"increase\"\n",
    "ax.text(ax.get_xlim()[1] + 0.02*(ax.get_xlim()[1]-ax.get_xlim()[0]),\n",
    "        0.5, f\"Δ = {delta:+.2f} pp  ({chg} {abs(atten):.1f}%)\",\n",
    "        va='center', ha='left', fontsize=9, transform=ax.transData)\n",
    "\n",
    "# 在点旁直接标注“估计 [CI]”\n",
    "shift = 0.03 * (ax.get_xlim()[1]-ax.get_xlim()[0])\n",
    "# —— 把标签放在置信区间外侧（右侧或左侧），避免遮住误差线 ——\n",
    "span = ax.get_xlim()[1] - ax.get_xlim()[0]\n",
    "pad  = 0.02 * span  # 外侧偏移\n",
    "\n",
    "for i in range(2):\n",
    "    label = f\"{b[i]:.2f} [{lo[i]:.2f}, {hi[i]:.2f}]\"\n",
    "    if b[i] >= 0:\n",
    "        # 点在右半区：把标签放到上界 hi 的右侧\n",
    "        anchor_x = hi[i]\n",
    "        text_x   = hi[i] + pad\n",
    "        ha       = 'left'\n",
    "    else:\n",
    "        # 点在左半区：把标签放到下界 lo 的左侧\n",
    "        anchor_x = lo[i]\n",
    "        text_x   = lo[i] - pad\n",
    "        ha       = 'right'\n",
    "\n",
    "    ax.annotate(label, xy=(anchor_x, ypos[i]), xytext=(text_x, ypos[i]),\n",
    "                va='center', ha=ha, fontsize=9)\n",
    "\n",
    "ax.grid(axis='x', linestyle=':', alpha=0.35)\n",
    "ax.text(0.01, -0.35, f\"Same sample (listwise) N={len(dat_R)}; OLS (HC3).\", transform=ax.transAxes)\n",
    "ax.spines[['top','right']].set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 若需要保存：\n",
    "import pathlib; pathlib.Path(\"output\").mkdir(exist_ok=True)\n",
    "plt.savefig(\"output/figure2_ses_sameN.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a9b820",
   "metadata": {},
   "source": [
    "## 对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Robustness: add SES once — fresh-N (each model uses its own available rows) =====\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# 1) 各自样本\n",
    "coreA_cols_fresh = [OUTCOME, \"HFI_binary\", \"age_participant\",\n",
    "                    \"gender_participant\", \"ethn_participant\", \"child_numbers\"]\n",
    "ses_cols_fresh   = coreA_cols_fresh + [\"income\", \"employ\"]\n",
    "\n",
    "dat_coreA_fresh = df_model[coreA_cols_fresh].copy()\n",
    "for c in [\"HFI_binary\",\"age_participant\",\"child_numbers\"]:\n",
    "    dat_coreA_fresh[c] = pd.to_numeric(dat_coreA_fresh[c], errors=\"coerce\")\n",
    "dat_coreA_fresh = dat_coreA_fresh.dropna().copy()\n",
    "\n",
    "dat_ses_fresh = df_model[ses_cols_fresh].copy()\n",
    "for c in [\"HFI_binary\",\"age_participant\",\"child_numbers\",\"income\"]:\n",
    "    dat_ses_fresh[c] = pd.to_numeric(dat_ses_fresh[c], errors=\"coerce\")\n",
    "dat_ses_fresh = dat_ses_fresh.dropna().copy()\n",
    "\n",
    "print(f\"Core-A fresh-N: N={len(dat_coreA_fresh)}\")\n",
    "print(f\"+SES  fresh-N: N={len(dat_ses_fresh)}\")\n",
    "\n",
    "# 2) 拟合两个模型（各自 fresh-N）\n",
    "form_core = (\n",
    "    f\"{OUTCOME} ~ HFI_binary + age_participant\"\n",
    "    \" + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \" + child_numbers\"\n",
    ")\n",
    "form_ses  = form_core + \" + income + C(employ)\"\n",
    "\n",
    "res_core_fresh = fit_ols(form_core, dat_coreA_fresh, cov_type=\"HC3\")\n",
    "res_ses_fresh  = fit_ols(form_ses,  dat_ses_fresh,  cov_type=\"HC3\")\n",
    "\n",
    "# 3) 系数与区间\n",
    "b_core_f, ci_core_f, p_core_f = coef_ci(res_core_fresh, \"HFI_binary\")\n",
    "b_ses_f , ci_ses_f , p_ses_f  = coef_ci(res_ses_fresh,  \"HFI_binary\")\n",
    "\n",
    "# 4) 变化幅度（fresh-N）\n",
    "atten_f = np.nan\n",
    "if pd.notna(b_core_f) and not np.isclose(b_core_f, 0.0):\n",
    "    atten_f = 100.0 * (b_core_f - b_ses_f) / abs(b_core_f)\n",
    "chg_f = \"attenuation\" if atten_f >= 0 else \"increase\"\n",
    "\n",
    "print(f\"HFI (fresh-N):\")\n",
    "print(f\"  Core-A = {b_core_f:.2f} pp [{ci_core_f[0]:.2f}, {ci_core_f[1]:.2f}]  (p={p_core_f:.3f})\")\n",
    "print(f\"  +SES   = {b_ses_f:.2f} pp [{ci_ses_f[0]:.2f}, {ci_ses_f[1]:.2f}]  (p={p_ses_f:.3f})\")\n",
    "print(f\"  Δ = {b_ses_f-b_core_f:+.2f} pp  ({chg_f} {abs(atten_f):.1f}%)\")\n",
    "\n",
    "# 5) 与 same-N 的对比（如果前面变量在环境中）\n",
    "try:\n",
    "    _ = b_core  # same-N 变量是否存在\n",
    "    print(\"\\nCompare to same-N:\")\n",
    "    print(f\"  Core-A: fresh − same = {b_core_f - b_core:+.2f} pp\")\n",
    "    print(f\"  +SES  : fresh − same = {b_ses_f  - b_ses:+.2f} pp\")\n",
    "    print(f\"  Δ(change due to SES): fresh − same = {(b_ses_f-b_core_f) - (b_ses-b_core):+.2f} pp\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# 可选：导出一张对比小表\n",
    "out_tab_fresh = pd.DataFrame({\n",
    "    \"model\":[\"Core-A (fresh)\",\"+SES (fresh)\"],\n",
    "    \"beta\":[b_core_f, b_ses_f],\n",
    "    \"ci_lo\":[ci_core_f[0], ci_ses_f[0]],\n",
    "    \"ci_hi\":[ci_core_f[1], ci_ses_f[1]],\n",
    "    \"p\":[p_core_f, p_ses_f],\n",
    "    \"N\":[len(dat_coreA_fresh), len(dat_ses_fresh)]\n",
    "})\n",
    "out_tab_fresh.to_csv(\"output/robustness_ses_freshN.csv\", index=False)\n",
    "print(\"Saved: output/robustness_ses_freshN.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9870f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Figure 5. +SES effect — same-N vs fresh-N (overlay dumbbells) =====\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "# 需要：同一内核里已有 same-N 的 b_core, ci_core, b_ses, ci_ses, len(dat_R)\n",
    "# 以及 fresh-N 的 b_core_f, ci_core_f, b_ses_f, ci_ses_f, len(dat_coreA_fresh), len(dat_ses_fresh)\n",
    "\n",
    "# 统一横轴范围覆盖两套区间\n",
    "all_bounds = np.array([ci_core[0], ci_core[1], ci_ses[0], ci_ses[1],\n",
    "                       ci_core_f[0], ci_core_f[1], ci_ses_f[0], ci_ses_f[1]], float)\n",
    "mx = float(np.max(np.abs(all_bounds))) * 1.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.6, 3.8), dpi=200)\n",
    "ax.set_xlim(-mx, mx)\n",
    "ax.axvspan(-mx, 0, color=\"0.95\", zorder=0)\n",
    "ax.axvline(0, ls='--', lw=1.6, zorder=1)\n",
    "\n",
    "# y 位置：0 = same-N, 1 = fresh-N\n",
    "ypos = np.array([0,1])\n",
    "\n",
    "# same-N\n",
    "b_same  = np.array([b_core,  b_ses ], float)\n",
    "lo_same = np.array([ci_core[0], ci_ses[0]], float)\n",
    "hi_same = np.array([ci_core[1], ci_ses[1]], float)\n",
    "ax.errorbar(\n",
    "    b_same,\n",
    "    np.full(b_same.shape, ypos[0]),                   # ← 原来是 ypos[0]\n",
    "    xerr=np.vstack([b_same-lo_same, hi_same-b_same]),\n",
    "    fmt='o', capsize=7, elinewidth=2, markersize=5, label=\"same-N\", zorder=3\n",
    ")\n",
    "ax.plot(b_same,  [ypos[0], ypos[0]], lw=1.6, alpha=0.9, zorder=2)\n",
    "\n",
    "# fresh-N（不同点形+虚线连接）\n",
    "b_fresh  = np.array([b_core_f, b_ses_f], float)\n",
    "lo_fresh = np.array([ci_core_f[0], ci_ses_f[0]], float)\n",
    "hi_fresh = np.array([ci_core_f[1], ci_ses_f[1]], float)\n",
    "ax.errorbar(\n",
    "    b_fresh,\n",
    "    np.full(b_fresh.shape, ypos[1]),                  # ← 原来是 ypos[1]\n",
    "    xerr=np.vstack([b_fresh-lo_fresh, hi_fresh-b_fresh]),\n",
    "    fmt='s', capsize=7, elinewidth=2, markersize=5, label=\"fresh-N\", zorder=3\n",
    ")\n",
    "ax.plot(b_fresh, [ypos[1], ypos[1]], lw=1.6, alpha=0.9, linestyle=':', zorder=2)\n",
    "\n",
    "# 轴与标签\n",
    "ax.set_yticks(ypos, [\"same-N\", \"fresh-N\"])\n",
    "ax.set_xlabel(\"HFI → UPF% (pp), coefficient with 95% CI\")\n",
    "ax.set_title(\"SES robustness: same-N vs fresh-N\")\n",
    "\n",
    "# 右侧写 Δ\n",
    "shift = 0.03 * (ax.get_xlim()[1]-ax.get_xlim()[0])\n",
    "ax.text(ax.get_xlim()[1] + shift, ypos[0], f\"Δ(same)  = {b_same[1]-b_same[0]:+.2f} pp\", va='center', ha='left', fontsize=9)\n",
    "ax.text(ax.get_xlim()[1] + shift, ypos[1], f\"Δ(fresh) = {b_fresh[1]-b_fresh[0]:+.2f} pp\", va='center', ha='left', fontsize=9)\n",
    "\n",
    "# 图注：两套样本量\n",
    "ax.text(0.01, -0.30,\n",
    "        f\"same-N: N={len(dat_R)}; fresh-N: Core-A N={len(dat_coreA_fresh)}, +SES N={len(dat_ses_fresh)}; OLS (HC3).\",\n",
    "        transform=ax.transAxes, fontsize=9)\n",
    "\n",
    "ax.legend(frameon=False, loc=\"upper left\")\n",
    "ax.grid(axis='x', ls=':', alpha=0.35)\n",
    "ax.spines[['top','right']].set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存（可选）\n",
    "import pathlib; pathlib.Path('output').mkdir(exist_ok=True)\n",
    "plt.savefig('output/figureSx_ses_same_vs_fresh.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 直接列出“缺 SES”的那几行的关键信息（case-centric）\n",
    "miss = df_model[[OUTCOME,\"HFI_binary\",\"ethn_participant\",\"gender_participant\",\n",
    "                 \"age_participant\",\"child_numbers\",\"income\",\"employ\"]].copy()\n",
    "miss = miss[miss[\"income\"].isna() | miss[\"employ\"].isna()]\n",
    "print(len(miss))              # 应该是 5\n",
    "print(miss[[\"HFI_binary\",\"ethn_participant\"]].value_counts())  # 谁占的多\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0664a49",
   "metadata": {},
   "source": [
    "# 5. Sensitivity（关键假设变化）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a211f5",
   "metadata": {},
   "source": [
    "## 5.1 HFI 三分类(有序) + 趋势检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Sensitivity: HFI 3-category + trend (print only) =====\n",
    "import numpy as np, pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "if \"HFI_category\" in df_model.columns:\n",
    "    # 统一类型与顺序\n",
    "    ord_dtype = CategoricalDtype(\n",
    "        categories=[\"Food secure\",\"Low food security\",\"Very low food security\"], ordered=True\n",
    "    )\n",
    "    df_model[\"HFI_category\"] = df_model[\"HFI_category\"].astype(ord_dtype)\n",
    "\n",
    "    # 同一套自变量 + 三分类列\n",
    "    sens_cols = [OUTCOME, \"HFI_binary\", \"age_participant\",\n",
    "                 \"gender_participant\", \"ethn_participant\", \"child_numbers\",\n",
    "                 \"HFI_category\"]\n",
    "    dat_sens = df_model[sens_cols].copy()\n",
    "    # 强转为数值，防止混入字符串\n",
    "    for col in [\"HFI_binary\",\"age_participant\",\"child_numbers\"]:\n",
    "        dat_sens[col] = pd.to_numeric(dat_sens[col], errors=\"coerce\")\n",
    "    dat_sens = dat_sens.dropna().copy()\n",
    "\n",
    "    print(f\"\\nSensitivity—有序三分类 样本量: {len(dat_sens)}\")\n",
    "\n",
    "    # (a) 类别模型（相对 Food secure）\n",
    "    form_cat = (\n",
    "        f\"{OUTCOME} ~ C(HFI_category, Treatment(reference='Food secure'))\"\n",
    "        \" + age_participant\"\n",
    "        \" + C(gender_participant, Treatment(reference=1))\"\n",
    "        \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "        \" + child_numbers\"\n",
    "    )\n",
    "    res_cat = fit_ols(form_cat, dat_sens, cov_type=\"HC3\")\n",
    "\n",
    "    print(\"\\nAdjusted difference vs Food secure:\")\n",
    "    for lev in [\"Low food security\",\"Very low food security\"]:\n",
    "        term = f\"C(HFI_category, Treatment(reference='Food secure'))[T.{lev}]\"\n",
    "        est, ci, p = coef_ci(res_cat, term)\n",
    "        print(f\"  {lev:>21}: {est:+.2f} pp (95%CI {ci[0]:.2f}, {ci[1]:.2f}); p={p:.3f}\")\n",
    "\n",
    "    # (b) 趋势检验（0/1/2 连续）\n",
    "    tr_map = {\"Food secure\":0, \"Low food security\":1, \"Very low food security\":2}\n",
    "    dat_sens[\"HFI_trend\"] = dat_sens[\"HFI_category\"].map(tr_map).astype(float)\n",
    "\n",
    "    form_trend = (\n",
    "        f\"{OUTCOME} ~ HFI_trend\"\n",
    "        \" + age_participant\"\n",
    "        \" + C(gender_participant, Treatment(reference=1))\"\n",
    "        \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "        \" + child_numbers\"\n",
    "    )\n",
    "    res_trend = fit_ols(form_trend, dat_sens, cov_type=\"HC3\")\n",
    "    est, ci, p = coef_ci(res_trend, \"HFI_trend\")\n",
    "    print(f\"\\np-trend: 每升一档 HFI，UPF% 变化 {est:.2f} pp (95%CI {ci[0]:.2f}, {ci[1]:.2f}); p={p:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n[跳过] 未找到 HFI_category（三分类）列，已跳过敏感性-三分类与趋势检验。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —— 共享：拟合类别模型 & 趋势模型，并做边际预测 ——\n",
    "import numpy as np, pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "cats = [\"Food secure\",\"Low food security\",\"Very low food security\"]\n",
    "df_model[\"HFI_category\"] = df_model[\"HFI_category\"].astype(\n",
    "    CategoricalDtype(categories=cats, ordered=True)\n",
    ")\n",
    "\n",
    "sens_cols = [OUTCOME, \"age_participant\",\"gender_participant\",\n",
    "             \"ethn_participant\",\"child_numbers\",\"HFI_category\"]\n",
    "dat = df_model[sens_cols].copy()\n",
    "for c in [\"age_participant\",\"child_numbers\"]:\n",
    "    dat[c] = pd.to_numeric(dat[c], errors=\"coerce\")\n",
    "dat = dat.dropna().copy()\n",
    "\n",
    "# 类别模型（相对 Food secure），与正文一致\n",
    "form_cat = (\n",
    "    f\"{OUTCOME} ~ C(HFI_category, Treatment(reference='Food secure'))\"\n",
    "    \" + age_participant + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White')) + child_numbers\"\n",
    ")\n",
    "res_cat = fit_ols(form_cat, dat, cov_type=\"HC3\")\n",
    "\n",
    "# 典型协变量下的预测均值（每类一条）\n",
    "grid = pd.DataFrame({\n",
    "    \"HFI_category\": cats,\n",
    "    \"age_participant\": dat[\"age_participant\"].mean(),\n",
    "    \"gender_participant\": 1,      # Male\n",
    "    \"ethn_participant\": \"White\",\n",
    "    \"child_numbers\": dat[\"child_numbers\"].mean(),\n",
    "})\n",
    "sf    = res_cat.get_prediction(grid).summary_frame(alpha=0.05)\n",
    "means = sf[\"mean\"].to_numpy(float)\n",
    "lo    = sf[\"mean_ci_lower\"].to_numpy(float)\n",
    "hi    = sf[\"mean_ci_upper\"].to_numpy(float)\n",
    "yerr  = np.vstack([means - lo, hi - means])\n",
    "n_by  = dat[\"HFI_category\"].value_counts().reindex(cats, fill_value=0).astype(int)\n",
    "\n",
    "# 趋势（0/1/2）\n",
    "dat[\"HFI_trend\"] = dat[\"HFI_category\"].map({\"Food secure\":0, \"Low food security\":1, \"Very low food security\":2}).astype(float)\n",
    "res_trend = fit_ols(\n",
    "    f\"{OUTCOME} ~ HFI_trend + age_participant + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White')) + child_numbers\",\n",
    "    dat, cov_type=\"HC3\"\n",
    ")\n",
    "slope, (slo, shi), p_trend = coef_ci(res_trend, \"HFI_trend\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Figure 2 (polished): Adjusted UPF% by HFI category + trend arrow ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# 1) 数据与模型（与主文一致）\n",
    "cats = [\"Food secure\",\"Low food security\",\"Very low food security\"]\n",
    "df_model[\"HFI_category\"] = df_model[\"HFI_category\"].astype(\n",
    "    CategoricalDtype(categories=cats, ordered=True)\n",
    ")\n",
    "\n",
    "sens_cols = [OUTCOME, \"age_participant\",\"gender_participant\",\n",
    "             \"ethn_participant\",\"child_numbers\",\"HFI_category\"]\n",
    "dat_sens = df_model[sens_cols].copy()\n",
    "for col in [\"age_participant\",\"child_numbers\"]:\n",
    "    dat_sens[col] = pd.to_numeric(dat_sens[col], errors=\"coerce\")\n",
    "dat_sens = dat_sens.dropna().copy()\n",
    "\n",
    "form_cat = (\n",
    "    f\"{OUTCOME} ~ C(HFI_category, Treatment(reference='Food secure'))\"\n",
    "    \" + age_participant + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White')) + child_numbers\"\n",
    ")\n",
    "res_cat = fit_ols(form_cat, dat_sens, cov_type=\"HC3\")\n",
    "\n",
    "# 2) 典型协变量预测\n",
    "grid = pd.DataFrame({\n",
    "    \"HFI_category\": cats,\n",
    "    \"age_participant\": dat_sens[\"age_participant\"].mean(),\n",
    "    \"gender_participant\": 1,\n",
    "    \"ethn_participant\": \"White\",\n",
    "    \"child_numbers\": dat_sens[\"child_numbers\"].mean(),\n",
    "})\n",
    "pred  = res_cat.get_prediction(grid).summary_frame(alpha=0.05)\n",
    "means = pred[\"mean\"].to_numpy(float)\n",
    "lo    = pred[\"mean_ci_lower\"].to_numpy(float)\n",
    "hi    = pred[\"mean_ci_upper\"].to_numpy(float)\n",
    "yerr  = np.vstack([means - lo, hi - means])\n",
    "n_by  = dat_sens[\"HFI_category\"].value_counts().reindex(cats, fill_value=0).astype(int)\n",
    "\n",
    "# 3) 趋势（0/1/2）\n",
    "tr_map = {\"Food secure\":0, \"Low food security\":1, \"Very low food security\":2}\n",
    "dat_sens[\"HFI_trend\"] = dat_sens[\"HFI_category\"].map(tr_map).astype(float)\n",
    "res_trend = fit_ols(\n",
    "    f\"{OUTCOME} ~ HFI_trend + age_participant + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White')) + child_numbers\",\n",
    "    dat_sens, cov_type=\"HC3\"\n",
    ")\n",
    "slope, (slo, shi), p_trend = coef_ci(res_trend, \"HFI_trend\")\n",
    "\n",
    "# 4) 画图（优化样式）\n",
    "plt.rcParams.update({\"axes.titlesize\":13,\"axes.labelsize\":11,\"xtick.labelsize\":11,\"ytick.labelsize\":10})\n",
    "fig, ax = plt.subplots(figsize=(7.8, 4.4), dpi=200)\n",
    "x = np.arange(len(cats))\n",
    "\n",
    "# marker 形状区分三类（黑白也清楚）\n",
    "markers = ['o','s','^']\n",
    "\n",
    "# 误差线 + 点\n",
    "ax.errorbar(x, means, yerr=yerr, fmt='none', capsize=6, elinewidth=2.2, zorder=2)\n",
    "for i in range(len(cats)):\n",
    "    ax.plot(x[i], means[i], markers[i], ms=7, zorder=3)\n",
    "\n",
    "# 连接线（淡一点）\n",
    "ax.plot(x, means, '-', linewidth=2.0, alpha=0.9, zorder=1)\n",
    "\n",
    "# 网格/坐标\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_ylabel(\"Adjusted predicted UPF% (with 95% CI)\")\n",
    "ax.set_xticks(x, [f\"{c}\\n(n={n_by[c]})\" for c in cats])\n",
    "ax.grid(axis='y', linestyle=':', alpha=0.35)\n",
    "ax.spines[['top','right']].set_visible(False)\n",
    "\n",
    "# 数值标签（不遮挡）\n",
    "for xi, m, l, h in zip(x, means, lo, hi):\n",
    "    ax.text(xi, m + 1.4, f\"{m:.1f}% [{l:.1f}, {h:.1f}]\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 顶部趋势箭头 + 注释（更像你的图1）\n",
    "y_top = max(hi) + 3.5\n",
    "ax.annotate(\"\", xy=(x[-1], y_top), xytext=(x[0], y_top),\n",
    "            arrowprops=dict(arrowstyle='-|>', lw=1.2))\n",
    "ax.text(np.mean(x), y_top + 1.2,\n",
    "        f\"Δ per level = {slope:+.2f} pp  [ {slo:.2f}, {shi:.2f} ];  p-trend = {p_trend:.3f}\",\n",
    "        ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_title(\"Figure 2. Adjusted UPF% by HFI category (typical covariates)\", loc='left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Figure 6 (clean zoom + pairwise tests) — 完整稳健版 ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "# 0) 依赖：需要已有 res_cat 与 linear_combo（用于 pairwise 对比）\n",
    "tL = \"C(HFI_category, Treatment(reference='Food secure'))[T.Low food security]\"\n",
    "tV = \"C(HFI_category, Treatment(reference='Food secure'))[T.Very low food security]\"\n",
    "est_LF, ci_LF, p_LF = linear_combo(res_cat, {tL: 1})           # Food secure → Low\n",
    "est_VL, ci_VL, p_VL = linear_combo(res_cat, {tV: 1, tL: -1})   # Low → Very low\n",
    "pair_info = {(0,1): {\"p\": p_LF, \"ci\": ci_LF}, (1,2): {\"p\": p_VL, \"ci\": ci_VL}}\n",
    "def stars(p): return \"***\" if p<0.001 else \"**\" if p<0.01 else \"*\" if p<0.05 else \"\"\n",
    "\n",
    "# 1) 从表里抽取三组均值和CI；找不到则回退到文本数值\n",
    "def get_threecat_from_table():\n",
    "    for cand in [\"table4_steps\", \"table4\", \"Table4_HFI_threecat\"]:\n",
    "        if cand in globals() and isinstance(globals()[cand], pd.DataFrame):\n",
    "            df = globals()[cand].copy()\n",
    "            # 兼容可能的列名变体\n",
    "            col_cat = [c for c in df.columns if \"category\" in c.lower()][0]\n",
    "            col_mean = [c for c in df.columns if \"mean\" in c.lower()][0]\n",
    "            col_ci   = [c for c in df.columns if \"95%\" in c or \"ci\" in c.lower()][0]\n",
    "            # 只保留三行（Food secure / Low / Very low 顺序）\n",
    "            order = [\"Food secure\",\"Low food security\",\"Very low food security\"]\n",
    "            df[col_cat] = df[col_cat].astype(str)\n",
    "            # 容错：去掉可能的多余空格\n",
    "            mapper = {\n",
    "                \"Food secure\":\"Food secure\",\n",
    "                \"Low food security\":\"Low food security\",\n",
    "                \"Very low food security\":\"Very low food security\"\n",
    "            }\n",
    "            df[col_cat] = df[col_cat].map(lambda s: mapper.get(s.strip(), s))\n",
    "            df = df.set_index(col_cat).reindex(order).reset_index()\n",
    "            cats3  = df[col_cat].tolist()\n",
    "            means3 = pd.to_numeric(df[col_mean], errors=\"coerce\").to_numpy(dtype=float)\n",
    "            # 解析 CI（允许 68.8–73.2 或 68.8-73.2）\n",
    "            ci_txt = df[col_ci].astype(str).str.replace(\"–\",\"-\", regex=False)\n",
    "            lo3 = pd.to_numeric(ci_txt.str.split(\"-\", n=1, expand=True)[0], errors=\"coerce\").to_numpy(dtype=float)\n",
    "            hi3 = pd.to_numeric(ci_txt.str.split(\"-\", n=1, expand=True)[1], errors=\"coerce\").to_numpy(dtype=float)\n",
    "            yerr3 = np.vstack([means3 - lo3, hi3 - means3])  # (2,3)\n",
    "            if np.any(np.isnan(means3)) or np.any(np.isnan(yerr3)):\n",
    "                raise ValueError(\"Parsed NaN from table.\")\n",
    "            return cats3, means3, yerr3\n",
    "    raise KeyError(\"no_table\")\n",
    "\n",
    "try:\n",
    "    cats3, means3, yerr3 = get_threecat_from_table()\n",
    "except Exception:\n",
    "    # 回退：使用你文中的数值\n",
    "    cats3  = [\"Food secure\",\"Low food security\",\"Very low food security\"]\n",
    "    means3 = np.array([71.0, 71.9, 74.8], dtype=float)\n",
    "    lo3    = np.array([68.8, 67.7, 70.9], dtype=float)\n",
    "    hi3    = np.array([73.2, 76.1, 78.7], dtype=float)\n",
    "    yerr3  = np.vstack([means3 - lo3, hi3 - means3])  # (2,3)\n",
    "\n",
    "# 2) y 轴范围：包住 CI 并留 10% padding（裁到 0–100）\n",
    "lo_all = means3 - yerr3[0]\n",
    "hi_all = means3 + yerr3[1]\n",
    "pad = max(1.0, 0.10 * (hi_all.max() - lo_all.min()))\n",
    "yl  = max(0.0, np.floor(lo_all.min() - pad))\n",
    "yh  = min(100.0, np.ceil (hi_all.max() + pad))\n",
    "\n",
    "# 3) 画图\n",
    "x = np.arange(3)\n",
    "markers = ['o','s','^']\n",
    "mfc, mec = 'white', '0.3'\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\":220,\"axes.titlesize\":15,\"axes.labelsize\":12,\n",
    "                     \"xtick.labelsize\":12,\"ytick.labelsize\":11})\n",
    "fig, ax = plt.subplots(figsize=(8.0, 4.4))\n",
    "ax.errorbar(x, means3, yerr=yerr3, fmt='none', capsize=6, elinewidth=2.2, zorder=2)\n",
    "for i in range(3):\n",
    "    ax.plot(x[i], means3[i], markers[i], ms=8, mfc=mfc, mec=mec, mew=1.5, zorder=3)\n",
    "ax.plot(x, means3, '-', lw=1.8, color='0.25', alpha=0.9, zorder=1)\n",
    "\n",
    "ax.set_ylim(yl, yh); ax.set_xlim(-0.25, 2.25)\n",
    "ax.set_xticks(x, cats3)\n",
    "ax.grid(axis='y', linestyle=':', alpha=0.35)\n",
    "ax.spines[['top','right']].set_visible(False)\n",
    "\n",
    "# 4) 相邻两档 Δ 标注\n",
    "def stars(p): return \"***\" if p<0.001 else \"**\" if p<0.01 else \"*\" if p<0.05 else \"\"\n",
    "gap = (yh - yl) * 0.06\n",
    "for i in (0,1):\n",
    "    midx  = (x[i] + x[i+1]) / 2\n",
    "    midy  = (means3[i] + means3[i+1]) / 2 + gap\n",
    "    delta = means3[i+1] - means3[i]\n",
    "    info  = pair_info[(i, i+1)]\n",
    "    label = f\"Δ = {delta:+.1f} pp  {stars(info['p'])} (p={info['p']:.3f})\"\n",
    "    ax.text(midx, midy, label, ha='center', va='bottom', fontsize=10, color='0.25')\n",
    "\n",
    "# 5) 右上角趋势徽标（若变量不存在就显示 n/a）\n",
    "try:\n",
    "    trend_text = f\"Trend per level: {float(slope):+-.2f} pp\\np-trend = {float(p_trend):.3f}\"\n",
    "except Exception:\n",
    "    trend_text = \"Trend per level: n/a\\np-trend = n/a\"\n",
    "ax.text(0.99, 0.98, trend_text, transform=ax.transAxes, ha='right', va='top',\n",
    "        fontsize=11, bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"0.8\", alpha=0.95))\n",
    "\n",
    "# —— 轴标签（保留），不要标题 ——\n",
    "ax.set_xlabel(\"HFI category\")                         # 横轴：三类名称已在 xticks，这里补充轴名\n",
    "ax.set_ylabel(\"Adjusted predicted UPF% (zoomed)\")     # 纵轴：模型调整后的 UPF%\n",
    "ax.set_title(\"\")                                      # 确保没有标题（清空）\n",
    "# 如果曾用过 suptitle，可顺带清空：\n",
    "# fig.suptitle(\"\")\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"DEBUG shapes -> means3:\", means3.shape, \"| yerr3:\", yerr3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056335d7",
   "metadata": {},
   "source": [
    "## 5.2 探索性交互：HFI × I(child_numbers≥3) （附录）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fcf09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Exploratory interaction: HFI × I(child_numbers ≥ 3) — print only =====\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# 1) 构造二元指示变量\n",
    "df_model[\"many_children\"] = (pd.to_numeric(df_model[\"child_numbers\"], errors=\"coerce\") >= 3).astype(\"int\")\n",
    "\n",
    "cols_c3 = [OUTCOME, \"HFI_binary\", \"age_participant\",\n",
    "           \"gender_participant\", \"ethn_participant\", \"child_numbers\", \"many_children\"]\n",
    "\n",
    "dat_c3 = df_model[cols_c3].copy()\n",
    "# 强制为数值（以免混入字符串）\n",
    "for c in [\"HFI_binary\",\"age_participant\",\"child_numbers\",\"many_children\"]:\n",
    "    dat_c3[c] = pd.to_numeric(dat_c3[c], errors=\"coerce\")\n",
    "dat_c3 = dat_c3.dropna().copy()\n",
    "\n",
    "print(f\"N (for children interaction): {len(dat_c3)}\")\n",
    "\n",
    "# 2) 设定模型：含主效应 + 交互\n",
    "form_c3 = (\n",
    "    f\"{OUTCOME} ~ HFI_binary + many_children + HFI_binary:many_children\"\n",
    "    \" + age_participant + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \" + child_numbers\"\n",
    ")\n",
    "# 说明：同时加入“child_numbers(连续)” 与 “many_children(阈值)” 是在检验\n",
    "# “≥3 的额外阈值效应”，many_children 可以理解为相对线性项的偏离。\n",
    "\n",
    "res_c3 = fit_ols(form_c3, dat_c3, cov_type=\"HC3\")\n",
    "\n",
    "# 3) 交互系数（差异的差异）\n",
    "e_int, ci_int, p_int = coef_ci(res_c3, \"HFI_binary:many_children\")\n",
    "print(f\"[交互项] HFI × I(≥3 children) = {e_int:+.2f} pp  (95%CI {ci_int[0]:.2f}, {ci_int[1]:.2f}); p={p_int:.3f}\")\n",
    "\n",
    "# 4) 简单斜率：<3 与 ≥3 的 HFI 效应\n",
    "def simple_slope(many):\n",
    "    combo = {\"HFI_binary\": 1.0}\n",
    "    if many == 1:\n",
    "        combo[\"HFI_binary:many_children\"] = 1.0\n",
    "    return linear_combo(res_c3, combo)  # (est, (lo,hi), p)\n",
    "\n",
    "rows = []\n",
    "lab_map = {0:\"< 3 children\", 1:\"≥ 3 children\"}\n",
    "for g in [0,1]:\n",
    "    est, ci, p = simple_slope(g)\n",
    "    rows.append({\"group\": lab_map[g], \"d\": est, \"lo\": ci[0], \"hi\": ci[1], \"p\": p})\n",
    "\n",
    "effects_ch = pd.DataFrame(rows)\n",
    "\n",
    "# 5) 各组样本量\n",
    "n_by = dat_c3.groupby(\"many_children\")[\"HFI_binary\"].size()\n",
    "effects_ch[\"n\"] = effects_ch[\"group\"].map({lab_map[0]: int(n_by.get(0,0)),\n",
    "                                           lab_map[1]: int(n_by.get(1,0))})\n",
    "\n",
    "# 漂亮打印\n",
    "print(\"\\nHFI effect (insecure − secure) by children group:\")\n",
    "for r in effects_ch.itertuples():\n",
    "    sig = \"*\" if r.p < 0.05 else \"\"\n",
    "    print(f\"  {r.group:>12}: {r.d:+.2f} pp [{r.lo:.2f}, {r.hi:.2f}]  p={r.p:.3f}{sig}\")\n",
    "print(f\"\\nOverall interaction p-value = {p_int:.3f}\")\n",
    "\n",
    "# 导出成表（论文表格/附录可用）\n",
    "effects_ch.to_csv(\"output/children_interaction_effects.csv\", index=False)\n",
    "print(\"Saved: output/children_interaction_effects.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e38f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Figure Sx. HFI effect by children group (insecure − secure), 95% CI =====\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "dfp = effects_ch.copy()\n",
    "dfp[\"label\"] = dfp.apply(lambda r: f\"{r['group']} (n={int(r['n'])})\", axis=1)\n",
    "# 固定顺序：<3 在上，≥3 在下\n",
    "order = [\"< 3 children\",\"≥ 3 children\"]\n",
    "dfp = dfp.set_index(\"group\").loc[order].reset_index()\n",
    "\n",
    "ypos = np.arange(len(dfp))\n",
    "sig = dfp[\"p\"].values < 0.05\n",
    "\n",
    "fig_h = 2.8 + 0.35*len(dfp)\n",
    "fig, ax = plt.subplots(figsize=(7.0, fig_h), dpi=180)\n",
    "\n",
    "# 95% CI 横条\n",
    "for i, (lo, hi) in enumerate(zip(dfp[\"lo\"], dfp[\"hi\"])):\n",
    "    ax.hlines(ypos[i], xmin=lo, xmax=hi, lw=4.0, zorder=2)\n",
    "\n",
    "# 点：显著实心、否则空心\n",
    "ax.plot(dfp[\"d\"].values[sig],  ypos[sig],  \"o\", ms=8, zorder=3)\n",
    "ax.plot(dfp[\"d\"].values[~sig], ypos[~sig], \"o\", ms=8, mfc=\"none\", zorder=3)\n",
    "\n",
    "# 0 线、对称范围\n",
    "mx = 1.05 * max(abs(dfp[\"lo\"].min()), abs(dfp[\"hi\"].max()))\n",
    "ax.set_xlim(-mx, mx)\n",
    "ax.axvspan(-mx, 0, color=\"0.96\", zorder=0)\n",
    "ax.axvline(0, ls=(0,(3,3)), lw=1.6)\n",
    "\n",
    "ax.set_yticks(ypos); ax.set_yticklabels(dfp[\"label\"], fontsize=11)\n",
    "ax.set_xlabel(\"Δ UPF% (pp) — Insecure minus Secure\", fontsize=11)\n",
    "\n",
    "# 右侧数值\n",
    "def stars(pv): return \"***\" if pv<1e-3 else \"**\" if pv<1e-2 else \"*\" if pv<0.05 else \"\"\n",
    "for yi, r in enumerate(dfp.itertuples()):\n",
    "    ax.text(1.01, yi, f\"{r.d:.2f} [{r.lo:.2f}, {r.hi:.2f}] {stars(r.p)}\",\n",
    "            transform=ax.get_yaxis_transform(), ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "ax.set_title(f\"HFI × I(children≥3)  (p-int={p_int:.3f})\",\n",
    "             loc=\"left\", fontsize=14, weight=\"bold\", pad=6)\n",
    "fig.text(0.12, 0.02, f\"OLS (HC3); N={len(dat_c3)}; refs: Male, White, Secure\",\n",
    "         ha=\"left\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "ax.grid(axis='x', ls=':', alpha=0.35)\n",
    "ax.spines[['top','right']].set_visible(False)\n",
    "plt.tight_layout(rect=[0.22, 0.06, 0.98, 0.95])\n",
    "# 若需要保存：\n",
    "import pathlib; pathlib.Path(\"output\").mkdir(exist_ok=True)\n",
    "plt.savefig(\"output/children_interaction_forest.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f8be3",
   "metadata": {},
   "source": [
    "## 5.4 年龄每 +1SD 的效应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6358d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refit age(+1 SD) model, self-contained ===\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# 构建同 Core-A 的自变量集合\n",
    "cols = [OUTCOME, \"HFI_binary\", \"age_participant\",\n",
    "        \"gender_participant\", \"ethn_participant\", \"child_numbers\"]\n",
    "dat_ageSD = df_model[cols].copy()\n",
    "\n",
    "# 强制转数值，并把年龄限制在 3–11（数据字典口径）\n",
    "for c in [\"HFI_binary\",\"age_participant\",\"child_numbers\"]:\n",
    "    dat_ageSD[c] = pd.to_numeric(dat_ageSD[c], errors=\"coerce\")\n",
    "dat_ageSD.loc[~dat_ageSD[\"age_participant\"].between(3, 11), \"age_participant\"] = np.nan\n",
    "dat_ageSD = dat_ageSD.dropna().copy()\n",
    "\n",
    "# 标准化年龄 z_age（按样本均值/SD）\n",
    "sd_age = dat_ageSD[\"age_participant\"].std()\n",
    "mu_age = dat_ageSD[\"age_participant\"].mean()\n",
    "dat_ageSD[\"z_age\"] = (dat_ageSD[\"age_participant\"] - mu_age) / sd_age\n",
    "\n",
    "form_ageSD = (\n",
    "    f\"{OUTCOME} ~ HFI_binary + z_age\"\n",
    "    \" + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \" + child_numbers\"\n",
    ")\n",
    "res_ageSD = fit_ols(form_ageSD, dat_ageSD, cov_type=\"HC3\")\n",
    "\n",
    "age_sd_est, age_sd_ci, age_sd_p = coef_ci(res_ageSD, \"z_age\")\n",
    "print(f\"年龄每 +1SD（SD={sd_age:.2f} 岁）UPF% 变化 = {age_sd_est:.2f} pp \"\n",
    "      f\"(95%CI {age_sd_ci[0]:.2f}, {age_sd_ci[1]:.2f}), p={age_sd_p:.3f}; N={len(dat_ageSD)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Figure S?. Adjusted mean UPF% across age (with 95% CI) ===\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "# 年龄网格（按数据/字典范围）\n",
    "age_min = max(3.0, float(dat_ageSD[\"age_participant\"].min()))\n",
    "age_max = min(11.0, float(dat_ageSD[\"age_participant\"].max()))\n",
    "age_grid = np.linspace(age_min, age_max, 120)\n",
    "\n",
    "# 从模型取设计矩阵与参数\n",
    "X      = np.asarray(res_ageSD.model.exog)\n",
    "names  = list(res_ageSD.model.exog_names)\n",
    "i_age  = names.index(\"z_age\")\n",
    "beta   = np.asarray(res_ageSD.params)\n",
    "covB   = np.asarray(res_ageSD.cov_params())\n",
    "zstar  = _zcrit(0.05)\n",
    "\n",
    "means, los, his = [], [], []\n",
    "for a in age_grid:\n",
    "    z = (a - mu_age) / sd_age            # 把年龄点换成 z_age\n",
    "    Xtmp = X.copy()\n",
    "    Xtmp[:, i_age] = z                   # 仅替换年龄列，其它协变量保持样本原值\n",
    "    xbar = Xtmp.mean(axis=0)             # 平均预测 = 平均设计行 × beta\n",
    "    mu  = float(xbar @ beta)\n",
    "    var = float(xbar @ covB @ xbar)\n",
    "    se  = np.sqrt(max(var, 0.0))\n",
    "    means.append(mu); los.append(mu - zstar*se); his.append(mu + zstar*se)\n",
    "\n",
    "means, los, his = map(np.asarray, (means, los, his))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6.6, 3.8), dpi=180)\n",
    "ax.plot(age_grid, means, lw=2)\n",
    "ax.fill_between(age_grid, los, his, alpha=0.25)\n",
    "ax.set_xlim(age_min, age_max)\n",
    "ax.set_xlabel(\"Age (years)\")\n",
    "ax.set_ylabel(\"Adjusted mean UPF% (pp)\")\n",
    "ax.set_title(\"Adjusted UPF% by age (Core-A covariate-adjusted)\")\n",
    "ax.grid(axis='y', ls=':', alpha=0.35)\n",
    "ax.spines[['top','right']].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#保存\n",
    "import pathlib; pathlib.Path(\"output\").mkdir(exist_ok=True)\n",
    "plt.savefig(\"output/figureS4_age_effect.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800610a1",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7968e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在单元格里先安装\n",
    "%pip install -q XlsxWriter\n",
    "\n",
    "# 然后照原来的写法导出\n",
    "with pd.ExcelWriter(\"/content/drive/MyDrive/UPF-HFI/Model/outcome/sample_tables.xlsx\",\n",
    "                    engine=\"xlsxwriter\") as xw:\n",
    "    table2.to_excel(xw, index=False, sheet_name=\"Table2_covariates\")\n",
    "    table3A.to_excel(xw, index=False, sheet_name=\"Table3_overall\")\n",
    "    table3B.to_excel(xw, index=False, sheet_name=\"Table3_by_ethnicity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sample tables (unadjusted) from df_model ===============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _pct(n, d, digits=1):\n",
    "    return None if d==0 else round(100.0*n/d, digits)\n",
    "\n",
    "def build_sample_tables(df_model, outcome_col=\"weighted_upf_percent\"):\n",
    "    \"\"\"\n",
    "    输入：df_model（已合并的数据框）\n",
    "    输出：table2_covariates, table3_overall, table3_by_ethnicity 三个 DataFrame\n",
    "    约定：HFI_binary 1=food-insecure, 0=food-secure；gender 1=Male, 2=Female\n",
    "    \"\"\"\n",
    "\n",
    "    needed = [\"HFI_binary\", \"age_participant\", \"gender_participant\",\n",
    "              \"ethn_participant\", \"child_numbers\"]\n",
    "    missing_cols = [c for c in needed if c not in df_model.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"df_model 缺少列：{missing_cols}\")\n",
    "\n",
    "    # ---- 标准化分类值（不改你原始列，只做副本）----\n",
    "    d = df_model.copy()\n",
    "\n",
    "    # 族裔：把 'All other ethnicities' 统一成 'Other'\n",
    "    eth = d[\"ethn_participant\"].astype(\"string\").str.strip()\n",
    "    eth = eth.replace({\"All other ethnicities\":\"Other\", \"all other ethnicities\":\"Other\"})\n",
    "    # 只保留我们关注的三类；其余记为缺失\n",
    "    eth = eth.where(eth.isin([\"White\",\"Asian\",\"Other\"]))\n",
    "    d[\"eth3\"] = pd.Categorical(eth, categories=[\"White\",\"Asian\",\"Other\"])\n",
    "\n",
    "    # 性别：1=Male, 2=Female（保持与你模型一致）\n",
    "    sex_map = {1:\"Boys\", 2:\"Girls\", \"1\":\"Boys\", \"2\":\"Girls\"}\n",
    "    sex = pd.to_numeric(d[\"gender_participant\"], errors=\"coerce\").map(sex_map)\n",
    "    d[\"sex2\"] = pd.Categorical(sex, categories=[\"Boys\",\"Girls\"])\n",
    "\n",
    "    # 家庭孩子数：按 1/2/3+ 归并\n",
    "    kids = pd.to_numeric(d[\"child_numbers\"], errors=\"coerce\")\n",
    "    kids_cat = pd.Series(pd.NA, index=kids.index, dtype=\"object\")\n",
    "    kids_cat[kids==1] = \"1\"\n",
    "    kids_cat[kids==2] = \"2\"\n",
    "    kids_cat[kids>=3] = \"3+\"\n",
    "    d[\"kids3\"] = pd.Categorical(kids_cat, categories=[\"1\",\"2\",\"3+\"])\n",
    "\n",
    "    # HFI：1=不安全, 0=安全\n",
    "    hfi = pd.to_numeric(d[\"HFI_binary\"], errors=\"coerce\")\n",
    "    d[\"HFI_bin\"] = hfi\n",
    "\n",
    "    N = len(d)\n",
    "\n",
    "    # ===== Table 2: 协变量样本特征（含缺失） =====\n",
    "    rows = []\n",
    "\n",
    "    # Age\n",
    "    age = pd.to_numeric(d[\"age_participant\"], errors=\"coerce\")\n",
    "    rows.append([\"Age (years)\", \"Mean (SD)\", np.nan,\n",
    "                 f\"{np.nanmean(age):.2f} ({np.nanstd(age, ddof=1):.2f})\",\n",
    "                 f\"{age.isna().sum()} ({_pct(age.isna().sum(), N)})\"])\n",
    "\n",
    "    # Sex\n",
    "    for cat in [\"Boys\",\"Girls\"]:\n",
    "        n = (d[\"sex2\"]==cat).sum()\n",
    "        rows.append([\"Sex\", cat, int(n), f\"{_pct(n, N):.1f}\", \"\"])\n",
    "    rows.append([\"Sex\", \"Missing\", int(d[\"sex2\"].isna().sum()),\n",
    "                 f\"{_pct(d['sex2'].isna().sum(), N):.1f}\", \"\"])\n",
    "\n",
    "    # Ethnicity\n",
    "    for cat in [\"White\",\"Asian\",\"Other\"]:\n",
    "        n = (d[\"eth3\"]==cat).sum()\n",
    "        rows.append([\"Ethnicity\", cat, int(n), f\"{_pct(n, N):.1f}\", \"\"])\n",
    "    rows.append([\"Ethnicity\", \"Missing\", int(d[\"eth3\"].isna().sum()),\n",
    "                 f\"{_pct(d['eth3'].isna().sum(), N):.1f}\", \"\"])\n",
    "\n",
    "    # Number of children\n",
    "    for cat in [\"1\",\"2\",\"3+\"]:\n",
    "        n = (d[\"kids3\"]==cat).sum()\n",
    "        rows.append([\"No. of children in household\", cat, int(n), f\"{_pct(n, N):.1f}\", \"\"])\n",
    "    rows.append([\"No. of children in household\", \"Missing\", int(d[\"kids3\"].isna().sum()),\n",
    "                 f\"{_pct(d['kids3'].isna().sum(), N):.1f}\", \"\"])\n",
    "\n",
    "    table2 = pd.DataFrame(rows, columns=[\"Variable\",\"Category\",\"n\",\"% / value\",\"Missing, n (%)\"])\n",
    "\n",
    "    # ===== Table 3: HFI 总体 + 按族裔 =====\n",
    "    # A. Overall\n",
    "    n_insec = int((d[\"HFI_bin\"]==1).sum())\n",
    "    n_sec   = int((d[\"HFI_bin\"]==0).sum())\n",
    "    n_hfi_miss = int(d[\"HFI_bin\"].isna().sum())\n",
    "    table3_overall = pd.DataFrame({\n",
    "        \"HFI status\":[\"Food insecure\",\"Food secure\",\"Missing\"],\n",
    "        \"n\":[n_insec, n_sec, n_hfi_miss],\n",
    "        \"%\":[_pct(n_insec,N), _pct(n_sec,N), _pct(n_hfi_miss,N)]\n",
    "    })\n",
    "\n",
    "    # B. By ethnicity (within-ethnicity percentages)\n",
    "    by_rows = []\n",
    "    for g in [\"White\",\"Asian\",\"Other\"]:\n",
    "        sub = d.loc[d[\"eth3\"]==g]\n",
    "        ntot = len(sub)\n",
    "        if ntot==0:\n",
    "            by_rows.append([g, 0, None, 0, 0])\n",
    "            continue\n",
    "        nin = int((sub[\"HFI_bin\"]==1).sum())\n",
    "        nse = int((sub[\"HFI_bin\"]==0).sum())\n",
    "        by_rows.append([g, nin, _pct(nin, ntot), nse, ntot])\n",
    "    table3_by_eth = pd.DataFrame(by_rows,\n",
    "                                 columns=[\"Ethnicity\",\"Food insecure, n\",\"% within ethnicity\",\n",
    "                                          \"Food secure, n\",\"n total\"])\n",
    "\n",
    "    # 美化百分比显示（None 保留为空白）\n",
    "    for col in [\"%\",\"% within ethnicity\",\"% / value\"]:\n",
    "        if col in table2.columns:\n",
    "            pass\n",
    "        if col in table3_overall.columns:\n",
    "            table3_overall[col] = table3_overall[col].map(lambda x: \"\" if x is None else f\"{x:.1f}\")\n",
    "        if col in table3_by_eth.columns:\n",
    "            table3_by_eth[col] = table3_by_eth[col].map(lambda x: \"\" if x is None else f\"{x:.1f}\")\n",
    "\n",
    "    return table2, table3_overall, table3_by_eth\n",
    "\n",
    "# —— 用法（df_model 已在你上面的代码里生成） ——\n",
    "table2, table3A, table3B = build_sample_tables(df_model)\n",
    "\n",
    "# 可选：导出到 Excel\n",
    "with pd.ExcelWriter(\"/content/drive/MyDrive/UPF-HFI/Model/outcome/sample_tables.xlsx\",\n",
    "                    engine=\"xlsxwriter\") as xw:\n",
    "    table2.to_excel(xw, index=False, sheet_name=\"Table2_covariates\")\n",
    "    table3A.to_excel(xw, index=False, sheet_name=\"Table3_overall\")\n",
    "    table3B.to_excel(xw, index=False, sheet_name=\"Table3_by_ethnicity\")\n",
    "\n",
    "print(\"已生成：Table 2（协变量）、Table 3A（HFI总体）、Table 3B（HFI×族裔）。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 统一口径：使用 Core-A 的分析样本 -----\n",
    "OUTCOME = \"weighted_upf_percent\"\n",
    "keep_cols = [\"UserID_clean\", OUTCOME, \"HFI_binary\",\n",
    "             \"age_participant\", \"gender_participant\",\n",
    "             \"ethn_participant\", \"child_numbers\"]\n",
    "\n",
    "d = df_model[keep_cols].copy()\n",
    "\n",
    "# 数值化 & 丢失统一处理（保证与 Core-A 一致）\n",
    "for c in [OUTCOME, \"HFI_binary\", \"age_participant\", \"gender_participant\", \"child_numbers\"]:\n",
    "    d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "# UPF 合法范围\n",
    "d.loc[~d[OUTCOME].between(0, 100), OUTCOME] = np.nan\n",
    "\n",
    "# 丢失：六列都不能缺\n",
    "d = d.dropna(subset=[OUTCOME, \"HFI_binary\", \"age_participant\",\n",
    "                     \"gender_participant\", \"ethn_participant\", \"child_numbers\"]).copy()\n",
    "\n",
    "# 合并后若同一ID重复，保留第一条\n",
    "d = d.drop_duplicates(subset=\"UserID_clean\").copy()\n",
    "\n",
    "# 族裔三类（与 3.1 完全一致）\n",
    "eth = d[\"ethn_participant\"].astype(\"string\").str.strip()\n",
    "eth = eth.replace({\"All other ethnicities\":\"Other\", \"all other ethnicities\":\"Other\"})\n",
    "eth = eth.where(eth.isin([\"White\",\"Asian\",\"Other\"]))\n",
    "d[\"eth3\"] = pd.Categorical(eth, categories=[\"White\",\"Asian\",\"Other\"])\n",
    "\n",
    "# ------------- 产出 Table 3（总体 + 按族裔，未调整） -------------\n",
    "N = len(d)\n",
    "# A. Overall\n",
    "tbl_overall = d[\"HFI_binary\"].value_counts().reindex([1,0], fill_value=0)\n",
    "table3_overall = pd.DataFrame({\n",
    "    \"HFI status\": [\"Food insecure\", \"Food secure\"],\n",
    "    \"n\": [int(tbl_overall.loc[1]), int(tbl_overall.loc[0])],\n",
    "    \"%\": [round(100*tbl_overall.loc[1]/N, 1), round(100*tbl_overall.loc[0]/N, 1)]\n",
    "})\n",
    "\n",
    "# B. By ethnicity（行内百分比）\n",
    "ct = pd.crosstab(d[\"eth3\"], d[\"HFI_binary\"]).reindex(index=[\"White\",\"Asian\",\"Other\"],\n",
    "                                                     columns=[1,0], fill_value=0)\n",
    "ct.index.name = \"Ethnicity\"   # 避免 KeyError\n",
    "table3_by_eth = (ct.rename(columns={1:\"Food insecure, n\", 0:\"Food secure, n\"})\n",
    "                   .reset_index())\n",
    "table3_by_eth[\"n total\"] = table3_by_eth[\"Food insecure, n\"] + table3_by_eth[\"Food secure, n\"]\n",
    "table3_by_eth[\"% within ethnicity\"] = (100*table3_by_eth[\"Food insecure, n\"] /\n",
    "                                       table3_by_eth[\"n total\"]).round(1)\n",
    "\n",
    "# ------------- 审计核对（必须全部 True / 正确数） -------------\n",
    "print(\"N（应为 304）:\", N)\n",
    "print(\"族裔合计（应为 304）:\", int(table3_by_eth[\"n total\"].sum()))\n",
    "print(\"各族裔 n total（应为 [167, 99, 38]）:\",\n",
    "      table3_by_eth[\"n total\"].astype(int).tolist())\n",
    "print(\"核对：不安全人数分层之和 == 总体不安全人数：\",\n",
    "      int(table3_by_eth[\"Food insecure, n\"].sum()) == int(table3_overall.loc[0,\"n\"]))\n",
    "\n",
    "# ------------- 显示（不会再报 Ethnicity KeyError） -------------\n",
    "print(\"\\nTable 3A — overall\\n\", table3_overall)\n",
    "print(\"\\nTable 3B — by ethnicity (row %)\\n\",\n",
    "      table3_by_eth[[\"Ethnicity\",\"Food insecure, n\",\"% within ethnicity\",\n",
    "                     \"Food secure, n\",\"n total\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用前面统一口径后的 d（N=304；含 OUTCOME='weighted_upf_percent' 和 d['eth3']=White/Asian/Other）\n",
    "\n",
    "g = d.groupby(\"eth3\")[\"weighted_upf_percent\"]\n",
    "table_upf_by_eth = (\n",
    "    g.agg(n=\"count\",\n",
    "          mean=lambda x: x.mean(),\n",
    "          sd=lambda x: x.std(ddof=1))\n",
    "     .reset_index()\n",
    "     .rename(columns={\"eth3\":\"Ethnicity\",\n",
    "                      \"mean\":\"Mean UPF% (unadjusted)\",\n",
    "                      \"sd\":\"SD\"})\n",
    ")\n",
    "\n",
    "# 可选：整体一行\n",
    "overall = d[\"weighted_upf_percent\"].agg(n=\"count\", mean=\"mean\", sd=lambda x: x.std(ddof=1))\n",
    "overall.name = \"Overall\"\n",
    "print(table_upf_by_eth)\n",
    "print(\"\\nOverall:\", overall.to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3790b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 已有 d（N=304；含 d['eth3'] 和 'weighted_upf_percent'）\n",
    "table_upf_by_eth = (\n",
    "    d.groupby(\"eth3\")[\"weighted_upf_percent\"]\n",
    "     .agg(n=\"count\", mean=\"mean\", sd=lambda x: x.std(ddof=1))\n",
    "     .reset_index().rename(columns={\"eth3\":\"Ethnicity\",\n",
    "                                    \"mean\":\"Mean UPF% (unadjusted)\",\n",
    "                                    \"sd\":\"SD\"})\n",
    ")\n",
    "table_upf_by_eth[\"Mean UPF% (unadjusted)\"] = table_upf_by_eth[\"Mean UPF% (unadjusted)\"].round(1)\n",
    "table_upf_by_eth[\"SD\"] = table_upf_by_eth[\"SD\"].round(2)\n",
    "\n",
    "# 导出\n",
    "with pd.ExcelWriter(\"/content/drive/MyDrive/UPF-HFI/Model/outcome/unadjusted_UPF_by_ethnicity.xlsx\",\n",
    "                    engine=\"openpyxl\") as xw:\n",
    "    table_upf_by_eth.to_excel(xw, index=False, sheet_name=\"UPF_by_ethnicity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a2f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Core-A 族裔分层 HFI 构成表（与回归样本完全一致） =====\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "OUTCOME = \"weighted_upf_percent\"\n",
    "assert OUTCOME in df_model.columns\n",
    "\n",
    "keep_cols = [\"UserID_clean\", OUTCOME, \"HFI_binary\",\n",
    "             \"age_participant\", \"gender_participant\",\n",
    "             \"ethn_participant\", \"child_numbers\"]\n",
    "d = df_model[keep_cols].copy()\n",
    "\n",
    "# 数值化统一\n",
    "for c in [OUTCOME, \"HFI_binary\", \"age_participant\", \"gender_participant\", \"child_numbers\"]:\n",
    "    d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "\n",
    "# 目标变量合法范围（0–100）\n",
    "d.loc[~d[OUTCOME].between(0, 100), OUTCOME] = np.nan\n",
    "\n",
    "# Core-A 完整案例（六列都不缺）\n",
    "d = d.dropna(subset=[OUTCOME, \"HFI_binary\", \"age_participant\",\n",
    "                     \"gender_participant\", \"ethn_participant\", \"child_numbers\"]).copy()\n",
    "\n",
    "# 若同一 ID 多条，仅保留第一条\n",
    "if \"UserID_clean\" in d.columns:\n",
    "    d = d.drop_duplicates(subset=\"UserID_clean\").copy()\n",
    "\n",
    "# 族裔三类（与 3.1 一致）\n",
    "eth = d[\"ethn_participant\"].astype(\"string\").str.strip()\n",
    "eth = eth.replace({\"All other ethnicities\": \"Other\", \"all other ethnicities\": \"Other\"})\n",
    "eth = eth.where(eth.isin([\"White\", \"Asian\", \"Other\"]))\n",
    "d[\"Ethnicity\"] = pd.Categorical(eth, categories=[\"White\", \"Asian\", \"Other\"])\n",
    "\n",
    "# —— A) 总体（未调整）——\n",
    "overall = d[\"HFI_binary\"].value_counts().reindex([1, 0], fill_value=0)\n",
    "table3_overall = pd.DataFrame({\n",
    "    \"HFI status\": [\"Food insecure\", \"Food secure\"],\n",
    "    \"n\": [int(overall.loc[1]), int(overall.loc[0])],\n",
    "    \"%\": [round(100*overall.loc[1]/len(d), 1), round(100*overall.loc[0]/len(d), 1)]\n",
    "})\n",
    "\n",
    "# —— B) 按族裔（行内百分比）——\n",
    "ct = pd.crosstab(d[\"Ethnicity\"], d[\"HFI_binary\"]).reindex(index=[\"White\", \"Asian\", \"Other\"],\n",
    "                                                          columns=[1, 0], fill_value=0)\n",
    "ct = ct.rename(columns={1: \"Food insecure, n\", 0: \"Food secure, n\"}).reset_index()\n",
    "ct[\"n total\"] = ct[\"Food insecure, n\"] + ct[\"Food secure, n\"]\n",
    "ct[\"% within ethnicity\"] = (100 * ct[\"Food insecure, n\"] / ct[\"n total\"]).round(1)\n",
    "\n",
    "# 列顺序与截图一致\n",
    "table3_by_ethnicity = ct[[\"Ethnicity\", \"Food insecure, n\", \"% within ethnicity\",\n",
    "                          \"Food secure, n\", \"n total\"]]\n",
    "\n",
    "# ===== 打印审计信息 =====\n",
    "print(f\"N（Core-A 分析样本）: {len(d)}\")\n",
    "print(\"各族裔 n total:\", table3_by_ethnicity[\"n total\"].astype(int).tolist())\n",
    "print(\"总体不安全人数:\", int(table3_overall.loc[0, \"n\"]))\n",
    "print(\"分层不安全人数之和:\", int(table3_by_ethnicity[\"Food insecure, n\"].sum()))\n",
    "print(\"\\nTable 3B — by ethnicity (row %)\\n\", table3_by_ethnicity)\n",
    "print(\"\\nTable 3A — overall\\n\", table3_overall)\n",
    "\n",
    "# ===== 保存为 Excel =====\n",
    "# 如需与既有文件合并，可把路径改成 sample_tables.xlsx 并调整写入模式\n",
    "out_path = \"/content/drive/MyDrive/UPF-HFI/Model/outcome/Table3_coreA.xlsx\"\n",
    "\n",
    "# 需要 XlsxWriter（在 Colab 可先:  %pip install -q XlsxWriter）\n",
    "with pd.ExcelWriter(out_path, engine=\"xlsxwriter\") as writer:\n",
    "    table3_overall.to_excel(writer, index=False, sheet_name=\"Table3_overall\")\n",
    "    table3_by_ethnicity.to_excel(writer, index=False, sheet_name=\"Table3_by_ethnicity\")\n",
    "\n",
    "print(\"\\nSaved to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Table 4 — HFI three-category: means & stepwise changes =====\n",
    "import pandas as pd, os\n",
    "\n",
    "# —— 输入（采用你图/文中的数值） ——\n",
    "means = {\n",
    "    \"Food secure\":            {\"mean\":71.0, \"ci\":\"68.8–73.2\", \"n\":208},\n",
    "    \"Low food security\":      {\"mean\":71.9, \"ci\":\"67.7–76.1\", \"n\":49},\n",
    "    \"Very low food security\": {\"mean\":74.8, \"ci\":\"70.9–78.7\", \"n\":47},\n",
    "}\n",
    "# 相邻层级的对比（来自你的标注：Low−Secure=+0.9, Very low−Low=+2.9）\n",
    "adjacent_deltas = {\n",
    "    \"Low food security\":      {\"delta\": +0.9, \"p\": 0.668},   # Low vs Secure\n",
    "    \"Very low food security\": {\"delta\": +2.9, \"p\": 0.243},   # Very low vs Low\n",
    "}\n",
    "\n",
    "rows = []\n",
    "order = [\"Food secure\", \"Low food security\", \"Very low food security\"]\n",
    "base_mean = means[\"Food secure\"][\"mean\"]\n",
    "\n",
    "for i, cat in enumerate(order):\n",
    "    m = means[cat][\"mean\"]\n",
    "    ci = means[cat][\"ci\"]\n",
    "    n  = means[cat][\"n\"]\n",
    "    # 相邻层级的增量（第一行无相邻对比）\n",
    "    if i == 0:\n",
    "        step_delta, p_step = None, None\n",
    "    else:\n",
    "        step_delta = adjacent_deltas[cat][\"delta\"]\n",
    "        p_step     = adjacent_deltas[cat][\"p\"]\n",
    "    # 累计相对 Food secure 的增量（描述性）\n",
    "    cum_delta = None if i == 0 else (m - base_mean)\n",
    "\n",
    "    rows.append({\n",
    "        \"HFI category\": cat,\n",
    "        \"Adjusted mean UPF%\": f\"{m:.1f}\",\n",
    "        \"95% CI\": ci,\n",
    "        \"n\": int(n),\n",
    "        \"Δ vs previous level (pp)\": \"—\" if step_delta is None else f\"{step_delta:+.1f}\",\n",
    "        \"p (vs previous level)\":    \"—\" if p_step is None     else f\"{p_step:.3f}\",\n",
    "        \"Cumulative Δ vs Food secure (pp)\": \"—\" if cum_delta is None else f\"{cum_delta:+.1f}\",\n",
    "    })\n",
    "\n",
    "table4_steps = pd.DataFrame(rows)\n",
    "\n",
    "# —— 审计打印 ——\n",
    "print(\"Sum of n (should be 304):\", int(pd.to_numeric(table4_steps[\"n\"], errors=\"coerce\").sum()))\n",
    "print(\"\\nTable 4 — HFI three-category: means & stepwise changes\\n\", table4_steps)\n",
    "\n",
    "# —— 追加写入到 Excel ——\n",
    "out_path = \"/content/drive/MyDrive/UPF-HFI/Model/outcome/sample_tables.xlsx\"\n",
    "mode = \"a\" if os.path.exists(out_path) else \"w\"\n",
    "try:\n",
    "    import openpyxl  # to enable append mode\n",
    "    with pd.ExcelWriter(out_path, engine=\"openpyxl\", mode=mode, if_sheet_exists=\"replace\") as xw:\n",
    "        table4_steps.to_excel(xw, index=False, sheet_name=\"Table4_threecat_steps\")\n",
    "    print(f\"\\nSaved to: {out_path} -> [Table4_threecat_steps]\")\n",
    "except Exception as e:\n",
    "    # 备选保存\n",
    "    alt_path = \"/content/drive/MyDrive/UPF-HFI/Model/outcome/Table4_threecat_steps.xlsx\"\n",
    "    with pd.ExcelWriter(alt_path, engine=\"xlsxwriter\") as xw:\n",
    "        table4_steps.to_excel(xw, index=False, sheet_name=\"Table4_threecat_steps\")\n",
    "    print(f\"\\nopenpyxl append failed ({e}). Saved to: {alt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4e7654",
   "metadata": {},
   "source": [
    "# 作废"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1afd67",
   "metadata": {},
   "source": [
    "# 6. Table（描述性摘要）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2509a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# === 列名映射（按你的表来；不一致就改这里） ===\n",
    "COL = dict(\n",
    "    y=\"weighted_upf_percent\",\n",
    "    HFI_bin=\"HFI_binary\",               # 0=secure, 1=insecure\n",
    "    age=\"age_participant\",\n",
    "    sex=\"gender_participant\",           # Male=1, Female=0（Male参照）\n",
    "    ethn=\"ethn_participant\",            # White / Asian / All other ethnicities\n",
    "    children=\"child_numbers\",\n",
    "    income=\"income\",\n",
    "    employ=\"employ\",\n",
    "    HFI_3=\"HFI_3cat\"                    # 可选：三分类 HFI\n",
    ")\n",
    "\n",
    "# 统一族裔标签 & 顺序\n",
    "DF = df.copy()  # 你的DataFrame\n",
    "DF[COL[\"ethn\"]] = DF[COL[\"ethn\"]].replace({\"All other ethnicities\":\"Other\"})\n",
    "eth_order = pd.CategoricalDtype(categories=[\"White\",\"Asian\",\"Other\"], ordered=True)\n",
    "DF[COL[\"ethn\"]] = DF[COL[\"ethn\"]].astype(eth_order)\n",
    "\n",
    "# UPF% 越界置缺\n",
    "DF.loc[(DF[COL[\"y\"]]<0)|(DF[COL[\"y\"]]>100), COL[\"y\"]] = np.nan\n",
    "\n",
    "# 格式与显著性\n",
    "def pstar(p):\n",
    "    return '***' if p<0.001 else '**' if p<0.01 else '*' if p<0.05 else ''\n",
    "def fmt_ci(b, lo, hi, p):\n",
    "    return f\"{b:.1f} [{lo:.1f}, {hi:.1f}]; p={p:.3f}{pstar(p)}\"\n",
    "def mean_sd_n(x):\n",
    "    return pd.Series({\"mean\":x.mean(), \"sd\":x.std(), \"n\":x.notna().sum()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "# Overall\n",
    "rows.append((\"Overall\",)+tuple(mean_sd_n(DF[COL[\"y\"]]).values))\n",
    "# By HFI\n",
    "for v,label in [(0,\"Food secure\"),(1,\"Food insecure\")]:\n",
    "    s = DF.loc[DF[COL[\"HFI_bin\"]]==v, COL[\"y\"]]\n",
    "    rows.append((f\"By HFI — {label}\",)+tuple(mean_sd_n(s).values))\n",
    "# By ethnicity（按预设顺序）\n",
    "for g in eth_order.categories:\n",
    "    s = DF.loc[DF[COL[\"ethn\"]]==g, COL[\"y\"]]\n",
    "    rows.append((f\"By ethnicity — {g}\",)+tuple(mean_sd_n(s).values))\n",
    "\n",
    "tbl2 = pd.DataFrame(rows, columns=[\"Stratum\",\"mean\",\"sd\",\"n\"])\n",
    "tbl2[\"UPF% mean (SD)\"] = tbl2.apply(lambda r:f\"{r['mean']:.1f} ({r['sd']:.1f})\", axis=1)\n",
    "tbl2 = tbl2[[\"Stratum\",\"UPF% mean (SD)\",\"n\"]]\n",
    "\n",
    "# 好看一点的导出\n",
    "tbl2.to_csv(\"table2_unadjusted_upf.csv\", index=False)\n",
    "print(\"Saved: table2_unadjusted_upf.csv\")\n",
    "tbl2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ONE-CELL: 生成 “表3（已调整）” 全量两列表 ======\n",
    "import numpy as np, pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# -------- 小工具 --------\n",
    "def fit_ols(formula, data, cov_type=\"HC3\"):\n",
    "    return smf.ols(formula, data=data).fit(cov_type=cov_type)\n",
    "\n",
    "def coef_ci(res, name):\n",
    "    \"\"\"返回 (b, lo, hi, p)；若找不到参数名直接抛错，便于定位。\"\"\"\n",
    "    b = res.params[name]\n",
    "    lo, hi = res.conf_int().loc[name]\n",
    "    p = res.pvalues[name]\n",
    "    return float(b), float(lo), float(hi), float(p)\n",
    "\n",
    "def fmt(b, lo, hi, p):\n",
    "    return f\"{b:.1f} [{lo:.1f}, {hi:.1f}]; p={p:.3f}\"\n",
    "\n",
    "def wald_p_for_interactions(res, names):\n",
    "    R = np.zeros((len(names), len(res.params)))\n",
    "    for i, nm in enumerate(names):\n",
    "        R[i, list(res.params.index).index(nm)] = 1.0\n",
    "    return float(res.wald_test(R).pvalue)\n",
    "\n",
    "# -------- 数据准备（与你当前环境一致）--------\n",
    "DF = df_model.copy()                   # 用你的 df_model\n",
    "OUTCOME = OUTCOME                      # 已在你环境中定义\n",
    "\n",
    "# 统一族裔标签\n",
    "DF[\"ethn_participant\"] = DF[\"ethn_participant\"].replace({\"All other ethnicities\":\"Other\"})\n",
    "eth_levels = [\"White\",\"Asian\",\"Other\"]\n",
    "\n",
    "# UPF% 越界置缺\n",
    "DF.loc[(DF[OUTCOME] < 0) | (DF[OUTCOME] > 100), OUTCOME] = np.nan\n",
    "\n",
    "# -------- Core-A --------\n",
    "form_coreA = (\n",
    "    f\"{OUTCOME} ~ HFI_binary + age_participant\"\n",
    "    \" + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \" + child_numbers\"\n",
    ")\n",
    "need_coreA = [OUTCOME,\"HFI_binary\",\"age_participant\",\"gender_participant\",\"ethn_participant\",\"child_numbers\"]\n",
    "datA = DF[need_coreA].dropna().copy()\n",
    "\n",
    "resA = fit_ols(form_coreA, datA)\n",
    "b1, lo1, hi1, p1 = coef_ci(resA, \"HFI_binary\")\n",
    "\n",
    "# -------- Core-B（HFI×ethnicity）--------\n",
    "form_coreB = (\n",
    "    f\"{OUTCOME} ~ HFI_binary*C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \" + age_participant + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + child_numbers\"\n",
    ")\n",
    "resB = fit_ols(form_coreB, datA)\n",
    "\n",
    "# White 层 = 主效应；Asian/Other = 主效应 + 对应交互\n",
    "param_idx = resB.params.index.tolist()\n",
    "def inter_name(level):\n",
    "    for nm in param_idx:\n",
    "        if nm.startswith(\"HFI_binary:\") and f\"[T.{level}]\" in nm:\n",
    "            return nm\n",
    "    raise KeyError(f\"找不到交互项：{level}\")\n",
    "\n",
    "by_eth = []\n",
    "# White\n",
    "by_eth.append((\"White\",) + coef_ci(resB, \"HFI_binary\"))\n",
    "# Asian / Other\n",
    "for lvl in [\"Asian\",\"Other\"]:\n",
    "    nm = inter_name(lvl)\n",
    "    # 线性组合 β_HFI + β_(HFI×lvl)\n",
    "    L = np.zeros(len(resB.params))\n",
    "    L[param_idx.index(\"HFI_binary\")] = 1.0\n",
    "    L[param_idx.index(nm)] += 1.0\n",
    "    est = float(L @ resB.params)\n",
    "    se  = float(np.sqrt(L @ resB.cov_params() @ L))\n",
    "    z   = est/se\n",
    "    p   = 2*stats.norm.sf(abs(z))\n",
    "    lo, hi = est-1.96*se, est+1.96*se\n",
    "    by_eth.append((lvl, est, lo, hi, p))\n",
    "\n",
    "p_inter = wald_p_for_interactions(resB, [inter_name(\"Asian\"), inter_name(\"Other\")])\n",
    "\n",
    "# -------- 敏感性：HFI 三分类 + 趋势 --------\n",
    "sens_rows = []\n",
    "bT=loT=hiT=pT=np.nan\n",
    "if \"HFI_category\" in DF.columns:        # 你的列名\n",
    "    # 规范顺序\n",
    "    order = [\"Food secure\",\"Low food security\",\"Very low food security\"]\n",
    "    DF = DF.copy()\n",
    "    DF = DF[need_coreA + [\"HFI_category\"]].dropna().copy()\n",
    "    DF[\"HFI_category\"] = pd.Categorical(DF[\"HFI_category\"], categories=order, ordered=True)\n",
    "\n",
    "    # (a) 类别模型（参照 Food secure）\n",
    "    form_cat = (\n",
    "        f\"{OUTCOME} ~ C(HFI_category, Treatment(reference='Food secure'))\"\n",
    "        \" + age_participant + C(gender_participant, Treatment(reference=1))\"\n",
    "        \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "        \" + child_numbers\"\n",
    "    )\n",
    "    res_cat = fit_ols(form_cat, DF)\n",
    "    for lev, lab in [(\"Low food security\",\"Low vs secure\"),\n",
    "                     (\"Very low food security\",\"Very low vs secure\")]:\n",
    "        nm = f\"C(HFI_category, Treatment(reference='Food secure'))[T.{lev}]\"\n",
    "        sens_rows.append((lab,) + coef_ci(res_cat, nm))\n",
    "\n",
    "    # (b) 线性趋势：0/1/2\n",
    "    tr_map = {\"Food secure\":0, \"Low food security\":1, \"Very low food security\":2}\n",
    "    DF[\"_HFI_trend\"] = DF[\"HFI_category\"].map(tr_map).astype(float)\n",
    "    form_tr = (\n",
    "        f\"{OUTCOME} ~ _HFI_trend + age_participant\"\n",
    "        \" + C(gender_participant, Treatment(reference=1))\"\n",
    "        \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "        \" + child_numbers\"\n",
    "    )\n",
    "    res_tr = fit_ols(form_tr, DF)\n",
    "    bT, loT, hiT, pT = coef_ci(res_tr, \"_HFI_trend\")\n",
    "\n",
    "# -------- SES 稳健性：same-N / fresh-N --------\n",
    "rows_ses = []\n",
    "if {\"income\",\"employ\"}.issubset(df_model.columns):\n",
    "    # same-N\n",
    "    ses_cols = need_coreA + [\"income\",\"employ\"]\n",
    "    dat_same = df_model[ses_cols].copy()\n",
    "    for c in [\"HFI_binary\",\"age_participant\",\"child_numbers\",\"income\"]:\n",
    "        dat_same[c] = pd.to_numeric(dat_same[c], errors=\"coerce\")\n",
    "    dat_same = dat_same.dropna().copy()\n",
    "\n",
    "    res_core_same = fit_ols(form_coreA, dat_same)\n",
    "    form_plusSES  = form_coreA + \" + income + C(employ)\"\n",
    "    res_plus_same = fit_ols(form_plusSES, dat_same)\n",
    "\n",
    "    b_coreS, lo_coreS, hi_coreS, p_coreS = coef_ci(res_core_same,  \"HFI_binary\")\n",
    "    b_SES_S, lo_SES_S, hi_SES_S, p_SES_S = coef_ci(res_plus_same, \"HFI_binary\")\n",
    "    pct_change = (b_SES_S - b_coreS)/b_coreS*100 if b_coreS!=0 else np.nan\n",
    "\n",
    "    rows_ses.append((\"SES robustness — same-N (β1 change, %; N)\",\n",
    "                     f\"{pct_change:.1f}% ; N={len(dat_same)}\"))\n",
    "\n",
    "    # fresh-N（各自可用样本）\n",
    "    dat_coreF = df_model[need_coreA].copy()\n",
    "    for c in [\"HFI_binary\",\"age_participant\",\"child_numbers\"]:\n",
    "        dat_coreF[c] = pd.to_numeric(dat_coreF[c], errors=\"coerce\")\n",
    "    dat_coreF = dat_coreF.dropna().copy()\n",
    "\n",
    "    dat_SESF  = df_model[ses_cols].copy()\n",
    "    for c in [\"HFI_binary\",\"age_participant\",\"child_numbers\",\"income\"]:\n",
    "        dat_SESF[c] = pd.to_numeric(dat_SESF[c], errors=\"coerce\")\n",
    "    dat_SESF = dat_SESF.dropna().copy()\n",
    "\n",
    "    res_coreF = fit_ols(form_coreA,  dat_coreF)\n",
    "    res_SESF  = fit_ols(form_plusSES, dat_SESF)\n",
    "    b_SESF, lo_SESF, hi_SESF, p_SESF = coef_ci(res_SESF, \"HFI_binary\")\n",
    "else:\n",
    "    b_SESF=lo_SESF=hi_SESF=p_SESF=np.nan\n",
    "\n",
    "# -------- 组装两列表 --------\n",
    "rows3 = []\n",
    "rows3.append((\"Core-A: insecure − secure (pp)\", fmt(b1, lo1, hi1, p1)))\n",
    "\n",
    "rows3.append((\"Core-B (HFI×ethnicity):\", \"\"))\n",
    "for lbl, b, lo, hi, p in by_eth:\n",
    "    rows3.append((f\"  {lbl}: insecure − secure\", fmt(b, lo, hi, p)))\n",
    "rows3.append((\"  p-interaction (Wald)\", f\"{p_inter:.3f}\"))\n",
    "\n",
    "# 敏感性\n",
    "if len(sens_rows)>0:\n",
    "    rows3.append((\"Sensitivity (HFI three-level):\", \"\"))\n",
    "    for lab, b, lo, hi, p in sens_rows:\n",
    "        rows3.append((f\"  {lab}\", fmt(b, lo, hi, p)))\n",
    "    rows3.append((\"  Linear trend (0→2, pp/level)\", fmt(bT, loT, hiT, pT)))\n",
    "\n",
    "# SES 稳健性\n",
    "if len(rows_ses)>0:\n",
    "    rows3.extend(rows_ses)\n",
    "    if not np.isnan(b_SESF):\n",
    "        rows3.append((\"SES robustness — fresh-N (β1 + SES; N)\",\n",
    "                      f\"{b_SESF:.1f} [{lo_SESF:.1f}, {hi_SESF:.1f}]; p={p_SESF:.3f}; N={len(dat_SESF)}\"))\n",
    "\n",
    "tbl3 = pd.DataFrame(rows3, columns=[\"Contrast / Estimand\", \"Value\"])\n",
    "tbl3.to_csv(\"table3_adjusted_effects.csv\", index=False)\n",
    "print(\"Saved: table3_adjusted_effects.csv\")\n",
    "tbl3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca252d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在之前生成 tbl2 的基础上，追加“按子女数”分层\n",
    "d = DF.copy()\n",
    "d[\"child_ge3\"] = (pd.to_numeric(d[\"child_numbers\"], errors=\"coerce\") >= 3)\n",
    "\n",
    "def _mean_sd_n(x):\n",
    "    return pd.Series({\"mean\":x.mean(), \"sd\":x.std(), \"n\":x.notna().sum()})\n",
    "\n",
    "add_rows = []\n",
    "for v,lab in [(False,\"By children — <3\"), (True,\"By children — ≥3\")]:\n",
    "    sub = d.loc[d[\"child_ge3\"]==v, OUTCOME]\n",
    "    s = _mean_sd_n(sub)\n",
    "    add_rows.append((lab, f\"{s['mean']:.1f} ({s['sd']:.1f})\", int(s['n'])))\n",
    "\n",
    "# 也可用 0/1/2/3+ 的四档：把上面两行替换为下面注释\n",
    "# d[\"child_4lvl\"] = pd.cut(pd.to_numeric(d[\"child_numbers\"], errors=\"coerce\"),\n",
    "#                          bins=[-0.5,0.5,1.5,2.5,100], labels=[\"0\",\"1\",\"2\",\"3+\"])\n",
    "# for g, subdf in d.groupby(\"child_4lvl\", observed=True):\n",
    "#     s = _mean_sd_n(subdf[OUTCOME]); add_rows.append((f\"By children — {g}\", f\"{s['mean']:.1f} ({s['sd']:.1f})\", int(s['n'])))\n",
    "\n",
    "tbl2_children = pd.DataFrame(add_rows, columns=[\"Stratum\",\"UPF% mean (SD)\",\"n\"])\n",
    "tbl2 = pd.concat([tbl2, tbl2_children], ignore_index=True)\n",
    "tbl2.to_csv(\"table2_unadjusted_upf.csv\", index=False)\n",
    "tbl2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在生成 tbl3 之后追加 “HFI × I(children ≥3)” 交互的两行与整体 p 值\n",
    "from scipy import stats\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "dat_c3 = df_model[[OUTCOME,\"HFI_binary\",\"age_participant\",\"gender_participant\",\n",
    "                   \"ethn_participant\",\"child_numbers\"]].copy()\n",
    "for c in [\"HFI_binary\",\"age_participant\",\"child_numbers\"]:\n",
    "    dat_c3[c] = pd.to_numeric(dat_c3[c], errors=\"coerce\")\n",
    "dat_c3[\"many_children\"] = (dat_c3[\"child_numbers\"] >= 3).astype(\"int\")\n",
    "dat_c3 = dat_c3.dropna().copy()\n",
    "\n",
    "form_c3 = (\n",
    "    f\"{OUTCOME} ~ HFI_binary + many_children + HFI_binary:many_children\"\n",
    "    \" + age_participant + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \" + child_numbers\"\n",
    ")\n",
    "res_c3 = smf.ols(form_c3, data=dat_c3).fit(cov_type=\"HC3\")\n",
    "\n",
    "# 交互项\n",
    "b_int = res_c3.params[\"HFI_binary:many_children\"]\n",
    "se   = np.sqrt(res_c3.cov_params().loc[\"HFI_binary:many_children\",\"HFI_binary:many_children\"])\n",
    "lo, hi = b_int - 1.96*se, b_int + 1.96*se\n",
    "p_int  = res_c3.pvalues[\"HFI_binary:many_children\"]\n",
    "\n",
    "# <3 的 HFI 效应 = β_HFI； ≥3 的 HFI 效应 = β_HFI + 交互\n",
    "b0, lo0, hi0, p0 = coef_ci(res_c3, \"HFI_binary\")\n",
    "L = np.zeros(len(res_c3.params)); idx = list(res_c3.params.index)\n",
    "L[idx.index(\"HFI_binary\")] = 1; L[idx.index(\"HFI_binary:many_children\")] += 1\n",
    "est = float(L @ res_c3.params); se = float(np.sqrt(L @ res_c3.cov_params() @ L))\n",
    "z = est/se; p1 = 2*stats.norm.sf(abs(z)); lo1, hi1 = est-1.96*se, est+1.96*se\n",
    "\n",
    "rows_children = [\n",
    "    (\"Children interaction (HFI × I≥3):\", \"\"),\n",
    "    (\"  <3 children: insecure − secure\", f\"{b0:.1f} [{lo0:.1f}, {hi0:.1f}]; p={p0:.3f}\"),\n",
    "    (\"  ≥3 children: insecure − secure\",  f\"{est:.1f} [{lo1:.1f}, {hi1:.1f}]; p={p1:.3f}\"),\n",
    "    (\"  p-interaction\", f\"{p_int:.3f}\")\n",
    "]\n",
    "\n",
    "tbl3 = pd.concat([tbl3, pd.DataFrame(rows_children, columns=tbl3.columns)], ignore_index=True)\n",
    "tbl3.to_csv(\"table3_adjusted_effects.csv\", index=False)\n",
    "tbl3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b7387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ONE-CELL: 生成 “表3（已调整）”——含样本数的三列版 ======\n",
    "import numpy as np, pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# -------- 小工具 --------\n",
    "def fit_ols(formula, data, cov_type=\"HC3\"):\n",
    "    return smf.ols(formula, data=data).fit(cov_type=cov_type)\n",
    "\n",
    "def coef_ci(res, name):\n",
    "    \"\"\"返回 (b, lo, hi, p)；若找不到参数名直接抛错。\"\"\"\n",
    "    b = res.params[name]\n",
    "    lo, hi = res.conf_int().loc[name]\n",
    "    p = res.pvalues[name]\n",
    "    return float(b), float(lo), float(hi), float(p)\n",
    "\n",
    "def fmt_pp(b, lo, hi, p, dec=1):\n",
    "    f = f\"{{:.{dec}f}}\"\n",
    "    return f\"{f.format(b)} [{f.format(lo)}, {f.format(hi)}]; p={p:.3f}\"\n",
    "\n",
    "def wald_p_for_interactions(res, names):\n",
    "    R = np.zeros((len(names), len(res.params)))\n",
    "    for i, nm in enumerate(names):\n",
    "        R[i, list(res.params.index).index(nm)] = 1.0\n",
    "    return float(res.wald_test(R).pvalue)\n",
    "\n",
    "def pct_str(n, N):\n",
    "    return f\"{n} ({(n/N)*100:.1f}%)\"\n",
    "\n",
    "# -------- 数据准备（与你当前环境一致）--------\n",
    "DF = df_model.copy()\n",
    "OUTCOME = OUTCOME\n",
    "\n",
    "# 统一族裔标签 & 越界置缺\n",
    "DF[\"ethn_participant\"] = DF[\"ethn_participant\"].replace({\"All other ethnicities\":\"Other\"})\n",
    "eth_levels = [\"White\",\"Asian\",\"Other\"]\n",
    "DF.loc[(DF[OUTCOME] < 0) | (DF[OUTCOME] > 100), OUTCOME] = np.nan\n",
    "\n",
    "# Core-A 完整案例数据\n",
    "need_coreA = [OUTCOME,\"HFI_binary\",\"age_participant\",\"gender_participant\",\"ethn_participant\",\"child_numbers\"]\n",
    "datA = DF[need_coreA].dropna().copy()\n",
    "N_overall = len(datA)\n",
    "\n",
    "# -------- Core-A --------\n",
    "form_coreA = (\n",
    "    f\"{OUTCOME} ~ HFI_binary + age_participant\"\n",
    "    \" + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \" + child_numbers\"\n",
    ")\n",
    "resA = fit_ols(form_coreA, datA)\n",
    "b1, lo1, hi1, p1 = coef_ci(resA, \"HFI_binary\")\n",
    "\n",
    "# 年龄（每 +1 SD）的展示口径（不改变模型，仅换标度）\n",
    "sd_age = float(datA[\"age_participant\"].std(ddof=1))\n",
    "b_age, lo_age, hi_age, p_age = coef_ci(resA, \"age_participant\")\n",
    "b_ageSD, lo_ageSD, hi_ageSD = b_age*sd_age, lo_age*sd_age, hi_age*sd_age\n",
    "\n",
    "# -------- Core-B（HFI×ethnicity）--------\n",
    "form_coreB = (\n",
    "    f\"{OUTCOME} ~ HFI_binary*C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \" + age_participant + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + child_numbers\"\n",
    ")\n",
    "resB = fit_ols(form_coreB, datA)\n",
    "\n",
    "param_idx = resB.params.index.tolist()\n",
    "def inter_name(level):\n",
    "    for nm in param_idx:\n",
    "        if nm.startswith(\"HFI_binary:\") and f\"[T.{level}]\" in nm:\n",
    "            return nm\n",
    "    raise KeyError(f\"交互项缺失：{level}\")\n",
    "\n",
    "# 各族裔样本 n(%)\n",
    "eth_counts = datA[\"ethn_participant\"].value_counts().reindex(eth_levels).fillna(0).astype(int)\n",
    "sample_eth = {lvl: pct_str(int(eth_counts[lvl]), N_overall) for lvl in eth_levels}\n",
    "\n",
    "by_eth = []\n",
    "# White = 主效应\n",
    "by_eth.append((\"White\", sample_eth[\"White\"],) + coef_ci(resB, \"HFI_binary\"))\n",
    "# Asian / Other = 主效应 + 交互\n",
    "for lvl in [\"Asian\",\"Other\"]:\n",
    "    nm = inter_name(lvl)\n",
    "    L = np.zeros(len(resB.params))\n",
    "    L[param_idx.index(\"HFI_binary\")] = 1.0\n",
    "    L[param_idx.index(nm)] += 1.0\n",
    "    est = float(L @ resB.params)\n",
    "    se  = float(np.sqrt(L @ resB.cov_params() @ L))\n",
    "    z   = est/se\n",
    "    p   = 2*stats.norm.sf(abs(z))\n",
    "    lo, hi = est-1.96*se, est+1.96*se\n",
    "    by_eth.append((lvl, sample_eth[lvl], est, lo, hi, p))\n",
    "\n",
    "p_inter_eth = wald_p_for_interactions(resB, [inter_name(\"Asian\"), inter_name(\"Other\")])\n",
    "\n",
    "# -------- 敏感性：HFI 三分类 + 趋势 --------\n",
    "sens_rows = [] ; sample_low = sample_vlow = sample_trend = \"\"\n",
    "if \"HFI_category\" in DF.columns:\n",
    "    order = [\"Food secure\",\"Low food security\",\"Very low food security\"]\n",
    "    DF_cat = DF[need_coreA + [\"HFI_category\"]].dropna().copy()\n",
    "    DF_cat[\"HFI_category\"] = pd.Categorical(DF_cat[\"HFI_category\"], categories=order, ordered=True)\n",
    "    counts_cat = DF_cat[\"HFI_category\"].value_counts().reindex(order).fillna(0).astype(int)\n",
    "    sample_low   = f\"n={int(counts_cat['Low food security'])}\"\n",
    "    sample_vlow  = f\"n={int(counts_cat['Very low food security'])}\"\n",
    "    sample_trend = f\"N={len(DF_cat)}\"\n",
    "\n",
    "    # (a) 类别效应\n",
    "    form_cat = (\n",
    "        f\"{OUTCOME} ~ C(HFI_category, Treatment(reference='Food secure'))\"\n",
    "        \" + age_participant + C(gender_participant, Treatment(reference=1))\"\n",
    "        \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "        \" + child_numbers\"\n",
    "    )\n",
    "    res_cat = fit_ols(form_cat, DF_cat)\n",
    "    nm_low  = \"C(HFI_category, Treatment(reference='Food secure'))[T.Low food security]\"\n",
    "    nm_vlow = \"C(HFI_category, Treatment(reference='Food secure'))[T.Very low food security]\"\n",
    "    sens_rows.append((\"Low food security\", sample_low,) + coef_ci(res_cat, nm_low))\n",
    "    sens_rows.append((\"Very low food security\", sample_vlow,) + coef_ci(res_cat, nm_vlow))\n",
    "\n",
    "    # (b) 线性趋势\n",
    "    tr_map = {\"Food secure\":0, \"Low food security\":1, \"Very low food security\":2}\n",
    "    DF_cat[\"_HFI_trend\"] = DF_cat[\"HFI_category\"].map(tr_map).astype(float)\n",
    "    form_tr = (\n",
    "        f\"{OUTCOME} ~ _HFI_trend + age_participant\"\n",
    "        \" + C(gender_participant, Treatment(reference=1))\"\n",
    "        \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "        \" + child_numbers\"\n",
    "    )\n",
    "    res_tr = fit_ols(form_tr, DF_cat)\n",
    "    bT, loT, hiT, pT = coef_ci(res_tr, \"_HFI_trend\")\n",
    "\n",
    "# -------- SES 稳健性：same-N / fresh-N --------\n",
    "# -------- SES 稳健性：same-N / fresh-N（修复 categorical 被数值化的问题）--------\n",
    "ses_same_row = ses_fresh_row = None\n",
    "if {\"income\",\"employ\"}.issubset(df_model.columns):\n",
    "    ses_cols = need_coreA + [\"income\",\"employ\"]\n",
    "\n",
    "    # === same-N：与 +SES 使用完全相同的数据行 ===\n",
    "    dat_same = df_model[ses_cols].copy()\n",
    "\n",
    "    # 只数值化真正的数值列；分类变量保持原始标签\n",
    "    for c in [\"HFI_binary\",\"age_participant\",\"child_numbers\",\"income\"]:\n",
    "        dat_same[c] = pd.to_numeric(dat_same[c], errors=\"coerce\")\n",
    "\n",
    "    # 这些列必须同时非缺失，才能进入 same-N 两个模型\n",
    "    dat_same = dat_same.dropna(\n",
    "        subset=[\"HFI_binary\",\"age_participant\",\"gender_participant\",\n",
    "                \"ethn_participant\",\"child_numbers\",\"income\",\"employ\"]\n",
    "    ).copy()\n",
    "\n",
    "    # 仍然确保性别/族裔是 category（避免 patsy 误判）\n",
    "    dat_same[\"gender_participant\"] = dat_same[\"gender_participant\"].astype(\"category\")\n",
    "    dat_same[\"ethn_participant\"]   = dat_same[\"ethn_participant\"].astype(\"category\")\n",
    "    dat_same[\"employ\"]             = dat_same[\"employ\"].astype(\"category\")\n",
    "\n",
    "    res_core_same = fit_ols(form_coreA, dat_same)\n",
    "    form_plusSES  = form_coreA + \" + income + C(employ)\"\n",
    "    res_plus_same = fit_ols(form_plusSES, dat_same)\n",
    "\n",
    "    b_coreS, *_                    = coef_ci(res_core_same,  \"HFI_binary\")\n",
    "    b_SES_S, lo_SES_S, hi_SES_S, p_SES_S = coef_ci(res_plus_same, \"HFI_binary\")\n",
    "    d_abs  = b_SES_S - b_coreS\n",
    "    d_pct  = (d_abs / b_coreS * 100) if b_coreS != 0 else np.nan\n",
    "    ses_same_row = (\"SES robustness — same-N (Δ vs Core-A)\",\n",
    "                    f\"N={len(dat_same)}\",\n",
    "                    f\"β1(+SES)={fmt_pp(b_SES_S, lo_SES_S, hi_SES_S, p_SES_S, dec=2)}; Δ={d_abs:+.2f} (+{d_pct:.1f}%)\")\n",
    "\n",
    "    # === fresh-N：Core-A 用可用样本；+SES 用有 SES 的样本 ===\n",
    "    dat_coreF = df_model[need_coreA].copy()\n",
    "    for c in [\"HFI_binary\",\"age_participant\",\"child_numbers\"]:\n",
    "        dat_coreF[c] = pd.to_numeric(dat_coreF[c], errors=\"coerce\")\n",
    "    dat_coreF = dat_coreF.dropna(\n",
    "        subset=[\"HFI_binary\",\"age_participant\",\"gender_participant\",\n",
    "                \"ethn_participant\",\"child_numbers\"]\n",
    "    ).copy()\n",
    "    dat_coreF[\"gender_participant\"] = dat_coreF[\"gender_participant\"].astype(\"category\")\n",
    "    dat_coreF[\"ethn_participant\"]   = dat_coreF[\"ethn_participant\"].astype(\"category\")\n",
    "\n",
    "    dat_SESF = df_model[ses_cols].copy()\n",
    "    for c in [\"HFI_binary\",\"age_participant\",\"child_numbers\",\"income\"]:\n",
    "        dat_SESF[c] = pd.to_numeric(dat_SESF[c], errors=\"coerce\")\n",
    "    dat_SESF = dat_SESF.dropna(\n",
    "        subset=[\"HFI_binary\",\"age_participant\",\"gender_participant\",\n",
    "                \"ethn_participant\",\"child_numbers\",\"income\",\"employ\"]\n",
    "    ).copy()\n",
    "    dat_SESF[\"gender_participant\"] = dat_SESF[\"gender_participant\"].astype(\"category\")\n",
    "    dat_SESF[\"ethn_participant\"]   = dat_SESF[\"ethn_participant\"].astype(\"category\")\n",
    "    dat_SESF[\"employ\"]             = dat_SESF[\"employ\"].astype(\"category\")\n",
    "\n",
    "    res_SESF  = fit_ols(form_plusSES, dat_SESF)\n",
    "    res_coreF = fit_ols(form_coreA,  dat_coreF)\n",
    "\n",
    "    b_SESF, lo_SESF, hi_SESF, p_SESF = coef_ci(res_SESF, \"HFI_binary\")\n",
    "    b_coreF, *_ = coef_ci(res_coreF, \"HFI_binary\")\n",
    "    d_abs_F = b_SESF - b_coreF\n",
    "    d_pct_F = (d_abs_F / b_coreF * 100) if b_coreF != 0 else np.nan\n",
    "\n",
    "    ses_fresh_row = (\"SES robustness — fresh-N (Δ vs Core-A)\",\n",
    "                     f\"Core-A N={len(dat_coreF)} / +SES N={len(dat_SESF)}\",\n",
    "                     f\"β1(+SES)={fmt_pp(b_SESF, lo_SESF, hi_SESF, p_SESF, dec=2)}; Δ={d_abs_F:+.2f} (+{d_pct_F:.1f}%)\")\n",
    "\n",
    "    # 与 Core-A（全部可用样本）比较\n",
    "    res_coreF = fit_ols(form_coreA, dat_coreF)\n",
    "    b_coreF, *_ = coef_ci(res_coreF, \"HFI_binary\")\n",
    "    d_abs_F = b_SESF - b_coreF\n",
    "    d_pct_F = (d_abs_F / b_coreF * 100) if b_coreF != 0 else np.nan\n",
    "    ses_fresh_row = (\"SES robustness — fresh-N (Δ vs Core-A)\",\n",
    "                     f\"Core-A N={len(dat_coreF)} / +SES N={len(dat_SESF)}\",\n",
    "                     f\"β1(+SES)={fmt_pp(b_SESF, lo_SESF, hi_SESF, p_SESF, dec=2)}; Δ={d_abs_F:+.2f} (+{d_pct_F:.1f}%)\")\n",
    "\n",
    "# -------- 儿童交互：HFI × I(≥3 子女) --------\n",
    "child_rows = [] ; p_inter_child = np.nan\n",
    "datC = datA.copy()\n",
    "datC[\"_Ige3\"] = (datC[\"child_numbers\"] >= 3).astype(int)\n",
    "form_child = (\n",
    "    f\"{OUTCOME} ~ HFI_binary*_Ige3 + age_participant\"\n",
    "    \" + C(gender_participant, Treatment(reference=1))\"\n",
    "    \" + C(ethn_participant, Treatment(reference='White'))\"\n",
    "    \" + child_numbers\"\n",
    ")\n",
    "resC = fit_ols(form_child, datC)\n",
    "# <3: HFI 主效应\n",
    "b_lt, lo_lt, hi_lt, p_lt = coef_ci(resC, \"HFI_binary\")\n",
    "n_lt = int((datC[\"_Ige3\"]==0).sum())\n",
    "# ≥3: HFI + 交互\n",
    "nm_c = \"HFI_binary:_Ige3\"\n",
    "L = np.zeros(len(resC.params))\n",
    "L[list(resC.params.index).index(\"HFI_binary\")] = 1.0\n",
    "L[list(resC.params.index).index(nm_c)] += 1.0\n",
    "est = float(L @ resC.params)\n",
    "se  = float(np.sqrt(L @ resC.cov_params() @ L))\n",
    "z   = est/se\n",
    "p_ge = 2*stats.norm.sf(abs(z))\n",
    "lo_ge, hi_ge = est-1.96*se, est+1.96*se\n",
    "n_ge = int((datC[\"_Ige3\"]==1).sum())\n",
    "p_inter_child = wald_p_for_interactions(resC, [nm_c])\n",
    "\n",
    "# -------- 组装三列表 --------\n",
    "rows = []\n",
    "\n",
    "# Core-A（主效应 + 年龄展示）\n",
    "rows.append((\"Core-A: insecure − secure (pp)\", f\"N={N_overall}\", fmt_pp(b1, lo1, hi1, p1)))\n",
    "rows.append((\"Core-A: age (+1 SD)\",              f\"N={N_overall}\", fmt_pp(b_ageSD, lo_ageSD, hi_ageSD, p_age)))\n",
    "\n",
    "# Core-B（族裔）\n",
    "rows.append((f\"Core-B (HFI × ethnicity)\", f\"N={N_overall}\", \"\"))\n",
    "for lbl, samp, b, lo, hi, p in by_eth:\n",
    "    rows.append((f\"{lbl}: insecure − secure\", samp, fmt_pp(b, lo, hi, p)))\n",
    "rows.append((\"*p-interaction (Wald)*\", \"\", f\"{p_inter_eth:.3f}\"))\n",
    "\n",
    "# 敏感性\n",
    "if sens_rows:\n",
    "    rows.append((f\"Sensitivity: HFI three-level (vs secure)\", f\"N={len(DF_cat)}\", \"\"))\n",
    "    rows.extend([(lab, samp, fmt_pp(b, lo, hi, p)) for (lab, samp, b, lo, hi, p) in sens_rows])\n",
    "    rows.append((\"Linear trend (0→2, pp/level)\", sample_trend, fmt_pp(bT, loT, hiT, pT)))\n",
    "\n",
    "# SES 稳健性\n",
    "if ses_same_row:  rows.append(ses_same_row)\n",
    "if ses_fresh_row: rows.append(ses_fresh_row)\n",
    "\n",
    "# 儿童交互\n",
    "rows.append((f\"Children interaction (HFI × I≥3)\", f\"N={N_overall}\", \"\"))\n",
    "rows.append((\"<3 children: insecure − secure\",  f\"n={n_lt}\", fmt_pp(b_lt, lo_lt, hi_lt, p_lt)))\n",
    "rows.append((\"≥3 children: insecure − secure\", f\"n={n_ge}\", fmt_pp(est, lo_ge, hi_ge, p_ge)))\n",
    "rows.append((\"*p-interaction*\", \"\", f\"{p_inter_child:.3f}\"))\n",
    "\n",
    "tbl3 = pd.DataFrame(rows, columns=[\"Contrast / Estimand\", \"Sample (n, %)\", \"Adjusted effect (pp)\"])\n",
    "tbl3.to_csv(\"table3_adjusted_effects_withN.csv\", index=False)\n",
    "print(\"Saved: table3_adjusted_effects_withN.csv\")\n",
    "tbl3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6baa27",
   "metadata": {},
   "source": [
    "# 模型诊断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —— 简洁版影响图：只标 Top-K（按 Cook’s D） ——\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "\n",
    "inf = OLSInfluence(res_coreA)\n",
    "lev  = inf.hat_matrix_diag\n",
    "stud = inf.resid_studentized_external\n",
    "cook = inf.cooks_distance[0]\n",
    "\n",
    "K = 8  # 只标注 Cook's D 最大的 K 个点\n",
    "topk_idx = np.argsort(cook)[-K:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.2, 4))\n",
    "ax.scatter(lev, stud, s=18, alpha=0.35)  # 小点 + 半透明\n",
    "ax.axhline(0, color=\"k\", lw=0.6, alpha=0.6)\n",
    "\n",
    "# 只给 top-K 加标注\n",
    "for i in topk_idx:\n",
    "    ax.annotate(str(i), (lev[i], stud[i]), xytext=(3, 3),\n",
    "                textcoords=\"offset points\", fontsize=9)\n",
    "\n",
    "ax.set_xlabel(\"Leverage\")\n",
    "ax.set_ylabel(\"Studentized residuals\")\n",
    "ax.set_title(\"Influence (only top-K labelled)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —— Cook’s D / 杠杆阈值 + leave-one-out（自动识别 HFI 系数名） ——\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "\n",
    "# 1) 自动找到“HFI 主效应”的系数名（避免手写错）\n",
    "def find_hfi_coef_name(res):\n",
    "    names = list(getattr(res.model, \"exog_names\", []))\n",
    "    # 优先精确匹配（你若用的是数值 0/1 变量）\n",
    "    for cand in (\"HFI_binary\", \"HFI\", \"hfi_binary\", \"hfi\"):\n",
    "        if cand in names:\n",
    "            return cand\n",
    "    # 其次：categorical 的常见写法（排除交互项，名字里不含 ':'）\n",
    "    for nm in names:\n",
    "        if (\"hfi\" in nm.lower()) and (\":\" not in nm):\n",
    "            return nm\n",
    "    raise ValueError(f\"找不到 HFI 主效应系数名；exog_names={names}\")\n",
    "\n",
    "# 2) 既可按“名字”也可按“位置”取系数与CI\n",
    "def get_param_and_ci(res, name):\n",
    "    try:  # 先按标签\n",
    "        b = float(res.params[name])\n",
    "        ci = res.conf_int()\n",
    "        try:\n",
    "            lo, hi = float(ci.loc[name, 0]), float(ci.loc[name, 1])\n",
    "        except Exception:  # 若 conf_int 是 ndarray\n",
    "            idx = res.model.exog_names.index(name)\n",
    "            lo, hi = float(ci[idx, 0]), float(ci[idx, 1])\n",
    "        return b, lo, hi\n",
    "    except Exception:  # 回退按位置\n",
    "        names = list(getattr(res.model, \"exog_names\", []))\n",
    "        idx = names.index(name)\n",
    "        par = np.asarray(res.params); ci = np.asarray(res.conf_int())\n",
    "        return float(par[idx]), float(ci[idx, 0]), float(ci[idx, 1])\n",
    "\n",
    "# 3) 影响度 & 阈值\n",
    "infl   = OLSInfluence(res_coreA)\n",
    "cook_d = infl.cooks_distance[0]\n",
    "hat    = infl.hat_matrix_diag\n",
    "\n",
    "n  = int(res_coreA.nobs)\n",
    "p  = int(res_coreA.df_model) + 1\n",
    "thr_cook = 4 / n\n",
    "thr_hat  = 2 * p / n\n",
    "\n",
    "flag_pos = np.where((cook_d > thr_cook) | (hat > thr_hat))[0].astype(int)\n",
    "print(f\"flagged positions (Cook>{thr_cook:.3f} or hat>{thr_hat:.3f}):\", flag_pos.tolist())\n",
    "\n",
    "# 4) 基准系数与数据\n",
    "coef_name = find_hfi_coef_name(res_coreA)\n",
    "print(\"→ 识别到的 HFI 系数名:\", coef_name)\n",
    "\n",
    "b_full, lo_full, hi_full = get_param_and_ci(res_coreA, coef_name)\n",
    "\n",
    "df_full = res_coreA.model.data.frame\n",
    "formula = res_coreA.model.formula\n",
    "\n",
    "# 5) leave-one-out：按“行位置”删除（与 index 标签无关）\n",
    "def refit_drop_by_pos(pos):\n",
    "    keep = np.ones(len(df_full), dtype=bool)\n",
    "    keep[int(pos)] = False\n",
    "    df_keep = df_full.iloc[keep]\n",
    "    res = smf.ols(formula=formula, data=df_keep).fit(cov_type=\"HC3\")\n",
    "    return get_param_and_ci(res, coef_name)\n",
    "\n",
    "for pos in flag_pos:\n",
    "    b, lo, hi = refit_drop_by_pos(pos)\n",
    "    denom = (abs(b_full) if b_full != 0 else 1.0)  # 避免除0\n",
    "    d_pct = 100 * (b - b_full) / denom\n",
    "    print(f\"drop pos {int(pos):>3}: beta1={b:.3f} (CI {lo:.3f}, {hi:.3f}); Δ vs full = {d_pct:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360130b7",
   "metadata": {},
   "source": [
    "## B-1 误差线图（主推）：每删 1 个点后 β₁(HFI) 的点估计与 95%CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1cf2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总每个被标记点的 LOO 结果\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rows = []\n",
    "for pos in flag_pos:\n",
    "    b, lo, hi = refit_drop_by_pos(pos)\n",
    "    d_pct = 100*(b - b_full)/(abs(b_full) if b_full != 0 else 1.0)\n",
    "    rows.append({\"pos\": int(pos), \"beta\": b, \"lo\": lo, \"hi\": hi, \"delta_pct\": d_pct})\n",
    "\n",
    "df_loo = pd.DataFrame(rows).sort_values(\"beta\").reset_index(drop=True)\n",
    "\n",
    "# 误差线图：横轴是 β1（pp），纵轴是被删观测的位置\n",
    "fig, ax = plt.subplots(figsize=(6, 4.2))\n",
    "y = np.arange(len(df_loo))\n",
    "ax.hlines(y, df_loo[\"lo\"], df_loo[\"hi\"])      # 95%CI\n",
    "ax.plot(df_loo[\"beta\"], y, \"o\")               # 点估计\n",
    "ax.axvline(b_full, linestyle=\"--\")            # 全样本 β1 参考线\n",
    "ax.set_xlabel(\"β₁(HFI) after dropping one obs (pp)\")\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(df_loo[\"pos\"])\n",
    "ax.set_ylabel(\"flagged row (position)\")\n",
    "ax.set_title(\"Leave-one-out on flagged points\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1128da8",
   "metadata": {},
   "source": [
    "## B-2 影响力度条形图：β₁ 相对变化（%） 的大小比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只看相对变化（%），便于一眼看出“谁影响最大”\n",
    "df_delta = df_loo.sort_values(\"delta_pct\").reset_index(drop=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3.8))\n",
    "ax.barh(np.arange(len(df_delta)), df_delta[\"delta_pct\"])\n",
    "ax.axvline(0, linestyle=\"--\")\n",
    "ax.set_yticks(range(len(df_delta)))\n",
    "ax.set_yticklabels(df_delta[\"pos\"])\n",
    "ax.set_xlabel(\"Relative change in β₁ ( % )\")\n",
    "ax.set_ylabel(\"flagged row (position)\")\n",
    "ax.set_title(\"Impact of dropping each flagged point\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "K = 12\n",
    "df_top = df_loo.reindex(df_loo[\"delta_pct\"].abs().nlargest(K).index).sort_values(\"delta_pct\")\n",
    "# 然后把上面条形图里的 df_delta 换成 df_top 即可\n",
    "id_col = \"UserID_clean\"  # 改成你的 ID 列名\n",
    "df_loo[\"id\"] = df_full.iloc[df_loo[\"pos\"]][id_col].to_numpy()\n",
    "# 然后把上面两个图里的 yticklabels 从 df_loo[\"pos\"] 改成 df_loo[\"id\"] 即可\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e5a252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
