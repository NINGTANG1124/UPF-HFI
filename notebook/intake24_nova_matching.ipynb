{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJanuw+gQsn3hFimSTGc5d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NINGTANG1124/UPF-HFI/blob/main/notebook/intake24_nova_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect googledrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9izKK0g9LUQ",
        "outputId": "06f043b2-8055-4105-ccf4-0eb58339e768"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: 读取 intake 数据（含 Descriptionen 和 FoodGroupen）\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/UPF-HFI/Bradford_original data/1. Dietmasterfile_foodlevel_clean.xls\"\n",
        "intake_df = pd.read_excel(file_path)\n"
      ],
      "metadata": {
        "id": "c7bYEbA49bs9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step2 清洗 Description 和 Foodgroup\n",
        "intake_df[\"Foodgroupen_clean\"] = (\n",
        "    intake_df[\"Foodgroupen\"].astype(str).str.lower().str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
        ")\n",
        "\n",
        "intake_df[\"Descriptionen_clean\"] = (\n",
        "    intake_df[\"Descriptionen\"].astype(str).str.lower().str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
        ")\n"
      ],
      "metadata": {
        "id": "QbNxG8Re9glK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# description\n",
        "def match_nova_by_description(text):\n",
        "    text = str(text).lower().strip()\n",
        "\n",
        "    # === NOVA 1: 饮用水 ===\n",
        "    if any(w in text for w in [\"tap water\", \"still water\", \"filtered water\", \"plain water\"]):\n",
        "        if \"flavour\" not in text:\n",
        "            return 1, \"plain water (description)\"\n",
        "\n",
        "    # === NOVA 1: 微观乳制品（plain）===\n",
        "    if any(w in text for w in [\"semi skimmed milk\", \"skimmed milk\", \"whole milk\"]) and \"flavour\" not in text:\n",
        "        return 1, \"plain milk\"\n",
        "    if any(w in text for w in [\"natural yoghurt\", \"fromage frais\"]) and \"flavour\" not in text:\n",
        "        return 1, \"plain yoghurt\"\n",
        "\n",
        "    # === NOVA 1: 精确 raw/unprocessed ===\n",
        "    import re\n",
        "\n",
        "    if re.search(r'\\braw\\b', text):\n",
        "        return 1, \"raw (word-bound)\"\n",
        "\n",
        "    # === NOVA 3: 自制、轻加工 ===\n",
        "    if any(w in text for w in [\"homemade\", \"home made\"]):\n",
        "        return 3, \"homemade\"\n",
        "    if any(w in text for w in [\"boiled\", \"mashed potato\", \"baked potato\", \"jacket potato\"]):\n",
        "        return 3, \"boiled/baked/jacket\"\n",
        "\n",
        "    # === NOVA 4: 工业加工麦片（如sachet类）===\n",
        "    if \"porridge sachet\" in text or (\"porridge\" in text and \"oat so simple\" in text):\n",
        "        return 4, \"sachet porridge (description)\"\n",
        "\n",
        "    # === NOVA 4: takeaway 快餐类 ===\n",
        "    if \"takeaway\" in text or \"take away\" in text:\n",
        "        return 4, \"takeaway food\"\n",
        "\n",
        "    # === NOVA 4: 零食/甜食/加工脂肪 ===\n",
        "    if any(w in text for w in [\"jam\", \"conserve\", \"marmalade\", \"chocolate spread\", \"ice cream topping\", \"marzipan\"]):\n",
        "        return 4, \"spread/syrup\"\n",
        "    if any(w in text for w in [\"cracker\", \"savoury biscuit\", \"cheddar biscuit\", \"cream cracker\"]):\n",
        "        return 4, \"processed snack\"\n",
        "    if any(w in text for w in [\"sweets\", \"gums\", \"jellies\", \"boiled sweets\", \"mints\", \"liquorice\", \"popcorn\"]):\n",
        "        return 4, \"sweet snack\"\n",
        "    if any(w in text for w in [\"ice cream\", \"dessert\", \"milkshake\"]):\n",
        "        return 4, \"processed dessert\"\n",
        "    if any(w in text for w in [\"margarine\", \"clover spread\", \"flora\"]):\n",
        "        return 4, \"processed fat\"\n",
        "    if \"flavoured milk\" in text or \"chocolate milk\" in text:\n",
        "        return 4, \"flavoured milk\"\n",
        "    if \"ketchup\" in text and \"home made\" not in text:\n",
        "        return 4, \"processed ketchup\"\n",
        "    if \"instant\" in text and \"porridge\" not in text:\n",
        "        return 4, \"instant food\"\n",
        "    # === NOVA 4: takeaway 快餐类 ===\n",
        "    if \"takeaway\" in text or \"take away\" in text:\n",
        "        return 4, \"takeaway food\"\n",
        "\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "qYXdm7e1rO18"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# group\n",
        "def match_nova_by_group(group, description):\n",
        "    group = str(group).lower().strip()\n",
        "    description = str(description).lower().strip()\n",
        "\n",
        "    # === NOVA 1: group 精确匹配 water 类 ===\n",
        "    if group.strip() in [\"water\", \"tap water\", \"filtered water\"]:\n",
        "        return 1, \"water (group)\"\n",
        "\n",
        "    # === NOVA 1: 未加工果蔬、牛奶、酸奶 ===\n",
        "    if \"fresh fruit\" in group:\n",
        "        return 1, \"fruit (group)\"\n",
        "    if \"dried fruit\" in group:\n",
        "        return 1, \"dried fruit (group)\"\n",
        "    if \"vegetables\" in group and \"fried\" not in group:\n",
        "        return 1, \"vegetables (group)\"\n",
        "    if any(word in group for word in [\"semi skimmed milk\", \"skimmed milk\", \"whole milk\"]):\n",
        "        if \"flavour\" not in description and \"fruit\" not in description:\n",
        "            return 1, \"milk (group)\"\n",
        "    if any(word in group for word in [\"natural yoghurt\", \"fromage frais\"]):\n",
        "        if \"flavour\" not in description and \"fruit\" not in description:\n",
        "            return 1, \"yoghurt/plain dairy (group)\"\n",
        "\n",
        "    # === NOVA 3: 最小加工脂肪 ===\n",
        "    if any(w in group for w in [\"olive oil\", \"rapeseed oil\", \"sunflower oil\", \"vegetable oil\", \"butter\"]):\n",
        "        return 3, \"culinary fat/oil (group)\"\n",
        "\n",
        "    # === NOVA 4: 糖浆、早餐谷物、加工脂肪 ===\n",
        "    if any(w in group for w in [\"margarine\", \"fat spread\", \"flora\", \"dairy fat spreads\", \"hard marg\"]):\n",
        "        return 4, \"processed fat (group)\"\n",
        "    if any(w in group for w in [\"jam\", \"conserve\", \"marmalade\"]):\n",
        "        return 4, \"preserves (group)\"\n",
        "    if \"other breakfast cereals\" in group or \"muesli\" in group or \"bran flakes\" in group:\n",
        "        return 4, \"processed cereal (group)\"\n",
        "\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "zRt397tRpuES"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(intake_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qbhgb3xD9Aq",
        "outputId": "80864d04-25c8-4c69-df78-bae20192f057"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['SurveyID', 'UserID', 'Source', 'Starttime', 'Submissiontime',\n",
            "       'Timetocomplete', 'Cookingoilused', 'Diet', 'Foodamount',\n",
            "       'Reasonforunusualfoodamount',\n",
            "       ...\n",
            "       'Modification_Identification', 'discontinued', 'NDNS_Checks',\n",
            "       'UserID_specific', 'Day', 'weekday', 'ratio', 'UserID_clean',\n",
            "       'Foodgroupen_clean', 'Descriptionen_clean'],\n",
            "      dtype='object', length=168)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CXyb9JwmQuYs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 3: 定义主函数 match_nova()\n",
        "def match_nova(row):\n",
        "    description = row[\"Descriptionen_clean\"]\n",
        "    group = row[\"Foodgroupen_clean\"]\n",
        "\n",
        "    # Step 1: try description\n",
        "    nova, reason = match_nova_by_description(description)\n",
        "    if nova is not None:\n",
        "        return pd.Series([nova, \"description: \" + reason])\n",
        "\n",
        "    # Step 2: fallback to group\n",
        "    nova, reason = match_nova_by_group(group, description)\n",
        "    if nova is not None:\n",
        "        return pd.Series([nova, \"group: \" + reason])\n",
        "\n",
        "    return pd.Series([None, None])\n"
      ],
      "metadata": {
        "id": "LgPplkugtTTV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intake_df[[\"NOVA_step1\", \"match_reason\"]] = intake_df.apply(match_nova, axis=1)"
      ],
      "metadata": {
        "id": "-FKu22i2tWJ9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_save = [\n",
        "    \"Descriptionen\",\n",
        "    \"Descriptionen_clean\",\n",
        "    \"Foodgroupen\",\n",
        "    \"Foodgroupen_clean\",\n",
        "    \"NOVA_step1\",      # 改这里\n",
        "    \"match_reason\"\n",
        "]\n",
        "intake_df[cols_to_save].to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step1.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "pkShExdwiNwg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Step 2: TF-IDF 高阈值匹配（>0.99）【数据源：VKesaite】 Intake 描述 vs FoodName 字段 特点：英国 NDNS 数据，语义贴合度高 匹配后字段： Matched_NOVA Source = 'tfidf_vk_099' Similarity_score\n",
        "\n",
        "🔹 Step 3: TF-IDF 中阈值匹配（>0.85）【数据源：Giulia FNDDS】 Intake 描述 vs FoodName/Description 字段（视结构而定） 特点：匹配面广但风格偏美式 可作为第二权重匹配源补充空值 匹配后： Source = 'tfidf_giulia_085'\n",
        "\n",
        "🔹 Step 4: TF-IDF 或 SBERT 语义匹配（>0.85）【数据源：OFF】 两种方式都可用： TF-IDF 匹配 product_name 字段 SBERT 匹配描述（推荐 MiniLM ） 用于最后补充空值，提高 recall（召回率） 匹配后： Source = 'tfidf_off' 或 'sbert_off'\n",
        "\n",
        "🔹 Step 5: 整合 + 人工补全 + Final 输出"
      ],
      "metadata": {
        "id": "2TluyxWVNvxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nova文件数据清洗\n",
        "# ndns\n",
        "ndns_df = pd.read_csv(\"/content/drive/MyDrive/UPF-HFI/nova/NDNS_NOVA_DATABASE.new2023.csv\", encoding=\"ISO-8859-1\")\n",
        "ndns_df.columns = ndns_df.columns.str.strip()\n",
        "ndns_df = ndns_df[[\"FoodName\", \"NOVA\"]].dropna()\n",
        "ndns_df[\"FoodName_clean\"] = ndns_df[\"FoodName\"].str.lower().str.replace(r\"[^\\w\\s]\", \" \", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "ndns_df = ndns_df.drop_duplicates(subset=[\"FoodName_clean\"])\n"
      ],
      "metadata": {
        "id": "OMkARVTN1cMN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 美国的\n",
        "giulia_df = pd.read_excel(\"/content/drive/MyDrive/UPF-HFI/nova/Training Data Original Given by NOVA Researchers - Corrections by Giulia Babak FNDDS 2009-10.xls\")\n",
        "giulia_df.columns = giulia_df.columns.str.strip()\n",
        "\n",
        "giulia_df = giulia_df[[\"Main_food_description\", \"SR_nova_group\"]].dropna()\n",
        "giulia_df = giulia_df.rename(columns={\"Main_food_description\": \"FoodName\", \"SR_nova_group\": \"NOVA\"})\n",
        "\n",
        "giulia_df[\"FoodName_clean\"] = giulia_df[\"FoodName\"].str.lower().str.replace(r\"[^\\w\\s]\", \" \", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "giulia_df = giulia_df.drop_duplicates(subset=[\"FoodName_clean\"])\n"
      ],
      "metadata": {
        "id": "AgvkK3io1vib"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# off的\n",
        "off_clean = []\n",
        "with open(\"/content/drive/MyDrive/UPF-HFI/nova/openfoodfacts-popular-24.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            entry = json.loads(line)\n",
        "            if not isinstance(entry, dict):\n",
        "                continue  # 跳过非对象\n",
        "            name = entry.get(\"product_name\") or entry.get(\"abbreviated_product_name\")\n",
        "            nova = entry.get(\"nova_group\")\n",
        "            if name and nova:\n",
        "                name_clean = re.sub(r\"[^\\w\\s]\", \" \", name.lower())\n",
        "                name_clean = re.sub(r\"\\s+\", \" \", name_clean).strip()\n",
        "                off_clean.append({\"FoodName_clean\": name_clean, \"NOVA\": nova})\n",
        "        except json.JSONDecodeError:\n",
        "            continue  # 忽略错误行\n",
        "\n",
        "off_df = pd.DataFrame(off_clean).drop_duplicates(subset=[\"FoodName_clean\"])\n"
      ],
      "metadata": {
        "id": "rjtOfuR12Idz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndns_df.to_csv(\"NDNS_clean.csv\", index=False)\n",
        "giulia_df.to_csv(\"Giulia_clean.csv\", index=False)\n",
        "off_df.to_csv(\"OFF_clean.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "4xz6rJST2_lG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 2：TF-IDF 匹配未完成部分（基于 NOVA 对照池）\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "OTXojW7IbVIm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 1. 构建 TF-IDF 匹配参考库（NOVA pool）\n",
        "nova_pool = pd.concat([ndns_df, giulia_df, off_df], ignore_index=True)\n",
        "nova_pool = nova_pool.drop_duplicates(subset=[\"FoodName_clean\"])  # 注意：我们用 FoodName_clean 作为 reference\n",
        "\n",
        "# ✅ 2. 加载 intake 数据（包含 Step1 的结果）\n",
        "intake_df = pd.read_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step1.csv\")\n",
        "\n",
        "# ✅ 3. 选出 Step1 未匹配的项（缺失 NOVA_step1 的）\n",
        "mask_missing = intake_df[\"NOVA_step1\"].isna()\n",
        "query_texts = intake_df.loc[mask_missing, \"Descriptionen_clean\"].dropna()\n",
        "query_texts_index = query_texts.index\n",
        "\n",
        "# ✅ 4. 构建 TF-IDF 向量器并转换为向量\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_ref = vectorizer.fit_transform(nova_pool[\"FoodName_clean\"])  # 参考库：FoodName_clean\n",
        "tfidf_query = vectorizer.transform(query_texts)                    # 查询项：Descriptionen_clean\n",
        "\n",
        "# ✅ 5. 计算相似度得分和索引\n",
        "similarity_matrix = cosine_similarity(tfidf_query, tfidf_ref)\n",
        "best_match_idx = similarity_matrix.argmax(axis=1)\n",
        "best_match_score = similarity_matrix.max(axis=1)\n",
        "\n",
        "# ✅ 6. 从匹配位置提取 nova 分数与匹配名称\n",
        "matched_nova = nova_pool.iloc[best_match_idx][\"NOVA\"].values\n",
        "matched_name = nova_pool.iloc[best_match_idx][\"FoodName_clean\"].values\n",
        "\n",
        "# ✅ 7. 回写进 intake 数据\n",
        "intake_df.loc[query_texts_index, \"NOVA_step2\"] = matched_nova\n",
        "intake_df.loc[query_texts_index, \"TFIDF_score\"] = best_match_score\n",
        "intake_df.loc[query_texts_index, \"TFIDF_match_name\"] = matched_name\n",
        "\n",
        "# ✅ 8. 可选：过滤低于阈值的匹配结果（设置为 None）\n",
        "threshold = 0.85\n",
        "intake_df.loc[intake_df[\"TFIDF_score\"] < threshold, [\"NOVA_step2\", \"TFIDF_match_name\"]] = [None, None]\n",
        "\n",
        "# ✅ 9. 保存最终结果\n",
        "intake_df.to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step2.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "X-NxAZia6Jl1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧩 Step 3：合并 Step1 与 Step2 匹配结果，形成最终 NOVA 列\n",
        "\n",
        "def combine_nova(row):\n",
        "    if pd.notna(row[\"NOVA_step1\"]):\n",
        "        return row[\"NOVA_step1\"]\n",
        "    elif pd.notna(row[\"NOVA_step2\"]):\n",
        "        return row[\"NOVA_step2\"]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "intake_df[\"NOVA_final\"] = intake_df.apply(combine_nova, axis=1)\n",
        "\n",
        "# 同时保留来源（说明匹配来源是 Step1 / Step2 / None）\n",
        "def get_reason(row):\n",
        "    if pd.notna(row[\"NOVA_step1\"]):\n",
        "        return \"Keyword\"\n",
        "    elif pd.notna(row[\"NOVA_step2\"]):\n",
        "        return \"TF-IDF\"\n",
        "    else:\n",
        "        return \"Unmatched\"\n",
        "\n",
        "intake_df[\"Match_source\"] = intake_df.apply(get_reason, axis=1)\n",
        "\n",
        "# ✅ 保存最终结果\n",
        "intake_df.to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step3.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "NX-zuEsc6jRB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Step 4：使用 SBERT 对剩余 NOVA_final 为空的食物进行语义匹配补全"
      ],
      "metadata": {
        "id": "bcR92-jE7KdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "PqbMD6e87JlI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 加载预训练模型（推荐 all-MiniLM-L6-v2）：\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-po5i9d7vel",
        "outputId": "289408f2-c3a1-4907-e493-ffc1738df4f8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🛠 Step 4：对 NOVA_final 为空的食物进行 SBERT 匹配\n",
        "# ✅ 1. 准备候选库（对 nova_pool 编码）\n",
        "# 确保你之前准备好的 nova_pool 有 FoodName_clean 列\n",
        "ref_texts = nova_pool[\"FoodName_clean\"].tolist()\n",
        "ref_embeddings = model.encode(ref_texts, convert_to_tensor=True)\n"
      ],
      "metadata": {
        "id": "LCxkp7aG71gB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 2. 选取待匹配食物（NOVA_final 为空）\n",
        "unmatched_df = intake_df[intake_df[\"NOVA_final\"].isna()].copy()\n",
        "query_texts = unmatched_df[\"Descriptionen_clean\"].dropna().tolist()\n",
        "query_indices = unmatched_df[\"Descriptionen_clean\"].dropna().index\n",
        "\n",
        "query_embeddings = model.encode(query_texts, convert_to_tensor=True)\n"
      ],
      "metadata": {
        "id": "lrebQJ1R75Y8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 3. 计算语义相似度并提取匹配结果\n",
        "cosine_scores = util.pytorch_cos_sim(query_embeddings, ref_embeddings)\n",
        "top_scores, top_indices = torch.max(cosine_scores, dim=1)\n",
        "\n",
        "# 写入结果\n",
        "intake_df.loc[query_indices, \"SBERT_score\"] = top_scores.cpu().numpy()\n",
        "intake_df.loc[query_indices, \"SBERT_match_name\"] = nova_pool.iloc[top_indices.cpu().numpy()][\"FoodName_clean\"].values\n",
        "intake_df.loc[query_indices, \"NOVA_step4\"] = nova_pool.iloc[top_indices.cpu().numpy()][\"NOVA\"].values\n"
      ],
      "metadata": {
        "id": "C6GsX7UD79Cy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 4. 更新最终列：NOVA_final + Match_source\n",
        "# 如果 Step3 没找到但 Step4 找到了，使用 SBERT 匹配结果\n",
        "intake_df[\"NOVA_final\"] = intake_df[\"NOVA_final\"].combine_first(intake_df[\"NOVA_step4\"])\n",
        "\n",
        "# 同样更新匹配来源\n",
        "intake_df[\"Match_source\"] = intake_df.apply(lambda row: (\n",
        "    \"SBERT\" if pd.notna(row[\"NOVA_step4\"]) and pd.isna(row[\"NOVA_step1\"]) and pd.isna(row[\"NOVA_step2\"])\n",
        "    else row[\"Match_source\"]\n",
        "), axis=1)\n"
      ],
      "metadata": {
        "id": "NT3oNuGY8EUt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(intake_df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilA1fLqHNZ6s",
        "outputId": "344874c1-d52a-4f3f-d1ef-96efbefcfea0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Descriptionen', 'Descriptionen_clean', 'Foodgroupen', 'Foodgroupen_clean', 'NOVA_step1', 'match_reason', 'NOVA_step2', 'TFIDF_score', 'TFIDF_match_name', 'NOVA_final', 'Match_source', 'SBERT_score', 'SBERT_match_name', 'NOVA_step4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col_order = [\n",
        "    \"Descriptionen\", \"Descriptionen_clean\", \"Foodgroupen\", \"Foodgroupen_clean\",\n",
        "\n",
        "    # === Step 1: Keyword 匹配 ===\n",
        "    \"NOVA_step1\", \"match_reason\",\n",
        "\n",
        "    # === Step 2: TF-IDF 匹配 ===\n",
        "    \"NOVA_step2\", \"TFIDF_match_name\", \"TFIDF_score\",\n",
        "\n",
        "    # === Step 3: SBERT 匹配 ===\n",
        "    \"NOVA_step4\", \"SBERT_match_name\", \"SBERT_score\",\n",
        "\n",
        "    # === 最终结果 ===\n",
        "    \"NOVA_final\", \"Match_source\"\n",
        "]\n",
        "\n",
        "\n",
        "# 按列顺序导出\n",
        "intake_df = intake_df[col_order]\n",
        "intake_df.to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step4_final.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "FSTQSiDGzFlx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unmatched_final = intake_df[intake_df[\"NOVA_final\"].isin([None, \"\", \"NC\"])]\n",
        "print(f\"未匹配上的食物数量：{len(unmatched_final)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WglYB0ua0dbF",
        "outputId": "8766ad68-c708-4a78-8754-e6aa84492c50"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "未匹配上的食物数量：414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 提取未匹配的行（None, \"\", \"NC\"）\n",
        "unmatched_final = intake_df[intake_df[\"NOVA_final\"].isin([None, \"\", \"NC\"])]\n",
        "\n",
        "# 用 Descriptionen_clean 统计频率\n",
        "nc_counts = unmatched_final[\"Descriptionen_clean\"].value_counts().reset_index()\n",
        "nc_counts.columns = [\"Descriptionen_clean\", \"count\"]\n",
        "\n",
        "\n",
        "# 展示前 30 个高频未匹配条目\n",
        "print(\"高频未匹配食物（前30）：\")\n",
        "print(nc_counts.head(30))\n",
        "\n",
        "# 可选：导出成 CSV 文件\n",
        "nc_counts.to_csv(\"/content/high_freq_nc_foods.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxzlQm4K4qde",
        "outputId": "859e664e-e750-4e5b-b6e0-abcf0f418141"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "高频未匹配食物（前30）：\n",
            "                                  Descriptionen_clean  count\n",
            "0   childrens' chewable vitamins with vitamin a (2...    240\n",
            "1                      vitamin d 400iu (10ug), tablet     34\n",
            "2        childrens' multivitamin and minerals, tablet     21\n",
            "3   bassetts chewy early health vitamins with a (4...     21\n",
            "4                   multivitamin and minerals, tablet     17\n",
            "5   childrens' vitamin c (120mg) plus zinc (3mg), ...     12\n",
            "6   prescription iron supplement, 27.5mg (e.g. syt...     12\n",
            "7                     childrens' multivitamins, drops     12\n",
            "8                    vitamin d 1000 iu (25ug), tablet      7\n",
            "9   wellkid multivitamin (age 4-12) (e.g. vitabiot...      7\n",
            "10  childrens' chewable multivitamins (age 3 plus)...      5\n",
            "11       calcium (250mg) & magnesium (157mg), capsule      4\n",
            "12         calcium (500mg) & vitamin d (10ug), tablet      4\n",
            "13                         magnesium (100 mg), tablet      4\n",
            "14                          vitamin c (200mg), tablet      3\n",
            "15              multivitamin with iron (14mg), tablet      2\n",
            "16                                iron (14mg), tablet      2\n",
            "17  calcium (400-600mg) with vitamin d (2-5ug), ta...      2\n",
            "18                         vitamin c (1000mg), tablet      1\n",
            "19          iron (14mg) with vitamin c (60mg), tablet      1\n",
            "20    fish oil (1100mg) with omega 3 (700mg), capsule      1\n",
            "21                      cod liver oil (500mg),capsule      1\n",
            "22  multivitamins (no minerals) (e.g. tesco/asda/b...      1\n"
          ]
        }
      ]
    }
  ]
}