{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhG9pswrLDIzYPTPe1Ze4Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NINGTANG1124/UPF-HFI/blob/main/notebook/intake24_nova_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect googledrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9izKK0g9LUQ",
        "outputId": "26228229-5670-4b6f-a418-f7dbf13f2e46"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: è¯»å– intake æ•°æ®ï¼ˆå« Descriptionen å’Œ FoodGroupenï¼‰\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/UPF-HFI/Bradford_original data/1. Dietmasterfile_foodlevel_clean.xls\"\n",
        "intake_df = pd.read_excel(file_path)\n"
      ],
      "metadata": {
        "id": "c7bYEbA49bs9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step2 æ¸…æ´— Description å’Œ Foodgroup\n",
        "intake_df[\"Foodgroupen_clean\"] = (\n",
        "    intake_df[\"Foodgroupen\"].astype(str).str.lower().str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
        ")\n",
        "\n",
        "intake_df[\"Descriptionen_clean\"] = (\n",
        "    intake_df[\"Descriptionen\"].astype(str).str.lower().str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
        ")\n"
      ],
      "metadata": {
        "id": "QbNxG8Re9glK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# description\n",
        "def match_nova_by_description(text):\n",
        "    text = str(text).lower().strip()\n",
        "\n",
        "    # === NOVA 1: é¥®ç”¨æ°´ ===\n",
        "    if any(w in text for w in [\"tap water\", \"still water\", \"filtered water\", \"plain water\"]):\n",
        "        if \"flavour\" not in text:\n",
        "            return 1, \"plain water (description)\"\n",
        "\n",
        "    # === NOVA 1: å¾®è§‚ä¹³åˆ¶å“ï¼ˆplainï¼‰===\n",
        "    if any(w in text for w in [\"semi skimmed milk\", \"skimmed milk\", \"whole milk\"]) and \"flavour\" not in text:\n",
        "        return 1, \"plain milk\"\n",
        "    if any(w in text for w in [\"natural yoghurt\", \"fromage frais\"]) and \"flavour\" not in text:\n",
        "        return 1, \"plain yoghurt\"\n",
        "\n",
        "    # === NOVA 1: ç²¾ç¡® raw/unprocessed ===\n",
        "    if text.startswith(\"raw \") or text.endswith(\" raw\") or \" (raw\" in text or \"uncooked\" in text:\n",
        "        return 1, \"raw/uncooked (precise)\"\n",
        "\n",
        "    # === NOVA 3: è‡ªåˆ¶ã€è½»åŠ å·¥ ===\n",
        "    if any(w in text for w in [\"homemade\", \"home made\"]):\n",
        "        return 3, \"homemade\"\n",
        "    if any(w in text for w in [\"boiled\", \"mashed potato\", \"baked potato\", \"jacket potato\"]):\n",
        "        return 3, \"boiled/baked/jacket\"\n",
        "\n",
        "    # === NOVA 4: å·¥ä¸šåŠ å·¥éº¦ç‰‡ï¼ˆå¦‚sachetç±»ï¼‰===\n",
        "    if \"porridge sachet\" in text or (\"porridge\" in text and \"oat so simple\" in text):\n",
        "        return 4, \"sachet porridge (description)\"\n",
        "\n",
        "    # === NOVA 4: takeaway å¿«é¤ç±» ===\n",
        "    if \"takeaway\" in text or \"take away\" in text:\n",
        "        return 4, \"takeaway food\"\n",
        "\n",
        "    # === NOVA 4: é›¶é£Ÿ/ç”œé£Ÿ/åŠ å·¥è„‚è‚ª ===\n",
        "    if any(w in text for w in [\"jam\", \"conserve\", \"marmalade\", \"chocolate spread\", \"ice cream topping\", \"marzipan\"]):\n",
        "        return 4, \"spread/syrup\"\n",
        "    if any(w in text for w in [\"cracker\", \"savoury biscuit\", \"cheddar biscuit\", \"cream cracker\"]):\n",
        "        return 4, \"processed snack\"\n",
        "    if any(w in text for w in [\"sweets\", \"gums\", \"jellies\", \"boiled sweets\", \"mints\", \"liquorice\", \"popcorn\"]):\n",
        "        return 4, \"sweet snack\"\n",
        "    if any(w in text for w in [\"ice cream\", \"dessert\", \"milkshake\"]):\n",
        "        return 4, \"processed dessert\"\n",
        "    if any(w in text for w in [\"margarine\", \"clover spread\", \"flora\"]):\n",
        "        return 4, \"processed fat\"\n",
        "    if \"flavoured milk\" in text or \"chocolate milk\" in text:\n",
        "        return 4, \"flavoured milk\"\n",
        "    if \"ketchup\" in text and \"home made\" not in text:\n",
        "        return 4, \"processed ketchup\"\n",
        "    if \"instant\" in text and \"porridge\" not in text:\n",
        "        return 4, \"instant food\"\n",
        "    # === NOVA 4: takeaway å¿«é¤ç±» ===\n",
        "    if \"takeaway\" in text or \"take away\" in text:\n",
        "        return 4, \"takeaway food\"\n",
        "\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "qYXdm7e1rO18"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# group\n",
        "def match_nova_by_group(group, description):\n",
        "    group = str(group).lower().strip()\n",
        "    description = str(description).lower().strip()\n",
        "\n",
        "    # === NOVA 1: group ç²¾ç¡®åŒ¹é… water ç±» ===\n",
        "    if group.strip() in [\"water\", \"tap water\", \"filtered water\"]:\n",
        "        return 1, \"water (group)\"\n",
        "\n",
        "    # === NOVA 1: æœªåŠ å·¥æœè”¬ã€ç‰›å¥¶ã€é…¸å¥¶ ===\n",
        "    if \"fresh fruit\" in group:\n",
        "        return 1, \"fruit (group)\"\n",
        "    if \"dried fruit\" in group:\n",
        "        return 1, \"dried fruit (group)\"\n",
        "    if \"vegetables\" in group and \"fried\" not in group:\n",
        "        return 1, \"vegetables (group)\"\n",
        "    if any(word in group for word in [\"semi skimmed milk\", \"skimmed milk\", \"whole milk\"]):\n",
        "        if \"flavour\" not in description and \"fruit\" not in description:\n",
        "            return 1, \"milk (group)\"\n",
        "    if any(word in group for word in [\"natural yoghurt\", \"fromage frais\"]):\n",
        "        if \"flavour\" not in description and \"fruit\" not in description:\n",
        "            return 1, \"yoghurt/plain dairy (group)\"\n",
        "\n",
        "    # === NOVA 3: æœ€å°åŠ å·¥è„‚è‚ª ===\n",
        "    if any(w in group for w in [\"olive oil\", \"rapeseed oil\", \"sunflower oil\", \"vegetable oil\", \"butter\"]):\n",
        "        return 3, \"culinary fat/oil (group)\"\n",
        "\n",
        "    # === NOVA 4: ç³–æµ†ã€æ—©é¤è°·ç‰©ã€åŠ å·¥è„‚è‚ª ===\n",
        "    if any(w in group for w in [\"margarine\", \"fat spread\", \"flora\", \"dairy fat spreads\", \"hard marg\"]):\n",
        "        return 4, \"processed fat (group)\"\n",
        "    if any(w in group for w in [\"jam\", \"conserve\", \"marmalade\"]):\n",
        "        return 4, \"preserves (group)\"\n",
        "    if \"other breakfast cereals\" in group or \"muesli\" in group or \"bran flakes\" in group:\n",
        "        return 4, \"processed cereal (group)\"\n",
        "\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "zRt397tRpuES"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Step 3: å®šä¹‰ä¸»å‡½æ•° match_nova()\n",
        "def match_nova(row):\n",
        "    description = row[\"Descriptionen_clean\"]\n",
        "    group = row[\"Foodgroupen_clean\"]\n",
        "\n",
        "    # Step 1: try description\n",
        "    nova, reason = match_nova_by_description(description)\n",
        "    if nova is not None:\n",
        "        return pd.Series([nova, \"description: \" + reason])\n",
        "\n",
        "    # Step 2: fallback to group\n",
        "    nova, reason = match_nova_by_group(group, description)\n",
        "    if nova is not None:\n",
        "        return pd.Series([nova, \"group: \" + reason])\n",
        "\n",
        "    return pd.Series([None, None])\n"
      ],
      "metadata": {
        "id": "LgPplkugtTTV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Step 4: åº”ç”¨åˆ° DataFrame ä¸Š\n",
        "intake_df[[\"NOVA\", \"match_reason\"]] = intake_df.apply(match_nova, axis=1)\n"
      ],
      "metadata": {
        "id": "-FKu22i2tWJ9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_save = [\n",
        "    \"Descriptionen\",\n",
        "    \"Foodgroupen\",\n",
        "    \"NOVA\",\n",
        "    \"match_reason\"\n",
        "]\n",
        "\n",
        "intake_df[cols_to_save].to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step1.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "pkShExdwiNwg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ Step 2: TF-IDF é«˜é˜ˆå€¼åŒ¹é…ï¼ˆ>0.99ï¼‰ã€æ•°æ®æºï¼šVKesaiteã€‘ Intake æè¿° vs FoodName å­—æ®µ ç‰¹ç‚¹ï¼šè‹±å›½ NDNS æ•°æ®ï¼Œè¯­ä¹‰è´´åˆåº¦é«˜ åŒ¹é…åå­—æ®µï¼š Matched_NOVA Source = 'tfidf_vk_099' Similarity_score\n",
        "\n",
        "ğŸ”¹ Step 3: TF-IDF ä¸­é˜ˆå€¼åŒ¹é…ï¼ˆ>0.85ï¼‰ã€æ•°æ®æºï¼šGiulia FNDDSã€‘ Intake æè¿° vs FoodName/Description å­—æ®µï¼ˆè§†ç»“æ„è€Œå®šï¼‰ ç‰¹ç‚¹ï¼šåŒ¹é…é¢å¹¿ä½†é£æ ¼åç¾å¼ å¯ä½œä¸ºç¬¬äºŒæƒé‡åŒ¹é…æºè¡¥å……ç©ºå€¼ åŒ¹é…åï¼š Source = 'tfidf_giulia_085'\n",
        "\n",
        "ğŸ”¹ Step 4: TF-IDF æˆ– SBERT è¯­ä¹‰åŒ¹é…ï¼ˆ>0.85ï¼‰ã€æ•°æ®æºï¼šOFFã€‘ ä¸¤ç§æ–¹å¼éƒ½å¯ç”¨ï¼š TF-IDF åŒ¹é… product_name å­—æ®µ SBERT åŒ¹é…æè¿°ï¼ˆæ¨è MiniLM ï¼‰ ç”¨äºæœ€åè¡¥å……ç©ºå€¼ï¼Œæé«˜ recallï¼ˆå¬å›ç‡ï¼‰ åŒ¹é…åï¼š Source = 'tfidf_off' æˆ– 'sbert_off'\n",
        "\n",
        "ğŸ”¹ Step 5: æ•´åˆ + äººå·¥è¡¥å…¨ + Final è¾“å‡º"
      ],
      "metadata": {
        "id": "2TluyxWVNvxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# novaæ–‡ä»¶æ•°æ®æ¸…æ´—\n",
        "# ndns\n",
        "ndns_df = pd.read_csv(\"/content/drive/MyDrive/UPF-HFI/nova/NDNS_NOVA_DATABASE.new2023.csv\", encoding=\"ISO-8859-1\")\n",
        "ndns_df.columns = ndns_df.columns.str.strip()\n",
        "ndns_df = ndns_df[[\"FoodName\", \"NOVA\"]].dropna()\n",
        "ndns_df[\"FoodName_clean\"] = ndns_df[\"FoodName\"].str.lower().str.replace(r\"[^\\w\\s]\", \" \", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "ndns_df = ndns_df.drop_duplicates(subset=[\"FoodName_clean\"])\n"
      ],
      "metadata": {
        "id": "OMkARVTN1cMN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ç¾å›½çš„\n",
        "giulia_df = pd.read_excel(\"/content/drive/MyDrive/UPF-HFI/nova/Training Data Original Given by NOVA Researchers - Corrections by Giulia Babak FNDDS 2009-10.xls\")\n",
        "giulia_df.columns = giulia_df.columns.str.strip()\n",
        "\n",
        "giulia_df = giulia_df[[\"Main_food_description\", \"SR_nova_group\"]].dropna()\n",
        "giulia_df = giulia_df.rename(columns={\"Main_food_description\": \"FoodName\", \"SR_nova_group\": \"NOVA\"})\n",
        "\n",
        "giulia_df[\"FoodName_clean\"] = giulia_df[\"FoodName\"].str.lower().str.replace(r\"[^\\w\\s]\", \" \", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "giulia_df = giulia_df.drop_duplicates(subset=[\"FoodName_clean\"])\n"
      ],
      "metadata": {
        "id": "AgvkK3io1vib"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# offçš„\n",
        "off_clean = []\n",
        "with open(\"/content/drive/MyDrive/UPF-HFI/nova/openfoodfacts-popular-24.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            entry = json.loads(line)\n",
        "            if not isinstance(entry, dict):\n",
        "                continue  # è·³è¿‡éå¯¹è±¡\n",
        "            name = entry.get(\"product_name\") or entry.get(\"abbreviated_product_name\")\n",
        "            nova = entry.get(\"nova_group\")\n",
        "            if name and nova:\n",
        "                name_clean = re.sub(r\"[^\\w\\s]\", \" \", name.lower())\n",
        "                name_clean = re.sub(r\"\\s+\", \" \", name_clean).strip()\n",
        "                off_clean.append({\"FoodName_clean\": name_clean, \"NOVA\": nova})\n",
        "        except json.JSONDecodeError:\n",
        "            continue  # å¿½ç•¥é”™è¯¯è¡Œ\n",
        "\n",
        "off_df = pd.DataFrame(off_clean).drop_duplicates(subset=[\"FoodName_clean\"])\n"
      ],
      "metadata": {
        "id": "rjtOfuR12Idz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndns_df.to_csv(\"NDNS_clean.csv\", index=False)\n",
        "giulia_df.to_csv(\"Giulia_clean.csv\", index=False)\n",
        "off_df.to_csv(\"OFF_clean.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "4xz6rJST2_lG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Step 2ï¼šTF-IDF åŒ¹é…æœªå®Œæˆéƒ¨åˆ†ï¼ˆåŸºäº NOVA å¯¹ç…§æ± ï¼‰\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "OTXojW7IbVIm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§± 1. åˆå¹¶å¯¹ç…§åº“ä½œä¸º TF-IDF çš„ reference\n",
        "nova_pool = pd.concat([ndns_df, giulia_df, off_df], ignore_index=True)\n",
        "nova_pool = nova_pool.drop_duplicates(subset=[\"FoodName_clean\"])\n",
        "\n",
        "# ğŸ§± 2. åŠ è½½ intake åŸå§‹æ•°æ®ï¼ˆå« Step1 ç»“æœï¼‰\n",
        "intake_df = pd.read_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step1.csv\")\n",
        "\n",
        "# ğŸ§± 3. é€‰å‡º NOVA_step1 æ˜¯ç¼ºå¤±çš„é£Ÿç‰©\n",
        "mask_missing = intake_df[\"NOVA_step1\"].isna()\n",
        "query_texts = intake_df.loc[mask_missing, \"food_name_clean\"].dropna()\n",
        "query_texts_index = query_texts.index\n",
        "\n",
        "# ğŸ§± 4. æ„å»º TF-IDF å‘é‡å™¨å¹¶è½¬æ¢\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_ref = vectorizer.fit_transform(nova_pool[\"FoodName_clean\"])\n",
        "tfidf_query = vectorizer.transform(query_texts)\n",
        "\n",
        "# ğŸ§± 5. åŒ¹é…å¹¶è¿”å›å¾—åˆ†å’ŒåŒ¹é…å†…å®¹\n",
        "similarity_matrix = cosine_similarity(tfidf_query, tfidf_ref)\n",
        "best_match_idx = similarity_matrix.argmax(axis=1)\n",
        "best_match_score = similarity_matrix.max(axis=1)\n",
        "matched_nova = nova_pool.iloc[best_match_idx][\"NOVA\"].values\n",
        "matched_name = nova_pool.iloc[best_match_idx][\"FoodName_clean\"].values\n",
        "\n",
        "# ğŸ§± 6. å›å†™ intake_df ä¸­\n",
        "intake_df.loc[query_texts_index, \"NOVA_step2\"] = matched_nova\n",
        "intake_df.loc[query_texts_index, \"TFIDF_score\"] = best_match_score\n",
        "intake_df.loc[query_texts_index, \"TFIDF_match_name\"] = matched_name\n",
        "\n",
        "# âœ… å¯é€‰ï¼šè®¾ç½®åŒ¹é…é˜ˆå€¼\n",
        "threshold = 0.85\n",
        "intake_df.loc[intake_df[\"TFIDF_score\"] < threshold, [\"NOVA_step2\", \"TFIDF_match_name\"]] = [None, None]\n",
        "\n",
        "# âœ… ä¿å­˜è¾“å‡º\n",
        "intake_df.to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step2.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "X-NxAZia6Jl1",
        "outputId": "6c769881-f9b3-42a2-8a34-212f8de3e1e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'NOVA_step1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'NOVA_step1'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-3557634488.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# ğŸ§± 3. é€‰å‡º NOVA_step1 æ˜¯ç¼ºå¤±çš„é£Ÿç‰©\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmask_missing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintake_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NOVA_step1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mquery_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintake_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_missing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"food_name_clean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mquery_texts_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'NOVA_step1'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Step 2 åˆ†æä»£ç \n",
        "# âœ… 1. åŒ¹é…æˆåŠŸæ•°é‡å’Œæ¯”ä¾‹\n",
        "matched_tfidf = intake_df[\"NOVA_step2\"].notna().sum()\n",
        "total_tfidf_targets = intake_df[\"NOVA_step1\"].isna().sum()\n",
        "match_rate_tfidf = matched_tfidf / total_tfidf_targets\n",
        "\n",
        "print(f\"ğŸ” Step 2ï¼ˆTF-IDFï¼‰åŒ¹é…æˆåŠŸæ•°: {matched_tfidf} / {total_tfidf_targets} = {match_rate_tfidf:.2%}\")\n",
        "\n",
        "# âœ… 2. åŒ¹é…ç½®ä¿¡åº¦ç»Ÿè®¡\n",
        "print(\"\\nğŸ“Š TF-IDF åŒ¹é…å¾—åˆ†æè¿°æ€§ç»Ÿè®¡ï¼š\")\n",
        "print(intake_df[\"TFIDF_score\"].describe())\n",
        "\n",
        "# âœ… 3. æŸ¥çœ‹ä½ç½®ä¿¡åº¦ï¼ˆå¾—åˆ† < 0.85ï¼‰ç¤ºä¾‹\n",
        "low_confidence = intake_df.query(\"TFIDF_score < 0.85 and TFIDF_score.notna()\").sort_values(by=\"TFIDF_score\")\n",
        "low_confidence[[\"food_name_clean\", \"TFIDF_match_name\", \"TFIDF_score\", \"NOVA_step2\"]].head(10)\n"
      ],
      "metadata": {
        "id": "IFYnCDwA61cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§© Step 3ï¼šåˆå¹¶ Step1 ä¸ Step2 åŒ¹é…ç»“æœï¼Œå½¢æˆæœ€ç»ˆ NOVA åˆ—\n",
        "\n",
        "def combine_nova(row):\n",
        "    if pd.notna(row[\"NOVA_step1\"]):\n",
        "        return row[\"NOVA_step1\"]\n",
        "    elif pd.notna(row[\"NOVA_step2\"]):\n",
        "        return row[\"NOVA_step2\"]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "intake_df[\"NOVA_final\"] = intake_df.apply(combine_nova, axis=1)\n",
        "\n",
        "# åŒæ—¶ä¿ç•™æ¥æºï¼ˆè¯´æ˜åŒ¹é…æ¥æºæ˜¯ Step1 / Step2 / Noneï¼‰\n",
        "def get_reason(row):\n",
        "    if pd.notna(row[\"NOVA_step1\"]):\n",
        "        return \"Keyword\"\n",
        "    elif pd.notna(row[\"NOVA_step2\"]):\n",
        "        return \"TF-IDF\"\n",
        "    else:\n",
        "        return \"Unmatched\"\n",
        "\n",
        "intake_df[\"Match_source\"] = intake_df.apply(get_reason, axis=1)\n",
        "\n",
        "# âœ… ä¿å­˜æœ€ç»ˆç»“æœ\n",
        "intake_df.to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step3.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "NX-zuEsc6jRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Step 3 åˆ†æä»£ç ï¼š\n",
        "# âœ… 1. åŒ¹é…æ¥æºåˆ†å¸ƒ\n",
        "print(\"\\nğŸ“Š åŒ¹é…æ¥æºåˆ†å¸ƒç»Ÿè®¡ï¼š\")\n",
        "print(intake_df[\"Match_source\"].value_counts(dropna=False))\n",
        "print(\"\\nğŸ“Š åŒ¹é…æ¥æºç™¾åˆ†æ¯”ï¼š\")\n",
        "print(intake_df[\"Match_source\"].value_counts(normalize=True, dropna=False).map(\"{:.2%}\".format))\n",
        "\n",
        "# âœ… 2. å¯é€‰ï¼šæ¯ç§ Match_source åœ¨ Foodgroupen ä¸­çš„åˆ†å¸ƒï¼ˆå¦‚éœ€æ·±å…¥åˆ†æï¼‰\n",
        "# pd.crosstab(intake_df[\"Foodgroupen\"], intake_df[\"Match_source\"])\n"
      ],
      "metadata": {
        "id": "ieBV0-RY6-Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ¯ Step 4ï¼šä½¿ç”¨ SBERT å¯¹å‰©ä½™ NOVA_final ä¸ºç©ºçš„é£Ÿç‰©è¿›è¡Œè¯­ä¹‰åŒ¹é…è¡¥å…¨"
      ],
      "metadata": {
        "id": "bcR92-jE7KdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "PqbMD6e87JlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ¨è all-MiniLM-L6-v2ï¼‰ï¼š\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "x-po5i9d7vel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ›  Step 4ï¼šå¯¹ NOVA_final ä¸ºç©ºçš„é£Ÿç‰©è¿›è¡Œ SBERT åŒ¹é…\n",
        "# âœ… 1. å‡†å¤‡å€™é€‰åº“ï¼ˆå¯¹ nova_pool ç¼–ç ï¼‰\n",
        "# ç¡®ä¿ä½ ä¹‹å‰å‡†å¤‡å¥½çš„ nova_pool æœ‰ FoodName_clean åˆ—\n",
        "ref_texts = nova_pool[\"FoodName_clean\"].tolist()\n",
        "ref_embeddings = model.encode(ref_texts, convert_to_tensor=True)\n"
      ],
      "metadata": {
        "id": "LCxkp7aG71gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… 2. é€‰å–å¾…åŒ¹é…é£Ÿç‰©ï¼ˆNOVA_final ä¸ºç©ºï¼‰\n",
        "unmatched_df = intake_df[intake_df[\"NOVA_final\"].isna()].copy()\n",
        "query_texts = unmatched_df[\"food_name_clean\"].dropna().tolist()\n",
        "query_indices = unmatched_df[\"food_name_clean\"].dropna().index\n",
        "\n",
        "query_embeddings = model.encode(query_texts, convert_to_tensor=True)\n"
      ],
      "metadata": {
        "id": "lrebQJ1R75Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… 3. è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¹¶æå–åŒ¹é…ç»“æœ\n",
        "cosine_scores = util.pytorch_cos_sim(query_embeddings, ref_embeddings)\n",
        "top_scores, top_indices = torch.max(cosine_scores, dim=1)\n",
        "\n",
        "# å†™å…¥ç»“æœ\n",
        "intake_df.loc[query_indices, \"SBERT_score\"] = top_scores.cpu().numpy()\n",
        "intake_df.loc[query_indices, \"SBERT_match_name\"] = nova_pool.iloc[top_indices.cpu().numpy()][\"FoodName_clean\"].values\n",
        "intake_df.loc[query_indices, \"NOVA_step4\"] = nova_pool.iloc[top_indices.cpu().numpy()][\"NOVA\"].values\n"
      ],
      "metadata": {
        "id": "C6GsX7UD79Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… 4. æ›´æ–°æœ€ç»ˆåˆ—ï¼šNOVA_final + Match_source\n",
        "# å¦‚æœ Step3 æ²¡æ‰¾åˆ°ä½† Step4 æ‰¾åˆ°äº†ï¼Œä½¿ç”¨ SBERT åŒ¹é…ç»“æœ\n",
        "intake_df[\"NOVA_final\"] = intake_df[\"NOVA_final\"].combine_first(intake_df[\"NOVA_step4\"])\n",
        "\n",
        "# åŒæ ·æ›´æ–°åŒ¹é…æ¥æº\n",
        "intake_df[\"Match_source\"] = intake_df.apply(lambda row: (\n",
        "    \"SBERT\" if pd.notna(row[\"NOVA_step4\"]) and pd.isna(row[\"NOVA_step1\"]) and pd.isna(row[\"NOVA_step2\"])\n",
        "    else row[\"Match_source\"]\n",
        "), axis=1)\n"
      ],
      "metadata": {
        "id": "NT3oNuGY8EUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ› ï¸ é‡æ–°æŒ‡å®šåˆ—é¡ºåº\n",
        "col_order = [\n",
        "    'food_name_clean', 'Descriptionen', 'Foodgroupen',\n",
        "    'NOVA_step1', 'Match_reason',\n",
        "    'NOVA_step2', 'TFIDF_score', 'TFIDF_match_name',\n",
        "    'NOVA_step4', 'SBERT_score', 'SBERT_match_name',\n",
        "    'Match_source', 'NOVA_final'\n",
        "]\n",
        "\n",
        "# åº”ç”¨é¡ºåºå¹¶å¯¼å‡º\n",
        "intake_df = intake_df[col_order]\n",
        "intake_df.to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step4_final.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "FSTQSiDGzFlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unmatched_final = intake_df[intake_df[\"NOVA_final\"].isin([None, \"\", \"NC\"])]\n",
        "print(f\"âŒ å®é™…æœªåŒ¹é…ä¸Šçš„é£Ÿç‰©æ•°é‡ï¼ˆå« NCï¼‰ï¼š{len(unmatched_final)}\")\n"
      ],
      "metadata": {
        "id": "WglYB0ua0dbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æå–æœªåŒ¹é…çš„è¡Œï¼ˆNone, \"\", \"NC\"ï¼‰\n",
        "unmatched_final = intake_df[intake_df[\"NOVA_final\"].isin([None, \"\", \"NC\"])]\n",
        "\n",
        "# æŒ‰ food_name_clean ç»Ÿè®¡é¢‘ç‡\n",
        "nc_counts = unmatched_final[\"food_name_clean\"].value_counts().reset_index()\n",
        "nc_counts.columns = [\"food_name_clean\", \"count\"]\n",
        "\n",
        "# å±•ç¤ºå‰ 30 ä¸ªé«˜é¢‘æœªåŒ¹é…æ¡ç›®\n",
        "print(\"ğŸ½ï¸ é«˜é¢‘æœªåŒ¹é…é£Ÿç‰©ï¼ˆå‰30ï¼‰ï¼š\")\n",
        "print(nc_counts.head(30))\n",
        "\n",
        "# å¯é€‰ï¼šå¯¼å‡ºæˆ CSV æ–‡ä»¶\n",
        "nc_counts.to_csv(\"/content/high_freq_nc_foods.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "JxzlQm4K4qde"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}