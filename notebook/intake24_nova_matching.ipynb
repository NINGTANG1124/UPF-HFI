{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhG9pswrLDIzYPTPe1Ze4Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NINGTANG1124/UPF-HFI/blob/main/notebook/intake24_nova_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect googledrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9izKK0g9LUQ",
        "outputId": "26228229-5670-4b6f-a418-f7dbf13f2e46"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: 读取 intake 数据（含 Descriptionen 和 FoodGroupen）\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/UPF-HFI/Bradford_original data/1. Dietmasterfile_foodlevel_clean.xls\"\n",
        "intake_df = pd.read_excel(file_path)\n"
      ],
      "metadata": {
        "id": "c7bYEbA49bs9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step2 清洗 Description 和 Foodgroup\n",
        "intake_df[\"Foodgroupen_clean\"] = (\n",
        "    intake_df[\"Foodgroupen\"].astype(str).str.lower().str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
        ")\n",
        "\n",
        "intake_df[\"Descriptionen_clean\"] = (\n",
        "    intake_df[\"Descriptionen\"].astype(str).str.lower().str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
        ")\n"
      ],
      "metadata": {
        "id": "QbNxG8Re9glK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# description\n",
        "def match_nova_by_description(text):\n",
        "    text = str(text).lower().strip()\n",
        "\n",
        "    # === NOVA 1: 饮用水 ===\n",
        "    if any(w in text for w in [\"tap water\", \"still water\", \"filtered water\", \"plain water\"]):\n",
        "        if \"flavour\" not in text:\n",
        "            return 1, \"plain water (description)\"\n",
        "\n",
        "    # === NOVA 1: 微观乳制品（plain）===\n",
        "    if any(w in text for w in [\"semi skimmed milk\", \"skimmed milk\", \"whole milk\"]) and \"flavour\" not in text:\n",
        "        return 1, \"plain milk\"\n",
        "    if any(w in text for w in [\"natural yoghurt\", \"fromage frais\"]) and \"flavour\" not in text:\n",
        "        return 1, \"plain yoghurt\"\n",
        "\n",
        "    # === NOVA 1: 精确 raw/unprocessed ===\n",
        "    if text.startswith(\"raw \") or text.endswith(\" raw\") or \" (raw\" in text or \"uncooked\" in text:\n",
        "        return 1, \"raw/uncooked (precise)\"\n",
        "\n",
        "    # === NOVA 3: 自制、轻加工 ===\n",
        "    if any(w in text for w in [\"homemade\", \"home made\"]):\n",
        "        return 3, \"homemade\"\n",
        "    if any(w in text for w in [\"boiled\", \"mashed potato\", \"baked potato\", \"jacket potato\"]):\n",
        "        return 3, \"boiled/baked/jacket\"\n",
        "\n",
        "    # === NOVA 4: 工业加工麦片（如sachet类）===\n",
        "    if \"porridge sachet\" in text or (\"porridge\" in text and \"oat so simple\" in text):\n",
        "        return 4, \"sachet porridge (description)\"\n",
        "\n",
        "    # === NOVA 4: takeaway 快餐类 ===\n",
        "    if \"takeaway\" in text or \"take away\" in text:\n",
        "        return 4, \"takeaway food\"\n",
        "\n",
        "    # === NOVA 4: 零食/甜食/加工脂肪 ===\n",
        "    if any(w in text for w in [\"jam\", \"conserve\", \"marmalade\", \"chocolate spread\", \"ice cream topping\", \"marzipan\"]):\n",
        "        return 4, \"spread/syrup\"\n",
        "    if any(w in text for w in [\"cracker\", \"savoury biscuit\", \"cheddar biscuit\", \"cream cracker\"]):\n",
        "        return 4, \"processed snack\"\n",
        "    if any(w in text for w in [\"sweets\", \"gums\", \"jellies\", \"boiled sweets\", \"mints\", \"liquorice\", \"popcorn\"]):\n",
        "        return 4, \"sweet snack\"\n",
        "    if any(w in text for w in [\"ice cream\", \"dessert\", \"milkshake\"]):\n",
        "        return 4, \"processed dessert\"\n",
        "    if any(w in text for w in [\"margarine\", \"clover spread\", \"flora\"]):\n",
        "        return 4, \"processed fat\"\n",
        "    if \"flavoured milk\" in text or \"chocolate milk\" in text:\n",
        "        return 4, \"flavoured milk\"\n",
        "    if \"ketchup\" in text and \"home made\" not in text:\n",
        "        return 4, \"processed ketchup\"\n",
        "    if \"instant\" in text and \"porridge\" not in text:\n",
        "        return 4, \"instant food\"\n",
        "    # === NOVA 4: takeaway 快餐类 ===\n",
        "    if \"takeaway\" in text or \"take away\" in text:\n",
        "        return 4, \"takeaway food\"\n",
        "\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "qYXdm7e1rO18"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# group\n",
        "def match_nova_by_group(group, description):\n",
        "    group = str(group).lower().strip()\n",
        "    description = str(description).lower().strip()\n",
        "\n",
        "    # === NOVA 1: group 精确匹配 water 类 ===\n",
        "    if group.strip() in [\"water\", \"tap water\", \"filtered water\"]:\n",
        "        return 1, \"water (group)\"\n",
        "\n",
        "    # === NOVA 1: 未加工果蔬、牛奶、酸奶 ===\n",
        "    if \"fresh fruit\" in group:\n",
        "        return 1, \"fruit (group)\"\n",
        "    if \"dried fruit\" in group:\n",
        "        return 1, \"dried fruit (group)\"\n",
        "    if \"vegetables\" in group and \"fried\" not in group:\n",
        "        return 1, \"vegetables (group)\"\n",
        "    if any(word in group for word in [\"semi skimmed milk\", \"skimmed milk\", \"whole milk\"]):\n",
        "        if \"flavour\" not in description and \"fruit\" not in description:\n",
        "            return 1, \"milk (group)\"\n",
        "    if any(word in group for word in [\"natural yoghurt\", \"fromage frais\"]):\n",
        "        if \"flavour\" not in description and \"fruit\" not in description:\n",
        "            return 1, \"yoghurt/plain dairy (group)\"\n",
        "\n",
        "    # === NOVA 3: 最小加工脂肪 ===\n",
        "    if any(w in group for w in [\"olive oil\", \"rapeseed oil\", \"sunflower oil\", \"vegetable oil\", \"butter\"]):\n",
        "        return 3, \"culinary fat/oil (group)\"\n",
        "\n",
        "    # === NOVA 4: 糖浆、早餐谷物、加工脂肪 ===\n",
        "    if any(w in group for w in [\"margarine\", \"fat spread\", \"flora\", \"dairy fat spreads\", \"hard marg\"]):\n",
        "        return 4, \"processed fat (group)\"\n",
        "    if any(w in group for w in [\"jam\", \"conserve\", \"marmalade\"]):\n",
        "        return 4, \"preserves (group)\"\n",
        "    if \"other breakfast cereals\" in group or \"muesli\" in group or \"bran flakes\" in group:\n",
        "        return 4, \"processed cereal (group)\"\n",
        "\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "zRt397tRpuES"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 3: 定义主函数 match_nova()\n",
        "def match_nova(row):\n",
        "    description = row[\"Descriptionen_clean\"]\n",
        "    group = row[\"Foodgroupen_clean\"]\n",
        "\n",
        "    # Step 1: try description\n",
        "    nova, reason = match_nova_by_description(description)\n",
        "    if nova is not None:\n",
        "        return pd.Series([nova, \"description: \" + reason])\n",
        "\n",
        "    # Step 2: fallback to group\n",
        "    nova, reason = match_nova_by_group(group, description)\n",
        "    if nova is not None:\n",
        "        return pd.Series([nova, \"group: \" + reason])\n",
        "\n",
        "    return pd.Series([None, None])\n"
      ],
      "metadata": {
        "id": "LgPplkugtTTV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 4: 应用到 DataFrame 上\n",
        "intake_df[[\"NOVA\", \"match_reason\"]] = intake_df.apply(match_nova, axis=1)\n"
      ],
      "metadata": {
        "id": "-FKu22i2tWJ9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_save = [\n",
        "    \"Descriptionen\",\n",
        "    \"Foodgroupen\",\n",
        "    \"NOVA\",\n",
        "    \"match_reason\"\n",
        "]\n",
        "\n",
        "intake_df[cols_to_save].to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step1.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "pkShExdwiNwg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Step 2: TF-IDF 高阈值匹配（>0.99）【数据源：VKesaite】 Intake 描述 vs FoodName 字段 特点：英国 NDNS 数据，语义贴合度高 匹配后字段： Matched_NOVA Source = 'tfidf_vk_099' Similarity_score\n",
        "\n",
        "🔹 Step 3: TF-IDF 中阈值匹配（>0.85）【数据源：Giulia FNDDS】 Intake 描述 vs FoodName/Description 字段（视结构而定） 特点：匹配面广但风格偏美式 可作为第二权重匹配源补充空值 匹配后： Source = 'tfidf_giulia_085'\n",
        "\n",
        "🔹 Step 4: TF-IDF 或 SBERT 语义匹配（>0.85）【数据源：OFF】 两种方式都可用： TF-IDF 匹配 product_name 字段 SBERT 匹配描述（推荐 MiniLM ） 用于最后补充空值，提高 recall（召回率） 匹配后： Source = 'tfidf_off' 或 'sbert_off'\n",
        "\n",
        "🔹 Step 5: 整合 + 人工补全 + Final 输出"
      ],
      "metadata": {
        "id": "2TluyxWVNvxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nova文件数据清洗\n",
        "# ndns\n",
        "ndns_df = pd.read_csv(\"/content/drive/MyDrive/UPF-HFI/nova/NDNS_NOVA_DATABASE.new2023.csv\", encoding=\"ISO-8859-1\")\n",
        "ndns_df.columns = ndns_df.columns.str.strip()\n",
        "ndns_df = ndns_df[[\"FoodName\", \"NOVA\"]].dropna()\n",
        "ndns_df[\"FoodName_clean\"] = ndns_df[\"FoodName\"].str.lower().str.replace(r\"[^\\w\\s]\", \" \", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "ndns_df = ndns_df.drop_duplicates(subset=[\"FoodName_clean\"])\n"
      ],
      "metadata": {
        "id": "OMkARVTN1cMN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 美国的\n",
        "giulia_df = pd.read_excel(\"/content/drive/MyDrive/UPF-HFI/nova/Training Data Original Given by NOVA Researchers - Corrections by Giulia Babak FNDDS 2009-10.xls\")\n",
        "giulia_df.columns = giulia_df.columns.str.strip()\n",
        "\n",
        "giulia_df = giulia_df[[\"Main_food_description\", \"SR_nova_group\"]].dropna()\n",
        "giulia_df = giulia_df.rename(columns={\"Main_food_description\": \"FoodName\", \"SR_nova_group\": \"NOVA\"})\n",
        "\n",
        "giulia_df[\"FoodName_clean\"] = giulia_df[\"FoodName\"].str.lower().str.replace(r\"[^\\w\\s]\", \" \", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "giulia_df = giulia_df.drop_duplicates(subset=[\"FoodName_clean\"])\n"
      ],
      "metadata": {
        "id": "AgvkK3io1vib"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# off的\n",
        "off_clean = []\n",
        "with open(\"/content/drive/MyDrive/UPF-HFI/nova/openfoodfacts-popular-24.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            entry = json.loads(line)\n",
        "            if not isinstance(entry, dict):\n",
        "                continue  # 跳过非对象\n",
        "            name = entry.get(\"product_name\") or entry.get(\"abbreviated_product_name\")\n",
        "            nova = entry.get(\"nova_group\")\n",
        "            if name and nova:\n",
        "                name_clean = re.sub(r\"[^\\w\\s]\", \" \", name.lower())\n",
        "                name_clean = re.sub(r\"\\s+\", \" \", name_clean).strip()\n",
        "                off_clean.append({\"FoodName_clean\": name_clean, \"NOVA\": nova})\n",
        "        except json.JSONDecodeError:\n",
        "            continue  # 忽略错误行\n",
        "\n",
        "off_df = pd.DataFrame(off_clean).drop_duplicates(subset=[\"FoodName_clean\"])\n"
      ],
      "metadata": {
        "id": "rjtOfuR12Idz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndns_df.to_csv(\"NDNS_clean.csv\", index=False)\n",
        "giulia_df.to_csv(\"Giulia_clean.csv\", index=False)\n",
        "off_df.to_csv(\"OFF_clean.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "4xz6rJST2_lG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 2：TF-IDF 匹配未完成部分（基于 NOVA 对照池）\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "OTXojW7IbVIm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧱 1. 合并对照库作为 TF-IDF 的 reference\n",
        "nova_pool = pd.concat([ndns_df, giulia_df, off_df], ignore_index=True)\n",
        "nova_pool = nova_pool.drop_duplicates(subset=[\"FoodName_clean\"])\n",
        "\n",
        "# 🧱 2. 加载 intake 原始数据（含 Step1 结果）\n",
        "intake_df = pd.read_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step1.csv\")\n",
        "\n",
        "# 🧱 3. 选出 NOVA_step1 是缺失的食物\n",
        "mask_missing = intake_df[\"NOVA_step1\"].isna()\n",
        "query_texts = intake_df.loc[mask_missing, \"food_name_clean\"].dropna()\n",
        "query_texts_index = query_texts.index\n",
        "\n",
        "# 🧱 4. 构建 TF-IDF 向量器并转换\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_ref = vectorizer.fit_transform(nova_pool[\"FoodName_clean\"])\n",
        "tfidf_query = vectorizer.transform(query_texts)\n",
        "\n",
        "# 🧱 5. 匹配并返回得分和匹配内容\n",
        "similarity_matrix = cosine_similarity(tfidf_query, tfidf_ref)\n",
        "best_match_idx = similarity_matrix.argmax(axis=1)\n",
        "best_match_score = similarity_matrix.max(axis=1)\n",
        "matched_nova = nova_pool.iloc[best_match_idx][\"NOVA\"].values\n",
        "matched_name = nova_pool.iloc[best_match_idx][\"FoodName_clean\"].values\n",
        "\n",
        "# 🧱 6. 回写 intake_df 中\n",
        "intake_df.loc[query_texts_index, \"NOVA_step2\"] = matched_nova\n",
        "intake_df.loc[query_texts_index, \"TFIDF_score\"] = best_match_score\n",
        "intake_df.loc[query_texts_index, \"TFIDF_match_name\"] = matched_name\n",
        "\n",
        "# ✅ 可选：设置匹配阈值\n",
        "threshold = 0.85\n",
        "intake_df.loc[intake_df[\"TFIDF_score\"] < threshold, [\"NOVA_step2\", \"TFIDF_match_name\"]] = [None, None]\n",
        "\n",
        "# ✅ 保存输出\n",
        "intake_df.to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step2.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "X-NxAZia6Jl1",
        "outputId": "6c769881-f9b3-42a2-8a34-212f8de3e1e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'NOVA_step1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'NOVA_step1'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-3557634488.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 🧱 3. 选出 NOVA_step1 是缺失的食物\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmask_missing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintake_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NOVA_step1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mquery_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintake_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_missing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"food_name_clean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mquery_texts_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'NOVA_step1'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 2 分析代码\n",
        "# ✅ 1. 匹配成功数量和比例\n",
        "matched_tfidf = intake_df[\"NOVA_step2\"].notna().sum()\n",
        "total_tfidf_targets = intake_df[\"NOVA_step1\"].isna().sum()\n",
        "match_rate_tfidf = matched_tfidf / total_tfidf_targets\n",
        "\n",
        "print(f\"🔍 Step 2（TF-IDF）匹配成功数: {matched_tfidf} / {total_tfidf_targets} = {match_rate_tfidf:.2%}\")\n",
        "\n",
        "# ✅ 2. 匹配置信度统计\n",
        "print(\"\\n📊 TF-IDF 匹配得分描述性统计：\")\n",
        "print(intake_df[\"TFIDF_score\"].describe())\n",
        "\n",
        "# ✅ 3. 查看低置信度（得分 < 0.85）示例\n",
        "low_confidence = intake_df.query(\"TFIDF_score < 0.85 and TFIDF_score.notna()\").sort_values(by=\"TFIDF_score\")\n",
        "low_confidence[[\"food_name_clean\", \"TFIDF_match_name\", \"TFIDF_score\", \"NOVA_step2\"]].head(10)\n"
      ],
      "metadata": {
        "id": "IFYnCDwA61cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧩 Step 3：合并 Step1 与 Step2 匹配结果，形成最终 NOVA 列\n",
        "\n",
        "def combine_nova(row):\n",
        "    if pd.notna(row[\"NOVA_step1\"]):\n",
        "        return row[\"NOVA_step1\"]\n",
        "    elif pd.notna(row[\"NOVA_step2\"]):\n",
        "        return row[\"NOVA_step2\"]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "intake_df[\"NOVA_final\"] = intake_df.apply(combine_nova, axis=1)\n",
        "\n",
        "# 同时保留来源（说明匹配来源是 Step1 / Step2 / None）\n",
        "def get_reason(row):\n",
        "    if pd.notna(row[\"NOVA_step1\"]):\n",
        "        return \"Keyword\"\n",
        "    elif pd.notna(row[\"NOVA_step2\"]):\n",
        "        return \"TF-IDF\"\n",
        "    else:\n",
        "        return \"Unmatched\"\n",
        "\n",
        "intake_df[\"Match_source\"] = intake_df.apply(get_reason, axis=1)\n",
        "\n",
        "# ✅ 保存最终结果\n",
        "intake_df.to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step3.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "NX-zuEsc6jRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 3 分析代码：\n",
        "# ✅ 1. 匹配来源分布\n",
        "print(\"\\n📊 匹配来源分布统计：\")\n",
        "print(intake_df[\"Match_source\"].value_counts(dropna=False))\n",
        "print(\"\\n📊 匹配来源百分比：\")\n",
        "print(intake_df[\"Match_source\"].value_counts(normalize=True, dropna=False).map(\"{:.2%}\".format))\n",
        "\n",
        "# ✅ 2. 可选：每种 Match_source 在 Foodgroupen 中的分布（如需深入分析）\n",
        "# pd.crosstab(intake_df[\"Foodgroupen\"], intake_df[\"Match_source\"])\n"
      ],
      "metadata": {
        "id": "ieBV0-RY6-Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Step 4：使用 SBERT 对剩余 NOVA_final 为空的食物进行语义匹配补全"
      ],
      "metadata": {
        "id": "bcR92-jE7KdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "PqbMD6e87JlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 加载预训练模型（推荐 all-MiniLM-L6-v2）：\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "x-po5i9d7vel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🛠 Step 4：对 NOVA_final 为空的食物进行 SBERT 匹配\n",
        "# ✅ 1. 准备候选库（对 nova_pool 编码）\n",
        "# 确保你之前准备好的 nova_pool 有 FoodName_clean 列\n",
        "ref_texts = nova_pool[\"FoodName_clean\"].tolist()\n",
        "ref_embeddings = model.encode(ref_texts, convert_to_tensor=True)\n"
      ],
      "metadata": {
        "id": "LCxkp7aG71gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 2. 选取待匹配食物（NOVA_final 为空）\n",
        "unmatched_df = intake_df[intake_df[\"NOVA_final\"].isna()].copy()\n",
        "query_texts = unmatched_df[\"food_name_clean\"].dropna().tolist()\n",
        "query_indices = unmatched_df[\"food_name_clean\"].dropna().index\n",
        "\n",
        "query_embeddings = model.encode(query_texts, convert_to_tensor=True)\n"
      ],
      "metadata": {
        "id": "lrebQJ1R75Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 3. 计算语义相似度并提取匹配结果\n",
        "cosine_scores = util.pytorch_cos_sim(query_embeddings, ref_embeddings)\n",
        "top_scores, top_indices = torch.max(cosine_scores, dim=1)\n",
        "\n",
        "# 写入结果\n",
        "intake_df.loc[query_indices, \"SBERT_score\"] = top_scores.cpu().numpy()\n",
        "intake_df.loc[query_indices, \"SBERT_match_name\"] = nova_pool.iloc[top_indices.cpu().numpy()][\"FoodName_clean\"].values\n",
        "intake_df.loc[query_indices, \"NOVA_step4\"] = nova_pool.iloc[top_indices.cpu().numpy()][\"NOVA\"].values\n"
      ],
      "metadata": {
        "id": "C6GsX7UD79Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 4. 更新最终列：NOVA_final + Match_source\n",
        "# 如果 Step3 没找到但 Step4 找到了，使用 SBERT 匹配结果\n",
        "intake_df[\"NOVA_final\"] = intake_df[\"NOVA_final\"].combine_first(intake_df[\"NOVA_step4\"])\n",
        "\n",
        "# 同样更新匹配来源\n",
        "intake_df[\"Match_source\"] = intake_df.apply(lambda row: (\n",
        "    \"SBERT\" if pd.notna(row[\"NOVA_step4\"]) and pd.isna(row[\"NOVA_step1\"]) and pd.isna(row[\"NOVA_step2\"])\n",
        "    else row[\"Match_source\"]\n",
        "), axis=1)\n"
      ],
      "metadata": {
        "id": "NT3oNuGY8EUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🛠️ 重新指定列顺序\n",
        "col_order = [\n",
        "    'food_name_clean', 'Descriptionen', 'Foodgroupen',\n",
        "    'NOVA_step1', 'Match_reason',\n",
        "    'NOVA_step2', 'TFIDF_score', 'TFIDF_match_name',\n",
        "    'NOVA_step4', 'SBERT_score', 'SBERT_match_name',\n",
        "    'Match_source', 'NOVA_final'\n",
        "]\n",
        "\n",
        "# 应用顺序并导出\n",
        "intake_df = intake_df[col_order]\n",
        "intake_df.to_csv(\"/content/drive/MyDrive/UPF-HFI/outcome/intake_with_nova_step4_final.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "FSTQSiDGzFlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unmatched_final = intake_df[intake_df[\"NOVA_final\"].isin([None, \"\", \"NC\"])]\n",
        "print(f\"❌ 实际未匹配上的食物数量（含 NC）：{len(unmatched_final)}\")\n"
      ],
      "metadata": {
        "id": "WglYB0ua0dbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 提取未匹配的行（None, \"\", \"NC\"）\n",
        "unmatched_final = intake_df[intake_df[\"NOVA_final\"].isin([None, \"\", \"NC\"])]\n",
        "\n",
        "# 按 food_name_clean 统计频率\n",
        "nc_counts = unmatched_final[\"food_name_clean\"].value_counts().reset_index()\n",
        "nc_counts.columns = [\"food_name_clean\", \"count\"]\n",
        "\n",
        "# 展示前 30 个高频未匹配条目\n",
        "print(\"🍽️ 高频未匹配食物（前30）：\")\n",
        "print(nc_counts.head(30))\n",
        "\n",
        "# 可选：导出成 CSV 文件\n",
        "nc_counts.to_csv(\"/content/high_freq_nc_foods.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "JxzlQm4K4qde"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}